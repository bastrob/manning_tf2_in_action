{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring models (Tensorboard)\n",
    "\n",
    "Montiroing models is crucial part when training a model. Continuous monitoring of the model enables to ensure the model training is functioning as intended. Furthermore, it can also provide insights to improvements that can be made to improve model performance and execution time. Here, we will see how we can use the TensorBoard to continuously monitor the model, profile the model as well as visualize various data types such as images and text.\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch09/13.1_Tensorboard.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important check before running the Tensorboard\n",
    "\n",
    "In order to make sure all the features of the Tensorboard work, make sure to instell the `libcupti` library. On linux you can install this using `sudo apt-get install libcupti-dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "%load_ext tensorboard\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Image Data on the TensorBoard\n",
    "\n",
    "## Importing the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>, 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}\n"
     ]
    }
   ],
   "source": [
    "# Construct a tf.data.Dataset\n",
    "fashion_ds = tfds.load('fashion_mnist')\n",
    "\n",
    "print(fashion_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training/validation/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_valid_test_datasets(fashion_ds, batch_size, flatten_images=False):\n",
    "    \n",
    "    # Get the training dataset, shuffle it, and output a tuple of (image, label) \n",
    "    train_ds = fashion_ds[\"train\"].shuffle(batch_size*20).map(lambda xy: (xy[\"image\"], tf.reshape(xy[\"label\"], [-1])))\n",
    "    # Get the testing dataset, and output a tuple of (image, label)\n",
    "    test_ds = fashion_ds[\"test\"].map(lambda xy: (xy[\"image\"], tf.reshape(xy[\"label\"], [-1])))\n",
    "    \n",
    "    if flatten_images:\n",
    "        # Flatten the images to a 1D vector for fully-connected networks\n",
    "        train_ds = train_ds.map(lambda x,y: (tf.reshape(x, [-1]), y))\n",
    "        test_ds = test_ds.map(lambda x,y: (tf.reshape(x, [-1]), y))\n",
    "    \n",
    "    # Make the validation dataset the first 10000 data\n",
    "    valid_ds = train_ds.take(10000).batch(batch_size)\n",
    "    # Make training dataset the rest\n",
    "    train_ds = train_ds.skip(10000).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, valid_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `tf.summary` to visualize images on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to the tensorboard\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "# Defining the ID to Label map\n",
    "id2label_map = {\n",
    "    0: \"T-shirt/top\",\n",
    "    1: \"Trouser\",\n",
    "    2:\"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle boot\"\n",
    "}\n",
    "\n",
    "print(\"Writing to the tensorboard\")\n",
    "\n",
    "!rm -rf ./logs/data/train\n",
    "\n",
    "image_logdir = \"./logs/data/train\"\n",
    "image_writer = tf.summary.create_file_writer(image_logdir)\n",
    "\n",
    "# Write an image with its category\n",
    "with image_writer.as_default():\n",
    "    for data in fashion_ds[\"train\"].batch(1).take(10):\n",
    "        tf.summary.image(id2label_map[int(data[\"label\"].numpy())], data[\"image\"], max_outputs=20, step=0)\n",
    "\n",
    "# Write a batch of images at once\n",
    "with image_writer.as_default():\n",
    "    for data in fashion_ds[\"train\"].batch(20).take(1):\n",
    "        pass\n",
    "    tf.summary.image(\"A training data batch\", data[\"image\"], max_outputs=20, step=0)\n",
    "\n",
    "print('\\tDone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spinning up the TensorBoard\n",
    " \n",
    "Here we're using tensorboard magic command on jupyter notebook. This gives us the same board inline, as if you were to open the Tensorboard in a browser tab. If you call the same command multiple times with the same `logdir` it will reuse the same Tensorboard. If the directories are different a new TensorBoard is spun up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-fe37783930554936\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-fe37783930554936\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Open [Tensorboard](http://localhost:6006) in the browser\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking models on TensorBoard\n",
    "\n",
    "Here we will compare two models; a fully-connected model and a convolutional neural network. To compare them we will use the Fashion-MNIST dataset.\n",
    "\n",
    "## Monitoring the performance of the fully-connected network\n",
    "\n",
    "Here we analyse the training and validation performance of the fully-connected network. We will track loss and accuracy of the model.\n",
    "\n",
    "### Fully-connected network\n",
    "\n",
    "Here we define a fully connected network with 3 layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "dense_model = models.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "dense_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 10.4001 - accuracy: 0.7231 - val_loss: 0.7339 - val_accuracy: 0.7896\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5937 - accuracy: 0.8189 - val_loss: 0.5266 - val_accuracy: 0.8301\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.4743 - accuracy: 0.8408 - val_loss: 0.4678 - val_accuracy: 0.8481\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4256 - accuracy: 0.8511 - val_loss: 0.4670 - val_accuracy: 0.8443\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.4090 - accuracy: 0.8568 - val_loss: 0.4750 - val_accuracy: 0.8380\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3882 - accuracy: 0.8617 - val_loss: 0.3970 - val_accuracy: 0.8642\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3669 - accuracy: 0.8662 - val_loss: 0.3998 - val_accuracy: 0.8621\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3566 - accuracy: 0.8699 - val_loss: 0.4240 - val_accuracy: 0.8583\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3621 - accuracy: 0.8687 - val_loss: 0.3616 - val_accuracy: 0.8764\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3439 - accuracy: 0.8782 - val_loss: 0.3801 - val_accuracy: 0.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f77690d63c8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "tr_ds, v_ds, ts_ds = get_train_valid_test_datasets(fashion_ds, batch_size=batch_size, flatten_images=True)\n",
    "\n",
    "# Defining the tensorboard callback, it will log information to the defined log_dir directory\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/dense\", profile_batch=0)\n",
    "\n",
    "# Train the model\n",
    "dense_model.fit(tr_ds, validation_data=v_ds, epochs=10, callbacks=[tb_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If TensorBoard is not running, run the following command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and profiling the performance of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                31370     \n",
      "=================================================================\n",
      "Total params: 36,826\n",
      "Trainable params: 36,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same', activation='relu', input_shape=(28,28,1)),\n",
    "    layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "conv_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 8s 3ms/step - loss: 1.2062 - accuracy: 0.7524 - val_loss: 0.4124 - val_accuracy: 0.8603\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3644 - accuracy: 0.8699 - val_loss: 0.3605 - val_accuracy: 0.8701\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.3173 - accuracy: 0.8861 - val_loss: 0.3532 - val_accuracy: 0.8781\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2952 - accuracy: 0.8927 - val_loss: 0.3525 - val_accuracy: 0.8786\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2732 - accuracy: 0.8993 - val_loss: 0.3615 - val_accuracy: 0.8814\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2659 - accuracy: 0.9011 - val_loss: 0.3725 - val_accuracy: 0.8816\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2531 - accuracy: 0.9063 - val_loss: 0.3798 - val_accuracy: 0.8801\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2414 - accuracy: 0.9108 - val_loss: 0.3980 - val_accuracy: 0.8804\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2385 - accuracy: 0.9127 - val_loss: 0.3926 - val_accuracy: 0.8829\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.2273 - accuracy: 0.9157 - val_loss: 0.4106 - val_accuracy: 0.8811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f776906b780>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!rm -rf ./logs/conv\n",
    "\n",
    "batch_size = 64\n",
    "tr_ds, v_ds, ts_ds = get_train_valid_test_datasets(fashion_ds, batch_size=batch_size, flatten_images=False)\n",
    "\n",
    "# This tensorboard call back does the followin\n",
    "# 1. Log loss and accuracy\n",
    "# 2. Profile the model memory/time for 370-410 batches\n",
    "# 3. Plot activation histograms for 5th and 10th epoch\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs/conv\", profile_batch=[370, 410], histogram_freq=5)\n",
    "\n",
    "conv_model.fit(tr_ds, validation_data=v_ds, epochs=10, callbacks=[tb_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Open [Tensorboard](http://localhost:6006) in the browser\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard with custom training loops\n",
    "\n",
    "Here we train two models with and without batch normalization. Then we will analyze the mean and standard deviation of the absolute weights of the second layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "dense_model = models.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(784,)),    \n",
    "    layers.Dense(256, activation='relu', name='log_layer'),    \n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "dense_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "dense_model_bn = models.Sequential([\n",
    "    layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(256, activation='relu', name='log_layer_bn'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "dense_model_bn.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "\tDone\n",
      "Training epoch 2\n",
      "\tDone\n",
      "Training epoch 3\n",
      "\tDone\n",
      "Training epoch 4\n",
      "\tDone\n",
      "Training epoch 5\n",
      "\tDone\n",
      "Training completed\n",
      "\n",
      "Training epoch 1\n",
      "\tDone\n",
      "Training epoch 2\n",
      "\tDone\n",
      "Training epoch 3\n",
      "\tDone\n",
      "Training epoch 4\n",
      "\tDone\n",
      "Training epoch 5\n",
      "\tDone\n",
      "Training completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./logs/weights_exp\n",
    "\n",
    "def train_model(model, dataset, log_dir, log_layer_name, epochs):    \n",
    "    \n",
    "    # Define the writer\n",
    "    writer = tf.summary.create_file_writer(log_dir)\n",
    "    \n",
    "    step = 0\n",
    "    # Open the writer\n",
    "    with writer.as_default():        \n",
    "        tot_iterations_in_epoch = 0  # Total iterations in an epoch\n",
    "        \n",
    "        # For every epoch\n",
    "        for e in range(epochs):\n",
    "            print(\"Training epoch {}\".format(e+1))\n",
    "            # For every iteration in the epoch\n",
    "            for batch in tr_ds:\n",
    "                # Compute the step\n",
    "                \n",
    "                # Train with one batch\n",
    "                model.train_on_batch(*batch)\n",
    "                # Get the weights of the layer [0] - weights / [1] - bias\n",
    "                w = model.get_layer(log_layer_name).get_weights()[0]\n",
    "                \n",
    "                # Log mean and std of absolute weights\n",
    "                tf.summary.scalar(\"mean_weights\", np.mean(np.abs(w)), step=step)\n",
    "                tf.summary.scalar(\"std_weights\", np.std(np.abs(w)), step=step)\n",
    "                \n",
    "                # Flush to the disk from the buffer\n",
    "                writer.flush()\n",
    "                \n",
    "                step += 1\n",
    "            print('\\tDone')\n",
    "    \n",
    "    print(\"Training completed\\n\")\n",
    "    \n",
    "batch_size = 64\n",
    "tr_ds, _, _ = get_train_valid_test_datasets(fashion_ds, batch_size=batch_size, flatten_images=True)\n",
    "train_model(dense_model, tr_ds, './logs/weights_exp/standard', \"log_layer\", 5)\n",
    "\n",
    "tr_ds, _, _ = get_train_valid_test_datasets(fashion_ds, batch_size=batch_size, flatten_images=True)\n",
    "train_model(dense_model_bn, tr_ds, './logs/weights_exp/bn', \"log_layer_bn\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing word vectors on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "if not os.path.exists(os.path.join('data','glove.6B.zip')):\n",
    "    \n",
    "    print(\"Downloading\")\n",
    "    url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','glove.6B.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    print(\"\\tDone\")\n",
    "    \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'glove.6B.50d.txt')):\n",
    "    print(\"Extracting data\")\n",
    "    with zipfile.ZipFile(os.path.join('data','glove.6B.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "    print(\"\\tDone\")\n",
    "else:\n",
    "    print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the most common words in the IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "review_ds = tfds.load('imdb_reviews')\n",
    "train_review_ds = review_ds[\"train\"]\n",
    "\n",
    "corpus = []\n",
    "for data in train_review_ds:      \n",
    "    txt = str(np.char.decode(data[\"text\"].numpy(), encoding='utf-8')).lower()\n",
    "    corpus.append(str(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 322198), ('a', 159953), ('and', 158572), ('of', 144462), ('to', 133967), ('is', 104171), ('in', 90527), ('i', 70480), ('this', 69714), ('that', 66292), ('it', 65505), ('/><br', 50935), ('was', 47024), ('as', 45102), ('for', 42843), ('with', 42729), ('but', 39764), ('on', 31619), ('movie', 30887), ('his', 29059), ('are', 28743), ('not', 28597), ('film', 27777), ('you', 27564), ('have', 27344), ('he', 26177), ('be', 25691), ('at', 22731), ('one', 22480), ('by', 21976), ('an', 21240), ('they', 20624), ('from', 19934), ('all', 19740), ('who', 19407), ('like', 18779), ('so', 18099), ('just', 17309), ('or', 16769), ('has', 16570), ('her', 16540), ('about', 16486), (\"it's\", 15970), ('some', 15280), ('if', 15189), ('out', 14510), ('what', 14055), ('very', 13633), ('when', 13609), ('more', 13170), ('there', 13094), ('she', 12234), ('would', 12027), ('even', 12010), ('good', 11926), ('my', 11766), ('only', 11566), ('their', 11317), ('no', 11273), ('really', 11065), ('had', 11042), ('which', 10898), ('can', 10797), ('up', 10776), ('were', 10528), ('see', 10410), ('than', 9807), ('we', 9417), ('-', 9355), ('been', 9074), ('into', 8990), ('get', 8959), ('will', 8926), ('story', 8743), ('much', 8739), ('because', 8736), ('most', 8477), ('how', 8456), ('other', 8229), ('also', 8007), ('first', 7985), ('its', 7963), ('time', 7945), ('do', 7904), (\"don't\", 7879), ('me', 7722), ('great', 7714), ('people', 7676), ('could', 7594), ('make', 7590), ('any', 7507), ('/>the', 7409), ('after', 7118), ('made', 7041), ('then', 6945), ('bad', 6816), ('think', 6773), ('being', 6390), ('many', 6388), ('him', 6385)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corpus = \" \".join(corpus)\n",
    "\n",
    "cnt = Counter(corpus.split())\n",
    "print(cnt.most_common(100))\n",
    "\n",
    "most_common_words = [w for w,_ in cnt.most_common(5000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read GloVe vectors and filter the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "Skipping line 9: field larger than field limit (131072)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>-0.41242</td>\n",
       "      <td>0.12170</td>\n",
       "      <td>0.34527</td>\n",
       "      <td>-0.044457</td>\n",
       "      <td>-0.49688</td>\n",
       "      <td>-0.17862</td>\n",
       "      <td>-0.00066</td>\n",
       "      <td>-0.656600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298710</td>\n",
       "      <td>-0.157490</td>\n",
       "      <td>-0.347580</td>\n",
       "      <td>-0.045637</td>\n",
       "      <td>-0.44251</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>-0.184110</td>\n",
       "      <td>-0.115140</td>\n",
       "      <td>-0.78581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.236820</td>\n",
       "      <td>-0.16899</td>\n",
       "      <td>0.40951</td>\n",
       "      <td>0.63812</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>-0.42852</td>\n",
       "      <td>-0.55641</td>\n",
       "      <td>-0.36400</td>\n",
       "      <td>-0.239380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080262</td>\n",
       "      <td>0.630030</td>\n",
       "      <td>0.321110</td>\n",
       "      <td>-0.467650</td>\n",
       "      <td>0.22786</td>\n",
       "      <td>0.360340</td>\n",
       "      <td>-0.378180</td>\n",
       "      <td>-0.566570</td>\n",
       "      <td>0.044691</td>\n",
       "      <td>0.30392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.151640</td>\n",
       "      <td>0.301770</td>\n",
       "      <td>-0.16763</td>\n",
       "      <td>0.17684</td>\n",
       "      <td>0.31719</td>\n",
       "      <td>0.339730</td>\n",
       "      <td>-0.43478</td>\n",
       "      <td>-0.31086</td>\n",
       "      <td>-0.44999</td>\n",
       "      <td>-0.294860</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000064</td>\n",
       "      <td>0.068987</td>\n",
       "      <td>0.087939</td>\n",
       "      <td>-0.102850</td>\n",
       "      <td>-0.13931</td>\n",
       "      <td>0.223140</td>\n",
       "      <td>-0.080803</td>\n",
       "      <td>-0.356520</td>\n",
       "      <td>0.016413</td>\n",
       "      <td>0.10216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.708530</td>\n",
       "      <td>0.570880</td>\n",
       "      <td>-0.47160</td>\n",
       "      <td>0.18048</td>\n",
       "      <td>0.54449</td>\n",
       "      <td>0.726030</td>\n",
       "      <td>0.18157</td>\n",
       "      <td>-0.52393</td>\n",
       "      <td>0.10381</td>\n",
       "      <td>-0.175660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347270</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>0.075693</td>\n",
       "      <td>-0.062178</td>\n",
       "      <td>-0.38988</td>\n",
       "      <td>0.229020</td>\n",
       "      <td>-0.216170</td>\n",
       "      <td>-0.225620</td>\n",
       "      <td>-0.093918</td>\n",
       "      <td>-0.80375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.680470</td>\n",
       "      <td>-0.039263</td>\n",
       "      <td>0.30186</td>\n",
       "      <td>-0.17792</td>\n",
       "      <td>0.42962</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>-0.41376</td>\n",
       "      <td>0.13228</td>\n",
       "      <td>-0.29847</td>\n",
       "      <td>-0.085253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094375</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.210480</td>\n",
       "      <td>-0.030880</td>\n",
       "      <td>-0.19722</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>-0.094340</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.064699</td>\n",
       "      <td>-0.26044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1         2        3        4        5         6        7   \\\n",
       "0                                                                       \n",
       "the  0.418000  0.249680 -0.41242  0.12170  0.34527 -0.044457 -0.49688   \n",
       ",    0.013441  0.236820 -0.16899  0.40951  0.63812  0.477090 -0.42852   \n",
       ".    0.151640  0.301770 -0.16763  0.17684  0.31719  0.339730 -0.43478   \n",
       "of   0.708530  0.570880 -0.47160  0.18048  0.54449  0.726030  0.18157   \n",
       "to   0.680470 -0.039263  0.30186 -0.17792  0.42962  0.032246 -0.41376   \n",
       "\n",
       "          8        9         10  ...        41        42        43        44  \\\n",
       "0                                ...                                           \n",
       "the -0.17862 -0.00066 -0.656600  ... -0.298710 -0.157490 -0.347580 -0.045637   \n",
       ",   -0.55641 -0.36400 -0.239380  ... -0.080262  0.630030  0.321110 -0.467650   \n",
       ".   -0.31086 -0.44999 -0.294860  ... -0.000064  0.068987  0.087939 -0.102850   \n",
       "of  -0.52393  0.10381 -0.175660  ... -0.347270  0.284830  0.075693 -0.062178   \n",
       "to   0.13228 -0.29847 -0.085253  ... -0.094375  0.018324  0.210480 -0.030880   \n",
       "\n",
       "          45        46        47        48        49       50  \n",
       "0                                                              \n",
       "the -0.44251  0.187850  0.002785 -0.184110 -0.115140 -0.78581  \n",
       ",    0.22786  0.360340 -0.378180 -0.566570  0.044691  0.30392  \n",
       ".   -0.13931  0.223140 -0.080803 -0.356520  0.016413  0.10216  \n",
       "of  -0.38988  0.229020 -0.216170 -0.225620 -0.093918 -0.80375  \n",
       "to  -0.19722  0.082279 -0.094340 -0.073297 -0.064699 -0.26044  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join('data', 'glove.6B.50d.txt'), header=None, index_col=0, sep=None, error_bad_lines=False, encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full size of Glove: 399694\n",
      "Size after only considering the most common words: (3595, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\"Full size of Glove: {}\".format(df.shape[0]))\n",
    "df_common = df.loc[df.index.isin(most_common_words)]\n",
    "print(\"Size after only considering the most common words: {}\".format(df_common.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the word vectors in order to be projected on TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3595, 50)\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.plugins import projector\n",
    "\n",
    "log_dir=os.path.join('logs', 'embeddings')\n",
    "# Save the weights we want to analyse as a variable. Note that the first\n",
    "# value represents any unknown word, which is not in the metadata, so\n",
    "# we will remove that value.\n",
    "weights = tf.Variable(df_common.values)\n",
    "print(weights.shape)\n",
    "# Create a checkpoint from embedding, the filename and key are\n",
    "# name of the tensor.\n",
    "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w') as f:\n",
    "    for w in df_common.index:\n",
    "        f.write(w+'\\n')\n",
    "        \n",
    "# Set up config\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "#embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIsualizing/highlighting word vecs\n",
    "# (?:fred|larry|mrs\\.|mr\\.|michelle|sea|denzel|beach|comedy|theater|idiotic|sadistic|marvelous|loving|gorg|bus|truck|lugosi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-23 07:37:31.354360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/embeddings/ --port 6007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Open [Tensorboard for Word Vectors](http://localhost:6007) in the browser\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
