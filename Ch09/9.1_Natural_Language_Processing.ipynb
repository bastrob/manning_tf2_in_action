{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss two important topics.\n",
    "\n",
    "* Sentiment analysis\n",
    "* Language modelling\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch09/9.1.Natural_Language_Processing.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and other housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "# http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):\n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):\n",
    "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
    "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>A1HP7NVNPFMA4N</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Ambrosia075</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "      <td>but when you do it's great.</td>\n",
       "      <td>1445040000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>A1JGAP0185YJI6</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>travis</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "      <td>But in spite of that it was fun, I liked it</td>\n",
       "      <td>1437955200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2015</td>\n",
       "      <td>A1YJWEXHQBWK2B</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Vincent G. Mezera</td>\n",
       "      <td>ok game.</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>1424649600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>A2204E1TH211HT</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>Grandma KR</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1424390400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>12 25, 2014</td>\n",
       "      <td>A2RF5B5H74JLPE</td>\n",
       "      <td>0700026657</td>\n",
       "      <td>jon</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "      <td>love this game</td>\n",
       "      <td>1419465600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0        5      True  10 17, 2015  A1HP7NVNPFMA4N  0700026657   \n",
       "1        4     False  07 27, 2015  A1JGAP0185YJI6  0700026657   \n",
       "2        3      True  02 23, 2015  A1YJWEXHQBWK2B  0700026657   \n",
       "3        2      True  02 20, 2015  A2204E1TH211HT  0700026657   \n",
       "4        5      True  12 25, 2014  A2RF5B5H74JLPE  0700026657   \n",
       "\n",
       "        reviewerName                                         reviewText  \\\n",
       "0        Ambrosia075  This game is a bit hard to get the hang of, bu...   \n",
       "1             travis  I played it a while but it was alright. The st...   \n",
       "2  Vincent G. Mezera                                           ok game.   \n",
       "3         Grandma KR  found the game a bit too complicated, not what...   \n",
       "4                jon  great game, I love it and have played it since...   \n",
       "\n",
       "                                       summary  unixReviewTime vote style  \\\n",
       "0                  but when you do it's great.      1445040000  NaN   NaN   \n",
       "1  But in spite of that it was fun, I liked it      1437955200  NaN   NaN   \n",
       "2                                  Three Stars      1424649600  NaN   NaN   \n",
       "3                                    Two Stars      1424390400  NaN   NaN   \n",
       "4                               love this game      1419465600  NaN   NaN   \n",
       "\n",
       "  image  \n",
       "0   NaN  \n",
       "1   NaN  \n",
       "2   NaN  \n",
       "3   NaN  \n",
       "4   NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_json(os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records')\n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (497577, 13)\n",
      "After cleaning up: (497419, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning up: {}\".format(review_df.shape))\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "print(\"After cleaning up: {}\".format(review_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking verified vs non-verified reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     332504\n",
       "False    164915\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"verified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check number of reviews for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    222335\n",
       "4     54878\n",
       "3     27973\n",
       "1     15200\n",
       "2     12118\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
    "verified_df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\manning.tf2\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    277213\n",
       "0     55291\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n",
    "verified_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "* Lower case (Spacy)\n",
    "* Remove stop words (Spacy)\n",
    "* Lemmatize (Spacy)\n",
    "* Remove punctuation (Keras)\n",
    "* Only keep most commmon n-worods (Keras)\n",
    "* Most common n-grams (Keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "data, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: She sells seashells by the seashore.\n",
      "After clean: sell seashell seashore\n",
      "\n",
      "Processing all the review data ...\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "nltk.download('wordnet', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "def clean_text(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = doc.replace(\"n\\'t \", ' not ')\n",
    "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
    "    doc = re.sub(r\"/d+\",\"\", doc)\n",
    "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  \n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    clean_text = [\n",
    "        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n",
    "        if p[0]=='N' or p[0]=='V' else w \\\n",
    "        for (w, p) in pos_tags\n",
    "    ]\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "sample_doc = 'She sells seashells by the seashore.'\n",
    "print(\"Before clean: {}\".format(sample_doc))\n",
    "print(\"After clean: {}\".format(' '.join(clean_text(sample_doc))))\n",
    "print(\"\\nProcessing all the review data ...\")\n",
    "data = data.apply(lambda x: clean_text(x))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game     443725\n",
       "not      276928\n",
       "play     143060\n",
       "'s       139472\n",
       "get      119875\n",
       "like     109808\n",
       "great    102651\n",
       "one       97790\n",
       "``        91834\n",
       "good      83159\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data_list = [w for doc in data for w in doc]\n",
    "cnt = Counter(data_list)\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "freq_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    137945.000000\n",
       "mean         79.732154\n",
       "std        1894.467706\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           1.000000\n",
       "75%           4.000000\n",
       "90%          19.000000\n",
       "max      443725.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(freq_df.median())\n",
    "freq_df.describe(percentiles=[0.25,0.5,0.75,0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length: 12.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    332504.000000\n",
       "mean         33.078255\n",
       "std          74.746220\n",
       "min           0.000000\n",
       "25%           4.000000\n",
       "50%          12.000000\n",
       "75%          30.000000\n",
       "80%          39.000000\n",
       "max        3158.000000\n",
       "Name: reviewText, dtype: float64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length_ser = data.str.len()\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "seq_length_ser.describe(percentiles=[0.25,0.5,0.75,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 6496\n",
      "Using a sequence length: 76\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 80).sum()\n",
    "n_seq = 39\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))\n",
    "print(\"Using a sequence length: {}\".format(n_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "tokenizer.fit_on_texts(data.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text to numbers\n",
    "\n",
    "Here we are going to transform the preprocessed text to number sequences and pad them to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
