{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss two important topics.\n",
    "\n",
    "* Sentiment analysis\n",
    "* Language modelling\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch09/9.1.Natural_Language_Processing.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and other housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "# http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):\n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):\n",
    "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
    "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2015</td>\n",
       "      <td>ok game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>12 25, 2014</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime  \\\n",
       "0        5      True  10 17, 2015   \n",
       "1        4     False  07 27, 2015   \n",
       "2        3      True  02 23, 2015   \n",
       "3        2      True  02 20, 2015   \n",
       "4        5      True  12 25, 2014   \n",
       "\n",
       "                                          reviewText  \n",
       "0  This game is a bit hard to get the hang of, bu...  \n",
       "1  I played it a while but it was alright. The st...  \n",
       "2                                           ok game.  \n",
       "3  found the game a bit too complicated, not what...  \n",
       "4  great game, I love it and have played it since...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_json(os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records')\n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up data\n",
    "\n",
    "Remove entries where the description is null or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (497577, 4)\n",
      "After cleaning up: (497419, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning up: {}\".format(review_df.shape))\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n",
    "print(\"After cleaning up: {}\".format(review_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking verified vs non-verified review counts\n",
    "\n",
    "To improve the quality of the data reviewed, we will only use the verified reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     332504\n",
       "False    164915\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"verified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the review count for each rating\n",
    "As you can see there are way too many 5 star reviews in our dataset. Therefore we must make sure to account for this imbalance when batching data and creating a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    222335\n",
       "4     54878\n",
       "3     27973\n",
       "1     15200\n",
       "2     12118\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
    "verified_df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map rating to a positive/negative label\n",
    "\n",
    "Here we map both 5 and 4 reviews to 1 (positive) and 3,2, and 1 to 0 (negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    277213\n",
       "0     55291\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n",
    "verified_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "* Lower case (Spacy)\n",
    "* Remove stop words (Spacy)\n",
    "* Lemmatize (Spacy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "data, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to nltk...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: She sells seashells by the seashore.\n",
      "After clean: sell seashell seashore\n",
      "\n",
      "Processing all the review data ...\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "nltk.download('wordnet', download_dir='nltk')\n",
    "nltk.download('stopwords', download_dir='nltk')\n",
    "nltk.download('punkt', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "def clean_text(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = doc.replace(\"n\\'t \", ' not ')\n",
    "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
    "    doc = re.sub(r\"/d+\",\"\", doc)\n",
    "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  \n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    clean_text = [\n",
    "        lemmatizer.lemmatize(w, pos=p[0].lower()) \\\n",
    "        if p[0]=='N' or p[0]=='V' else w \\\n",
    "        for (w, p) in pos_tags\n",
    "    ]\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "sample_doc = 'She sells seashells by the seashore.'\n",
    "print(\"Before clean: {}\".format(sample_doc))\n",
    "print(\"After clean: {}\".format(' '.join(clean_text(sample_doc))))\n",
    "print(\"\\nProcessing all the review data ...\")\n",
    "data = data.apply(lambda x: clean_text(x))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle(os.path.join('data','data.pkl'))\n",
    "labels.to_pickle(os.path.join('data','labels.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game     442821\n",
      "not      274722\n",
      "'s       139352\n",
      "play     139233\n",
      "get      119013\n",
      "like     109298\n",
      "great    102644\n",
      "one       97786\n",
      "good      83155\n",
      "time      69146\n",
      "dtype: int64\n",
      "\n",
      "Median: 1.0\n",
      "\n",
      "count    140380.000000\n",
      "mean         78.487919\n",
      "std        1861.175078\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           4.000000\n",
      "90%          19.000000\n",
      "max      442821.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data_list = [w for doc in data for w in doc]\n",
    "cnt = Counter(data_list)\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "print(freq_df.head(n=10))\n",
    "print(\"\\nMedian: {}\\n\".format(freq_df.median()))\n",
    "print(freq_df.describe(percentiles=[0.25,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median length: 12.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    332504.000000\n",
       "mean         33.136846\n",
       "std          74.867728\n",
       "min           0.000000\n",
       "25%           4.000000\n",
       "50%          12.000000\n",
       "75%          30.000000\n",
       "80%          39.000000\n",
       "max        3162.000000\n",
       "Name: reviewText, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length_ser = data.str.len()\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "seq_length_ser.describe(percentiles=[0.25,0.5,0.75,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 12402\n",
      "Using a sequence length: 39\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 25).sum()\n",
    "n_seq = 39\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))\n",
    "print(\"Using a sequence length: {}\".format(n_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110834.66666666667\n",
      "2.0\n",
      "12.0\n",
      "47.0\n"
     ]
    }
   ],
   "source": [
    "print(seq_length_ser.shape[0]/3.0)\n",
    "seq_length_ser = seq_length_ser.sort_values()\n",
    "print(seq_length_ser.iloc[:int(seq_length_ser.shape[0]/3.0)].median())\n",
    "print(seq_length_ser.iloc[int(seq_length_ser.shape[0]/3.0): int(seq_length_ser.shape[0]*2.0/3.0)].median())\n",
    "print(seq_length_ser.iloc[int(seq_length_ser.shape[0]*2.0/3.0):].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming text to numbers\n",
    "\n",
    "* Only keep most commmon n-worods (Keras)\n",
    "\n",
    "## What is not covered here\n",
    "* Instead of words, use n_grams to represent a review\n",
    "\n",
    "Here we are going to transform the preprocessed text to number sequences and pad them to a fixed length using tf.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Keras tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "tokenizer.fit_on_texts(data.tolist())\n",
    "data_seq = tokenizer.texts_to_sequences(data.tolist())\n",
    "\n",
    "data_seq = pd.Series(data_seq)\n",
    "labels = labels.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data to train/valid/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 310388\n",
      "Validation data: 11058\n",
      "Test data: 11058\n"
     ]
    }
   ],
   "source": [
    "def train_valid_test_split(text_seq, labels, train_fraction=0.8):\n",
    "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"\n",
    "    \n",
    "    # Remove empty data points\n",
    "    valid_ids = (data_seq.str.len()>0).index\n",
    "    text_seq = text_seq.loc[valid_ids]\n",
    "    labels = labels.loc[valid_ids]\n",
    "    \n",
    "    # Separate indices of negative and positive data points\n",
    "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels==1)].index)\n",
    "    \n",
    "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
    "    n_test = n_valid\n",
    "    \n",
    "    neg_test_inds = neg_indices.sample(n=n_test)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
    "    \n",
    "    pos_test_inds = pos_indices.sample(n=n_test)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test)\n",
    "    pos_train_inds = pos_indices.loc[\n",
    "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
    "    ]\n",
    "    \n",
    "    tr_x = text_seq.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = text_seq.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = text_seq.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    \n",
    "    print('Training data: {}'.format(len(tr_x)))\n",
    "    print('Validation data: {}'.format(len(v_x)))\n",
    "    print('Test data: {}'.format(len(ts_x)))\n",
    "    \n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "    \n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(data_seq, labels)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join('data','reviews.pkl'), 'wb') as f:\n",
    "    pickle.dump(((tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219329    1\n",
      "242101    1\n",
      "139482    1\n",
      "165147    0\n",
      "282412    1\n",
      "         ..\n",
      "244172    1\n",
      "54855     1\n",
      "17086     0\n",
      "79139     1\n",
      "68723     0\n",
      "Name: label, Length: 310388, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data', 'reviews.pkl'), 'rb') as f:\n",
    "    (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = pickle.load(f)\n",
    "    \n",
    "print(tr_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.data` Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be using a weight of 6.017113919471887 for negative samples\n"
     ]
    }
   ],
   "source": [
    "#data_seq_sample = data_seq[:1000]\n",
    "#label_sample = labels[:1000].reset_index(drop=True)\n",
    "\n",
    "with open(os.path.join('data', 'reviews.pkl'), 'rb') as f:\n",
    "    (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = pickle.load(f)\n",
    "\n",
    "def get_tf_pipeline(text_seq, labels, batch_size=64, bucket_boundaries=[5,15], max_length=50, shuffle=False):\n",
    "    \n",
    "    data_seq = [[b]+a for a,b in zip(text_seq, labels) ]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq)[:,:max_length])\n",
    "\n",
    "    bucket_fn = tf.data.experimental.bucket_by_sequence_length(\n",
    "        lambda x: tf.cast(tf.shape(x)[0],'int32'), \n",
    "        bucket_boundaries=bucket_boundaries, \n",
    "        bucket_batch_sizes=[batch_size,batch_size,batch_size], \n",
    "        padded_shapes=None,\n",
    "        padding_values=0, \n",
    "        pad_to_bucket_boundary=False\n",
    "    )\n",
    "\n",
    "    text_ds = text_ds.map(lambda x: x).apply(bucket_fn)\n",
    "    \n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "        \n",
    "    text_ds = text_ds.map(lambda x: (x[:,1:], x[:,0]))\n",
    "    #text_ds = text_ds.map(lambda x,y: (tf.one_hot(x, depth=n_vocab),y))\n",
    "    \n",
    "    return text_ds\n",
    "\n",
    "neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
    "print(\"Will be using a weight of {} for negative samples\".format(neg_weight))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the behavior of bucketing fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[1 2 0]\n",
      " [1 2 3]], shape=(2, 3), dtype=int32)\n",
      "\ty= tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[   3    2    6  543    2 3243    2  134   52   23    0]\n",
      " [   3   32   21    3    2    4  134   45    1    1   45]], shape=(2, 11), dtype=int32)\n",
      "\ty= tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[  2   4 214  21   0   0   0   0]\n",
      " [  3   4  42   7   3   2  45  52]], shape=(2, 8), dtype=int32)\n",
      "\ty= tf.Tensor([1 0], shape=(2,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int32)\n",
      "\ty= tf.Tensor([0], shape=(1,), dtype=int32)\n",
      "\n",
      "\n",
      "tf.Tensor(\n",
      "[[2 3 6 4 5]\n",
      " [2 0 9 7 0]], shape=(2, 5), dtype=int32)\n",
      "\ty= tf.Tensor([1 1], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = [[1,2],[1],[1,2,3], [2,3,6,4,5],[2,0,9,7],[2,4,214,21],[3,4,42,7,3,2,45,52],[3,2,6,543,2,3243,2,134,52,23],[3,32,21,3,2,4,134,45,1,1,45]]\n",
    "y = [0,0,0, 1, 1, 1, 0, 0, 0]\n",
    "\n",
    "a = get_tf_pipeline(x, y, batch_size=2, bucket_boundaries=[3,5], max_length=15, shuffle=True)\n",
    "\n",
    "for x,y in a.take(6):\n",
    "    print('\\n')\n",
    "    print(x)\n",
    "    print('\\ty=', y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some training data ...\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(64,), dtype=int32)\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[0 1 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1], shape=(64,), dtype=int32)\n",
      "\n",
      "Some validation data ...\n",
      "Input sequence shape: (64, 49)\n",
      "tf.Tensor(\n",
      "[0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1\n",
      " 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 0 1 0 0 0 0], shape=(64,), dtype=int32)\n",
      "Input sequence shape: (64, 13)\n",
      "tf.Tensor(\n",
      "[1 0 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1\n",
      " 0 0 1 1 0 0 0 1 1 1 1 0 1 0 0 1 0 1 1 0 0 1 1 0 1 1 0], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y)\n",
    "\n",
    "print(\"Some training data ...\")\n",
    "for x,y in train_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)\n",
    "\n",
    "print(\"\\nSome validation data ...\")\n",
    "for x,y in valid_ds.take(2):\n",
    "    print(\"Input sequence shape: {}\".format(x.shape))\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, None, 12402)       0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               6415872   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 6,482,433\n",
      "Trainable params: 6,482,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Create a mask to mask out zero inputs\n",
    "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),\n",
    "    # After creating the mask, convert inputs to onehot encoded inputs\n",
    "    tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x,'int32'), depth=n_vocab), input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "batch_size =128\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=batch_size, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=batch_size)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=batch_size)\n",
    "print('\\tDone...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Without masking\n",
    "\n",
    "```\n",
    "Using metric=val_loss and mode=min for EarlyStopping\n",
    "Epoch 1/10\n",
    "4852/4852 [==============================] - 100s 21ms/step - loss: 0.7669 - accuracy: 0.7991 - val_loss: 0.4141 - val_accuracy: 0.8158 - lr: 0.0010\n",
    "Epoch 2/10\n",
    "4852/4852 [==============================] - 99s 20ms/step - loss: 0.6120 - accuracy: 0.8472 - val_loss: 0.3767 - val_accuracy: 0.8296 - lr: 0.0010\n",
    "Epoch 3/10\n",
    "4852/4852 [==============================] - 99s 20ms/step - loss: 0.5260 - accuracy: 0.8729 - val_loss: 0.4139 - val_accuracy: 0.8264 - lr: 0.0010\n",
    "Epoch 4/10\n",
    "4852/4852 [==============================] - 93s 19ms/step - loss: 0.4431 - accuracy: 0.8965 - val_loss: 0.4197 - val_accuracy: 0.8296 - lr: 0.0010\n",
    "Epoch 5/10\n",
    "4852/4852 [==============================] - 93s 19ms/step - loss: 0.3685 - accuracy: 0.9179 - val_loss: 0.5000 - val_accuracy: 0.8103 - lr: 0.0010\n",
    "Epoch 6/10\n",
    "4852/4852 [==============================] - 96s 20ms/step - loss: 0.2712 - accuracy: 0.9426 - val_loss: 0.6457 - val_accuracy: 0.8149 - lr: 1.0000e-04\n",
    "Epoch 7/10\n",
    "4852/4852 [==============================] - 99s 20ms/step - loss: 0.2387 - accuracy: 0.9513 - val_loss: 0.7918 - val_accuracy: 0.8019 - lr: 1.0000e-04\n",
    "Epoch 8/10\n",
    "4852/4852 [==============================] - 99s 20ms/step - loss: 0.2179 - accuracy: 0.9566 - val_loss: 0.8088 - val_accuracy: 0.8046 - lr: 1.0000e-04\n",
    "It took 787.5877232551575 seconds to complete the training\n",
    "```\n",
    "\n",
    "With masking\n",
    "\n",
    "```\n",
    "Using metric=val_loss and mode=min for EarlyStopping\n",
    "Epoch 1/10\n",
    "4852/4852 [==============================] - 97s 20ms/step - loss: 0.7797 - accuracy: 0.7997 - val_loss: 0.4164 - val_accuracy: 0.8064 - lr: 0.0010\n",
    "Epoch 2/10\n",
    "4852/4852 [==============================] - 102s 21ms/step - loss: 0.6149 - accuracy: 0.8461 - val_loss: 0.3865 - val_accuracy: 0.8213 - lr: 0.0010\n",
    "Epoch 3/10\n",
    "4852/4852 [==============================] - 91s 19ms/step - loss: 0.5263 - accuracy: 0.8716 - val_loss: 0.3870 - val_accuracy: 0.8328 - lr: 0.0010\n",
    "Epoch 4/10\n",
    "4852/4852 [==============================] - 96s 20ms/step - loss: 0.4422 - accuracy: 0.8969 - val_loss: 0.4511 - val_accuracy: 0.8229 - lr: 0.0010\n",
    "Epoch 5/10\n",
    "4852/4852 [==============================] - 98s 20ms/step - loss: 0.3710 - accuracy: 0.9164 - val_loss: 0.4894 - val_accuracy: 0.8225 - lr: 0.0010\n",
    "Epoch 6/10\n",
    "4852/4852 [==============================] - 94s 19ms/step - loss: 0.2735 - accuracy: 0.9426 - val_loss: 0.6428 - val_accuracy: 0.8100 - lr: 1.0000e-04\n",
    "Epoch 7/10\n",
    "4852/4852 [==============================] - 94s 19ms/step - loss: 0.2425 - accuracy: 0.9501 - val_loss: 0.7428 - val_accuracy: 0.8048 - lr: 1.0000e-04\n",
    "Epoch 8/10\n",
    "4852/4852 [==============================] - 96s 20ms/step - loss: 0.2217 - accuracy: 0.9555 - val_loss: 0.8030 - val_accuracy: 0.8009 - lr: 1.0000e-04\n",
    "It took 775.7794342041016 seconds to complete the training\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "2427/2427 [==============================] - 76s 31ms/step - loss: 0.7702 - accuracy: 0.7960 - val_loss: 0.3840 - val_accuracy: 0.8303 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "2427/2427 [==============================] - 75s 31ms/step - loss: 0.6271 - accuracy: 0.8387 - val_loss: 0.3761 - val_accuracy: 0.8356 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "2427/2427 [==============================] - 76s 31ms/step - loss: 0.5384 - accuracy: 0.8635 - val_loss: 0.3865 - val_accuracy: 0.8389 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "2427/2427 [==============================] - 76s 31ms/step - loss: 0.4594 - accuracy: 0.8897 - val_loss: 0.4392 - val_accuracy: 0.8311 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "2427/2427 [==============================] - 76s 31ms/step - loss: 0.3887 - accuracy: 0.9096 - val_loss: 0.5186 - val_accuracy: 0.8290 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "2427/2427 [==============================] - 75s 31ms/step - loss: 0.2822 - accuracy: 0.9375 - val_loss: 0.6364 - val_accuracy: 0.8227 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "2427/2427 [==============================] - 75s 31ms/step - loss: 0.2578 - accuracy: 0.9441 - val_loss: 0.7593 - val_accuracy: 0.8131 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "2427/2427 [==============================] - 75s 31ms/step - loss: 0.2412 - accuracy: 0.9479 - val_loss: 0.7930 - val_accuracy: 0.8122 - lr: 1.0000e-04\n",
      "It took 631.7865388393402 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_sentiment_analysis.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 3s 30ms/step - loss: 0.8023 - accuracy: 0.8069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8022735714912415, 0.8069270849227905]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis with an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         1587584   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,785,729\n",
      "Trainable params: 1,785,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),\n",
    "    # Create a mask to mask out zero inputs\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, \n",
    "                              #mask_zero=True, \n",
    "                              input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n",
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/25\n",
      "2427/2427 [==============================] - 30s 13ms/step - loss: 0.7526 - accuracy: 0.8033 - val_loss: 0.3773 - val_accuracy: 0.8332 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.6131 - accuracy: 0.8452 - val_loss: 0.3635 - val_accuracy: 0.8395 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.5427 - accuracy: 0.8665 - val_loss: 0.3678 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Epoch 4/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.4788 - accuracy: 0.8848 - val_loss: 0.4156 - val_accuracy: 0.8378 - lr: 0.0010\n",
      "Epoch 5/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.4304 - accuracy: 0.8994 - val_loss: 0.4941 - val_accuracy: 0.8300 - lr: 0.0010\n",
      "Epoch 6/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.3354 - accuracy: 0.9259 - val_loss: 0.5840 - val_accuracy: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 7/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.3119 - accuracy: 0.9318 - val_loss: 0.6455 - val_accuracy: 0.8222 - lr: 1.0000e-04\n",
      "Epoch 8/25\n",
      "2427/2427 [==============================] - 30s 12ms/step - loss: 0.2947 - accuracy: 0.9361 - val_loss: 0.7101 - val_accuracy: 0.8235 - lr: 1.0000e-04\n",
      "It took 269.5488338470459 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "batch_size=128\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, batch_size=128, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y, batch_size=128,)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "print('\\tDone...')\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','3_sentiment_analysis.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=25, class_weight={0:neg_weight, 1:1.0}, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 5ms/step - loss: 0.7106 - accuracy: 0.8235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7106139063835144, 0.8234761953353882]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse some of the top positive/negative sentiments predicted by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 11058\n",
      "Pred: (11058, 1)\n",
      "Y: (11058,)\n"
     ]
    }
   ],
   "source": [
    "test_ds = get_tf_pipeline(ts_x, ts_y, batch_size=128)\n",
    "\n",
    "test_x = []\n",
    "test_pred = []\n",
    "test_y = []\n",
    "for x, y in test_ds:\n",
    "    test_x.append(x)    \n",
    "    test_pred.append(model.predict(x))\n",
    "    test_y.append(y)\n",
    "\n",
    "test_x = [doc for t in test_x for doc in t.numpy().tolist()]\n",
    "print(\"X: {}\".format(len(test_x)))\n",
    "test_pred = tf.concat(test_pred, axis=0).numpy()\n",
    "print(\"Pred: {}\".format(test_pred.shape))\n",
    "test_y = tf.concat(test_y, axis=0).numpy()\n",
    "print(\"Y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative reviews\n",
      "\n",
      "==================================================\n",
      "buy game high rating promise gameplay saw youtube story so-so graphic mediocre control terrible could not adjust control option preference .. crouch would hold onto left trigger could slip ... also double tap right trigger change weapon suck .. fire weapon require push right button not right trigger often \n",
      "\n",
      "attempt install game quad core windows 7 pc zero luck go back forth try every suggestion rockstar support absolutely useless game defect manufacturer not buy side note 'm also po amazon wo not anything either consumer guess 'm totally unk not right either rockstar amazon refund money terrible customer \n",
      "\n",
      "way product 5 star 28 review write tone lot review similar play 2 song expert drum say unless play tennis shoe fact screw not flush mean feel every kick specifically two screw leave plus pedal completely torn mount screw something actually go wrong pedal instal unscrew send back ea \n",
      "\n",
      "unk interactive stranger unk unk genre develop operation flashpoint various real-life computer sims military folk unk know come deliver good recreation ultra deadly unk modern combat engagement arma attempt simulate `` unk firepower '' soldier combine arm warfare set fictional sprawl island nation conveniently mirror terrain middle eastern country \n",
      "\n",
      "not cup tea \n",
      "\n",
      "\n",
      "Most positive reviews\n",
      "\n",
      "==================================================\n",
      "find something love every final fantasy game play thus far ff13 different really appreciate square enix 's latest trend shake gameplay new release still hammer best look rpg visuals particular generation example unk graphic ff12 absolutely love real-time battle system unique refresh ff13 continue trend excellent manner new paradigm \n",
      "\n",
      "know little squad base game genre know game fun not unk fun obliterate enemy mobile suit satisfy blow zombie head resident evil graphic presentation solid best franchise yes even superior certain next-gen gundam game say gameplay far perfect instance get point sneak enemy repeat receive point stealthy pilot 50 \n",
      "\n",
      "okay hdtv monitor cause ps3 switch game movie widescreen fullscreen every 5 minute someone tell need get hd component cable look price saw brand name sony much money basically name brand pay fancy retail packaging generic cable get quality without fancy packaging name brand embed cable favor save money \n",
      "\n",
      "absolutely phenomenal gaming mouse love programmable size button mouse surprising ergonomic design even though 's side button come software disk take notice due come little plastic unk surface grip 's size gamecube video game 's great software though little complicated love dpi indicator help sooo much dpi set individually \n",
      "\n",
      "first motorstorm come unk racing type one pioneer physic base race every track lot branch path every branch suitable different class vehicle take next level race much bigger apart mud also obstacles individual vehicle class small vehicle get stuck plant unk hazard time lot physic people complain vehicle slide \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_pred = np.argsort(test_pred.flatten())\n",
    "min_pred = sorted_pred[:5]\n",
    "max_pred = sorted_pred[-5:]\n",
    "\n",
    "print(\"Most negative reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in min_pred:    \n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n",
    "    \n",
    "print(\"\\nMost positive reviews\\n\")\n",
    "print(\"=\"*50)\n",
    "for i in max_pred:\n",
    "    print(\" \".join(tokenizer.sequences_to_texts([test_x[i]])), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Training the model with a custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, None, 6609)        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 200)               5448000   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 5,448,201\n",
      "Trainable params: 5,448,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Create a mask to mask out zero inputs\n",
    "    #tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),\n",
    "    # After creating the mask, convert inputs to onehot encoded inputs\n",
    "    tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x,'int32'), depth=n_vocab), input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(256, return_state=False, return_sequences=False),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    \n",
    "    pos_mask = tf.cast(tf.math.equal(y_true, 1),'float32')\n",
    "    n_pos = tf.reduce_sum(pos_mask)\n",
    "    neg_mask = tf.cast(tf.math.equal(y_true, 0),'float32')\n",
    "    n_neg = tf.reduce_sum(neg_mask)\n",
    "    \n",
    "    w_pos = n_neg / (n_pos+n_neg)\n",
    "    w_neg = n_pos / (n_pos+n_neg)\n",
    "    \n",
    "    w_mask = (pos_mask*w_pos) + (neg_mask*w_neg)\n",
    "    \n",
    "    bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    return tf.reduce_mean(bce(y_true, y_pred)*w_mask)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=weighted_binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data pipelines\n",
      "\tDone...\n",
      "Using metric=val_loss and mode=min for EarlyStopping\n",
      "Epoch 1/10\n",
      "4852/4852 [==============================] - 97s 20ms/step - loss: 0.0784 - accuracy: 0.8851 - val_loss: 0.2797 - val_accuracy: 0.7245 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "4852/4852 [==============================] - 98s 20ms/step - loss: 0.0600 - accuracy: 0.9120 - val_loss: 0.2601 - val_accuracy: 0.7491 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "4852/4852 [==============================] - 97s 20ms/step - loss: 0.0513 - accuracy: 0.9246 - val_loss: 0.2841 - val_accuracy: 0.7498 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "4852/4852 [==============================] - 95s 20ms/step - loss: 0.0421 - accuracy: 0.9379 - val_loss: 0.2820 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "4852/4852 [==============================] - 99s 20ms/step - loss: 0.0334 - accuracy: 0.9498 - val_loss: 0.3046 - val_accuracy: 0.7923 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "4852/4852 [==============================] - 99s 20ms/step - loss: 0.0224 - accuracy: 0.9650 - val_loss: 0.4176 - val_accuracy: 0.7763 - lr: 1.0000e-04\n",
      "Epoch 7/10\n",
      "4852/4852 [==============================] - 104s 21ms/step - loss: 0.0190 - accuracy: 0.9694 - val_loss: 0.4324 - val_accuracy: 0.7821 - lr: 1.0000e-04\n",
      "Epoch 8/10\n",
      "4852/4852 [==============================] - 107s 22ms/step - loss: 0.0169 - accuracy: 0.9723 - val_loss: 0.5171 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
      "It took 804.270180940628 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "print(\"Defining data pipelines\")\n",
    "train_ds = get_tf_pipeline(tr_x, tr_y, shuffle=True)\n",
    "valid_ds = get_tf_pipeline(v_x, v_y)\n",
    "test_ds = get_tf_pipeline(ts_x, ts_y)\n",
    "print('\\tDone...')\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','2_sentiment_analysis.log'))\n",
    "\n",
    "monitor_metric = 'val_loss'\n",
    "mode = 'min' if 'loss' in monitor_metric else 'max'\n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=3, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=6, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=10, callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
