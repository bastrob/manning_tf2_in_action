{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "# http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):\n",
    "    url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists(os.path.join('data','lm')):\n",
    "        os.mkdir(os.path.join('data','lm'))\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):\n",
    "    # Write to a file\n",
    "    tarf = tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"))\n",
    "    tarf.extractall(os.path.join(\"data\",\"lm\"))  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    stories = []\n",
    "\n",
    "    with open(path, 'r') as f:    \n",
    "        s = []\n",
    "        start_capture = False    \n",
    "        for row in f:\n",
    "\n",
    "            if start_capture:\n",
    "                s.append(row)\n",
    "\n",
    "            if row.startswith(\"_BOOK_TITLE_\"):\n",
    "                if len(s)>0:\n",
    "                    stories.append(' '.join(s).lower())            \n",
    "                s = []\n",
    "                start_capture=True            \n",
    "\n",
    "    if len(s)>0:\n",
    "        stories.append(' '.join(s).lower())  \n",
    "    \n",
    "    return stories\n",
    "\n",
    "stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\n",
    "val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\n",
    "test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 98 stories (train)\n",
      "Collected 5 stories (valid)\n",
      "Collected 5 stories (test)\n",
      "chapter i. -lcb- chapter heading picture : p1.jpg -rcb- how the fairies were not invited to court .\n",
      "\n",
      "\n",
      " a tale of the tontlawald long , long ago there stood in the midst of a country covered with lakes a \n"
     ]
    }
   ],
   "source": [
    "print(\"Collected {} stories (train)\".format(len(stories)))\n",
    "print(\"Collected {} stories (valid)\".format(len(val_stories)))\n",
    "print(\"Collected {} stories (test)\".format(len(test_stories)))\n",
    "print(stories[0][:100])\n",
    "print('\\n', stories[10][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like chocolates\n",
      "\t1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', 'o', 'l', 'a', 't', 'e', 's']\n",
      "\t2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']\n",
      "\t3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']\n",
      "e     455292\n",
      " t    344971\n",
      "he    310539\n",
      "d     308390\n",
      "th    284425\n",
      " a    268496\n",
      "t     257788\n",
      "s     227961\n",
      " h    192544\n",
      " s    182830\n",
      "dtype: int64\n",
      "\n",
      "Median: 136.5\n",
      "\n",
      "count      1074.000000\n",
      "mean      12106.919926\n",
      "std       36358.817692\n",
      "min           1.000000\n",
      "25%           5.000000\n",
      "50%         136.500000\n",
      "75%        6406.750000\n",
      "90%       34184.600000\n",
      "max      455292.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "def ngrams(text, n):\n",
    "    return [text[i:i+n] for i in range(0,len(text),n)]\n",
    "\n",
    "test_string = \"I like chocolates\"\n",
    "print(\"Original: {}\".format(test_string))\n",
    "for i in list(range(3)):\n",
    "    print(\"\\t{}-grams: {}\".format(i+1, ngrams(test_string, i+1)))\n",
    "    \n",
    "text = chain(*[ngrams(s, 2) for s in stories])\n",
    "cnt = Counter(text)\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "print(freq_df.head(n=10))\n",
    "print(\"\\nMedian: {}\\n\".format(freq_df.median()))\n",
    "print(freq_df.describe(percentiles=[0.25,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 735\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df>=10).sum()\n",
    "print(\"Size of vocabulary: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "n = 2 #ngram length\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "train_ngram_stories = [ngrams(s,n) for s in stories]\n",
    "tokenizer.fit_on_texts(train_ngram_stories)\n",
    "\n",
    "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
    "\n",
    "val_ngram_stories = [ngrams(s,n) for s in val_stories]\n",
    "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
    "\n",
    "test_ngram_stories = [ngrams(s,n) for s in test_stories]\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_pipeline(data_seq, n_seq, batch_size=64, stride=1, shuffle=True):\n",
    "        \n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq)) # tf.ragged.constant(data_seq)\n",
    "    \n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
    "        \n",
    "    text_ds = text_ds.flat_map(lambda x: tf.data.Dataset.from_tensor_slices(x).window(n_seq+1,shift=stride)    )\n",
    "    text_ds = text_ds.flat_map(lambda window: window.batch(n_seq+1, drop_remainder=True))\n",
    "    \n",
    "    #text_ds = text_ds.unbatch()\n",
    "    \n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "    \n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    text_ds = tf.data.Dataset.zip(text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return text_ds\n",
    "    #return x,y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 8, 12,  3, 12,  2],\n",
      "       [11,  2,  9,  2,  4],\n",
      "       [ 4,  3, 11,  2,  9],\n",
      "       [28,  3, 10,  4,  2],\n",
      "       [12,  2, 10, 14, 16],\n",
      "       [16, 16,  3, 11,  2]], dtype=int32)>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[12,  3, 12,  2,  5],\n",
      "       [ 2,  9,  2,  4,  7],\n",
      "       [ 3, 11,  2,  9,  2],\n",
      "       [ 3, 10,  4,  2,  9],\n",
      "       [ 2, 10, 14, 16, 16],\n",
      "       [16,  3, 11,  2,  9]], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[29, 29,  2,  7,  5],\n",
      "       [ 2,  4,  7,  3,  2],\n",
      "       [ 8,  3,  2, 21,  2],\n",
      "       [ 2, 20,  7,  5,  8],\n",
      "       [14, 16, 16,  3, 11],\n",
      "       [23,  4,  3, 11,  2]], dtype=int32)>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[29,  2,  7,  5, 11],\n",
      "       [ 4,  7,  3,  2, 10],\n",
      "       [ 3,  2, 21,  2, 26],\n",
      "       [20,  7,  5,  8, 17],\n",
      "       [16, 16,  3, 11,  2],\n",
      "       [ 4,  3, 11,  2,  9]], dtype=int32)>)\n",
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[12,  2,  5,  8, 12],\n",
      "       [11,  2,  9, 10,  2],\n",
      "       [21,  2, 26, 26,  2],\n",
      "       [ 7,  5,  8, 17,  3],\n",
      "       [ 3,  8, 12,  3, 12],\n",
      "       [17,  3,  2, 29, 29]], dtype=int32)>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 2,  5,  8, 12,  2],\n",
      "       [ 2,  9, 10,  2, 17],\n",
      "       [ 2, 26, 26,  2, 36],\n",
      "       [ 5,  8, 17,  3,  2],\n",
      "       [ 8, 12,  3, 12,  2],\n",
      "       [ 3,  2, 29, 29,  2]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "ds = get_tf_pipeline(train_data_seq, 5, batch_size=6)\n",
    "\n",
    "for a in ds.take(3):\n",
    "\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1708.02182.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \"\"\"\n",
    "    USAGE NOTICE: this metric accepts only logits for now (i.e. expect the same behaviour as from tf.keras.losses.SparseCategoricalCrossentropy with the a provided argument \"from_logits=True\", \n",
    "\t\there the same loss is used with \"from_logits=True\" enforced so you need to provide it in such a format)\n",
    "    METRIC DESCRIPTION:\n",
    "    Popular metric for evaluating language modelling architectures.\n",
    "    More info: http://cs224d.stanford.edu/lecture_notes/LectureNotes4.pdf.\n",
    "    DISCLAIMER: Original function created by Kirill Mavreshko in https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/example/run_gpt.py#L106. \n",
    "    My \"contribution\": I converted Kirill method's logic (and added a padding masking to to it) into this new Tensorflow 2.0 way of doing things via a stateful \"Metric\" object. This required making the metric a fully-fledged object by subclassing the Metric class. \n",
    "    \"\"\"\n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super(PerplexityMetric, self).__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      # The next 4 lines zero-out the padding from loss calculations, \n",
    "      # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t\n",
    "      #mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      #print(loss_.shape)\n",
    "      #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "      #loss_ *= mask\n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      step2 = K.exp(step1)\n",
    "      perplexity = K.mean(step2)\n",
    "      #perplexity = K.exp(K.mean(loss_))\n",
    "      \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      super(PerplexityMetric, self).update_state(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 512)         376832    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 1024)        4724736   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 512)         524800    \n",
      "_________________________________________________________________\n",
      "final_out (Dense)            (None, None, 735)         377055    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 735)         0         \n",
      "=================================================================\n",
      "Total params: 6,003,423\n",
      "Trainable params: 6,003,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "K.clear_session()\n",
    "n_seq=100\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n",
    "    #tf.keras.layers.GRU(512, return_state=False, return_sequences=True),\n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    #tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.8)),\n",
    "    #tf.keras.layers.Dense(512, activation='relu'),\n",
    "    #tf.keras.layers.TimeDistributed(tf.keras.layers.Dropout(0.8)),\n",
    "    tf.keras.layers.Dense(n_vocab, name='final_out'),\n",
    "    tf.keras.layers.Activation(activation='softmax')\n",
    "])\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2082006, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p = PerplexityMetric()\n",
    "true = [[0, 1,2],[0, 1,2]]\n",
    "pred = [[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]],[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]]]\n",
    "p.update_state(true, pred)\n",
    "print(p.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU got to around 35% accuracy and 39 validation perplexity\n",
    "\n",
    "5873 steps 128 batch size 50 stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_perplexity and mode=min for EarlyStopping\n",
      "Epoch 1/50\n",
      "2936/2936 [==============================] - 341s 116ms/step - loss: 2.3857 - accuracy: 0.4313 - perplexity: 15.2877 - val_loss: 2.4790 - val_accuracy: 0.4113 - val_perplexity: 12.6962 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2936/2936 [==============================] - 343s 117ms/step - loss: 2.0692 - accuracy: 0.4853 - perplexity: 8.4284 - val_loss: 2.4818 - val_accuracy: 0.4150 - val_perplexity: 12.8089 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2936/2936 [==============================] - 344s 117ms/step - loss: 2.0456 - accuracy: 0.4889 - perplexity: 8.2086 - val_loss: 2.4728 - val_accuracy: 0.4130 - val_perplexity: 12.6940 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2936/2936 [==============================] - 345s 117ms/step - loss: 2.0390 - accuracy: 0.4891 - perplexity: 8.1284 - val_loss: 2.4252 - val_accuracy: 0.4215 - val_perplexity: 12.0610 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2936/2936 [==============================] - 347s 118ms/step - loss: 2.0380 - accuracy: 0.4885 - perplexity: 8.0946 - val_loss: 2.4085 - val_accuracy: 0.4273 - val_perplexity: 11.9038 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2936/2936 [==============================] - 345s 118ms/step - loss: 2.0324 - accuracy: 0.4891 - perplexity: 8.0472 - val_loss: 2.4290 - val_accuracy: 0.4226 - val_perplexity: 12.1469 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2936/2936 [==============================] - 337s 115ms/step - loss: 2.0316 - accuracy: 0.4886 - perplexity: 8.0190 - val_loss: 2.3837 - val_accuracy: 0.4278 - val_perplexity: 11.5808 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2936/2936 [==============================] - 333s 114ms/step - loss: 2.0386 - accuracy: 0.4867 - perplexity: 8.0632 - val_loss: 2.3939 - val_accuracy: 0.4268 - val_perplexity: 11.7099 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2936/2936 [==============================] - 331s 113ms/step - loss: 2.0346 - accuracy: 0.4872 - perplexity: 8.0271 - val_loss: 2.3929 - val_accuracy: 0.4320 - val_perplexity: 11.6827 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 2.0752 - accuracy: 0.4750 - perplexity: 8.2709 - val_loss: 2.2333 - val_accuracy: 0.4550 - val_perplexity: 9.9208 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 2.0099 - accuracy: 0.4875 - perplexity: 7.7175 - val_loss: 2.2053 - val_accuracy: 0.4639 - val_perplexity: 9.7113 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9782 - accuracy: 0.4940 - perplexity: 7.4629 - val_loss: 2.2219 - val_accuracy: 0.4588 - val_perplexity: 9.8068 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9527 - accuracy: 0.4993 - perplexity: 7.2663 - val_loss: 2.2132 - val_accuracy: 0.4616 - val_perplexity: 9.7681 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9496 - accuracy: 0.4984 - perplexity: 7.2252 - val_loss: 2.1881 - val_accuracy: 0.4662 - val_perplexity: 9.5086 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9349 - accuracy: 0.5020 - perplexity: 7.1151 - val_loss: 2.1880 - val_accuracy: 0.4666 - val_perplexity: 9.5139 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9284 - accuracy: 0.5036 - perplexity: 7.0667 - val_loss: 2.1921 - val_accuracy: 0.4656 - val_perplexity: 9.5717 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9254 - accuracy: 0.5040 - perplexity: 7.0432 - val_loss: 2.1885 - val_accuracy: 0.4668 - val_perplexity: 9.5469 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "2936/2936 [==============================] - 327s 112ms/step - loss: 1.9238 - accuracy: 0.5044 - perplexity: 7.0315 - val_loss: 2.1881 - val_accuracy: 0.4670 - val_perplexity: 9.5387 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9230 - accuracy: 0.5046 - perplexity: 7.0259 - val_loss: 2.1880 - val_accuracy: 0.4670 - val_perplexity: 9.5393 - lr: 1.0000e-07\n",
      "It took 6348.724717617035 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_tf_pipeline(train_data_seq[:50], n_seq, stride=25, batch_size=128)\n",
    "valid_ds = get_tf_pipeline(val_data_seq, n_seq, stride=n_seq, batch_size=128)\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.log'))\n",
    "\n",
    "monitor_metric = 'val_perplexity'\n",
    "mode = 'min' \n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):  \n",
    "    if epoch==0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "#lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=5, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, epochs=50, \n",
    "          validation_data = valid_ds,\n",
    "          callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(os.path.join('models', '2_gram_lm.h5'), compile=False)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    376832      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, None, 1024), 4724736     embedding[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 512)    524800      gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "final_out (Dense)               (None, None, 735)    377055      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 735)    0           final_out[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,003,423\n",
      "Trainable params: 6,003,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "inp_state = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "emb_layer = tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,))\n",
    "emb_out = emb_layer(inp)\n",
    "    # Defining an LSTM layer\n",
    "gru_layer = tf.keras.layers.GRU(1024, return_state=True, return_sequences=True)\n",
    "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
    "\n",
    "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
    "dense_out = dense_layer(gru_out)\n",
    "final_layer = tf.keras.layers.Dense(n_vocab, name='final_out')\n",
    "final_out = final_layer(dense_out)\n",
    "softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
    "\n",
    "infer_model = tf.keras.models.Model(inputs=[inp, inp_state], outputs=[softmax_out, gru_state])\n",
    "\n",
    "emb_layer.set_weights(model.get_layer('embedding').get_weights())\n",
    "gru_layer.set_weights(model.get_layer('gru').get_weights())\n",
    "dense_layer.set_weights(model.get_layer('dense').get_weights())\n",
    "final_layer.set_weights(model.get_layer('final_out').get_weights())\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 173 predictions from input\n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank  , and of having nothing to do : once or twice she had peeped into the book her sister was reading , but it had no pictures or conversations in it , ` and what is the use of a book , ' thought alice ` without pictures or conversation ? ' unkunk. h. m. h. that 's all . ''\n",
      " `` i do n't know what it is , '' said the story girl , `` i 'm going to see them , and i 'l n't wait till i 'm going to take yourself on the water .\n",
      " i 'm going to see you again , and i 'll tell you what it is . ''\n",
      " ` whatsoeve , it 's all right , '' said mrs. jo , standing up and down they wended .\n",
      " `` therefore-rings them , '' said mrs. jo , smiling at the little girl .\n",
      " `` i 'm going to bed , '' said the king , `` and i 'm sure you 'll be able to discover what is the matter ... and i 'll tell younda williams , and i 'll be able to do it ... i 'm going to say that i have n't anythin ' that i have n't anything to do with them .\n",
      " i 'm going to say that , '' said the king , ` if you do n't know what i want you to die , and i 'll telegraph them to the place where you can .\n",
      " but i do n't know what it is .\n",
      " i 'm sure i can not bear to be anythings .\n",
      " i 'm sure-i think that is that there is nothing to do with them .\n",
      " i 'm not goining to see him at all .\n",
      " i 've\n"
     ]
    }
   ],
   "source": [
    "text = ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank  , and of having nothing to do : once or twice she had peeped into the book her sister was reading , but it had no pictures or conversations in it , ` and what is the use of a book , ' thought Alice ` without pictures or conversation ? ' \".lower(), \n",
    "    n\n",
    ")\n",
    "\n",
    "# CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making {} predictions from input\".format(len(seq[0])))\n",
    "\n",
    "model.reset_states()\n",
    "state = np.zeros(shape=(1,1024))\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state])\n",
    "\n",
    "# get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "x = np.array([[wid]])\n",
    "\n",
    "for _ in range(500):\n",
    "\n",
    "    out, state = infer_model.predict([x, state])\n",
    "    # Get the word id and the word\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()    \n",
    "    #i = np.random.choice(list(range(-2,0)), p=out_argsort[-2:]/out_argsort[-2:].sum())    \n",
    "    wid = int(out_argsort[-1])    \n",
    "    word = tokenizer.index_word[wid]\n",
    "    \n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 3\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = tokenizer.index_word[wid]\n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    \n",
    "    # recursively make the current prediction the next input\n",
    "    x = np.array([[wid]])\n",
    "    \n",
    "    \n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_one_step(model, input_, state):    \n",
    "    #print('a',input_,)\n",
    "    #print('b',state)\n",
    "    output, new_state = model.predict([input_, state])\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "\n",
    "def beam_search_v2(model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True):\n",
    "    \n",
    "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
    "        \n",
    "        if i == beam_depth:\n",
    "            # Return the results            \n",
    "            #print('end', sequence)\n",
    "            results.append((list(sequence), state, np.exp(log_prob)))            \n",
    "            return sequence, log_prob, state\n",
    "        else:\n",
    "            output, new_state = beam_one_step(model, input_, state)\n",
    "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "            #print(top_probs, top_ids)\n",
    "            for p, wid in zip(top_probs, top_ids):\n",
    "                # we are going to assigne a very low probability whenever the same symbol is repeating                                \n",
    "                new_log_prob = log_prob + np.log(p)\n",
    "                if len(sequence)>0 and wid == sequence[-1]:\n",
    "                    new_log_prob = new_log_prob + np.log(5e-1)\n",
    "                    \n",
    "                sequence.append(wid)                \n",
    "                _ = recursive_fn(np.array([[wid]]), new_state, sequence, new_log_prob, i+1)                                         \n",
    "                sequence.pop()\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, state, sequence, log_prob, 0)    \n",
    "\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 54 predictions from input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and there was no reason that her father had brought him the story girl 's face .\n",
      " `` i 'm going to bed , '' said the prince , `` and you can not be able to do it . ''\n",
      " `` i 'm sure i shall have to go to bed , '' he answered , with a smile .\n",
      " `` i 'm so happy , '' she said .\n",
      " `` i do n't know how to bring you into the world , and i 'll be sure that you would have thought that it would have been a long time .\n",
      " there was no time to be able to do it , and it would have been a little thing . ''\n",
      " `` i do n't know , '' she said .\n",
      " `` i 'm so glad you come back . ''\n",
      " `` i do n't know what to do with you , '' said the princess , who would not have been able to do that .\n",
      " `` well , i do n't know , '' said anne .\n",
      " `` i 'm going to believe that i have n't anything more if i could , and i 'm glad you have a good deal .\n",
      " that is what it is . ''\n",
      " `` i 'm glad of it , '' said mr. meredith , `` and then you will be so good to me , and that is why they are all right .\n",
      " i 'm not going to be there , but i do n't want anything exciting . ''\n",
      " `` what is the matter with you ? ''\n",
      " `` i do n't know what it is . ''\n",
      " `` i do n't know , '' said the princess , who had been a good deal of father .\n",
      " `` i 'm going to bed , '' said the king .\n",
      " `` what is the matter ? ''\n",
      " `` no , '' said anne , with a smile .\n",
      " `` i do n't know what to do , '' said mary .\n",
      " `` i 'm so glad you come back , '' said mrs. march , with\n"
     ]
    }
   ],
   "source": [
    "text = ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(), \n",
    "    #\"CHAPTER I\".lower(),\n",
    "    n\n",
    ")\n",
    "\n",
    "# CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making {} predictions from input\".format(len(seq[0])))\n",
    "\n",
    "#model.reset_states()\n",
    "state = np.zeros(shape=(1,1024))\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state])\n",
    "\n",
    "# get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "x = np.array([[wid]])\n",
    "\n",
    "for i in range(100):    \n",
    "    result = beam_search_v2(infer_model, x, state, 7, 2)\n",
    "    \n",
    "    #best_beam_ids, state, _  = result[0]       \n",
    "    \n",
    "    # to reduce importance of high prob ones\n",
    "    # if the same sequence is repeating we'll avoid it explicitly            \n",
    "    n_probs = np.array([p for _,_,p in result[:10]])\n",
    "    p_j = np.random.choice(list(range(n_probs.size)), p=n_probs/n_probs.sum())                    \n",
    "    best_beam_ids, state, _ = result[p_j]\n",
    "    x = np.array([[best_beam_ids[-1]]])\n",
    "            \n",
    "    text.extend([tokenizer.index_word[w] for w in best_beam_ids])    \n",
    "\n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def beam_search(model, input_, state, beam_depth=5, beam_width=3):\n",
    "    \n",
    "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
    "        output, state = beam_one_step(model, input_, state)\n",
    "        top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "        top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "        #print(top_probs, top_ids)\n",
    "        while i < beam_depth:\n",
    "            i += 1\n",
    "            for p, wid in zip(top_probs, top_ids):\n",
    "                log_prob += np.log(p)\n",
    "                \n",
    "                if log_prob > np.log(1e-6):                \n",
    "                    sequence.append(wid)\n",
    "                    sequence, prob, state = recursive_fn(np.array([[wid]]), state, sequence, log_prob, i)\n",
    "                    #print(sequence)\n",
    "                    if len(sequence)==beam_depth:\n",
    "                        results.append((list(sequence), state, np.exp(log_prob)))\n",
    "\n",
    "                    sequence.pop()\n",
    "\n",
    "                log_prob -= np.log(p)\n",
    "                \n",
    "        return sequence, log_prob, state\n",
    "    \n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, state, sequence, log_prob, 0)    \n",
    "    #print(results)\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
