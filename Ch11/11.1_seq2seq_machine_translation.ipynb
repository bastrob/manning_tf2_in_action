{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11/11.1_Seq2seq_machine_translation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (227080, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[1000:].sample(n=50000, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `SOS` and `EOS` tokens (Decoder)\n",
    "\n",
    "We will add these special tokens to the translated targets. `sos` indicates the start of the sentence and `eos` marks the end of the sentence. \n",
    "\n",
    "E.g. `Grüß Gott!` becomes `sos Grüß Gott! eos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DE\"] = 'sos ' + df[\"DE\"] + ' eos'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=5000, random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=5000, random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary sizes (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9506\n",
      "to     8831\n",
      "I      8368\n",
      "the    6982\n",
      "you    6076\n",
      "a      5653\n",
      "is     4311\n",
      "in     2733\n",
      "of     2676\n",
      "was    2318\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2249\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      40000\n",
      "eos      40000\n",
      "Tom       9968\n",
      "Ich       7813\n",
      "ist       4668\n",
      "nicht     4576\n",
      "zu        3712\n",
      "Sie       3380\n",
      "du        3083\n",
      "das       2933\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2456\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.371500\n",
      "std          2.616927\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max        101.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39545.000000\n",
      "mean         6.249766\n",
      "std          2.313737\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.00000\n",
      "mean         8.40025\n",
      "std          2.58841\n",
      "min          3.00000\n",
      "25%          7.00000\n",
      "50%          8.00000\n",
      "75%         10.00000\n",
      "max         77.00000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39271.000000\n",
      "mean         8.307708\n",
      "std          2.279042\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vocabulary size and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2249\n",
      "DE vocabulary size: 2456\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "\n",
    "# Create the text vectorization layer (English)\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Here we are calling adapt to fit the vectorization layer with text\n",
    "# so that it learns the vocabulary\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "\n",
    "# Create the text vectorization layer (German)\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,    \n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextVectorization` layer in action\n",
    " \n",
    "### How to use the layer (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "[['run'], [\"I'll go home\"], ['ectoplasmic residue']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[486   0   0]\n",
      " [ 79  47 106]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"run\"], [\"I\\'ll go home\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the layer (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "[['[sos] Geh'], ['geh lauf']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[  2 825   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [825   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(de_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"[sos] Geh\"], [\"geh lauf\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2249\n",
      "\n",
      "German\n",
      "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'das', 'du']\n",
      "2456\n"
     ]
    }
   ],
   "source": [
    "print(\"English\")\n",
    "# Print first few words in the vocabulary\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n",
    "\n",
    "print(\"\\nGerman\")\n",
    "# Print first few words in the vocabulary\n",
    "print(de_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(de_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Seq2Seq model\n",
    "\n",
    "Encoder decoder\n",
    "Uses internal vectorizers in both encoder and decoder\n",
    "Bidirectional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_vectorizer(list_of_strings, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,                \n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "    \n",
    "        \n",
    "def get_encoder_and_state(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, name='e_gru'), name='e_bidirectional_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
    "        \n",
    "    # We are also returning the final state of the encoder as that\n",
    "    # is passed as the initial state to the decoder\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    # Encoder's input\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')    \n",
    "    # Get the encoders final output\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    # Define an intermediate dense layer\n",
    "    dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    dense1_out = dense_layer_1(gru_out)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
    "    dense_final_out = dense_layer_final(dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[e_inp, inp], outputs=dense_final_out, name='final_seq2seq')\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder = get_encoder_and_state(en_vocab, en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_input (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_vectorizer (Functional)       (None, 20)           0           d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "e_input_final (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_embedding (Embedding)         (None, 20, 128)      314624      d_vectorizer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, 256)          486272      e_input_final[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "d_gru (GRU)                     (None, 20, 256)      296448      d_embedding[0][0]                \n",
      "                                                                 encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_1 (Dense)               (None, 20, 512)      131584      d_gru[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_final (Dense)           (None, 20, 2458)     1260954     d_dense_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,489,882\n",
      "Trainable params: 2,489,882\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MT models - BLEU metric\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
    "\n",
    "### Defining the BLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            t = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"eos.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            t = [doc if len(doc)>0 else '[UNK]' for doc in t ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            t = np.char.split(t).tolist()\n",
    "            \n",
    "            return t\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]\n",
    "        \n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7598356856515925,\n",
       " [0.8, 0.7777777777777778, 0.75, 0.7142857142857143],\n",
       " 1.0,\n",
       " 1.0,\n",
       " 10,\n",
       " 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = [['[UNK]', 'einmal', 'mÃ¼ssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃ¼ssen', 'mÃ¼ssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "compute_bleu(reference, translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with a custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 39/39\n",
      "Epoch 1/5\n",
      "\t(train) loss: 1.7673478011901562 - accuracy: 0.24513264504285195 - bleu: 0.002179387607593193\n",
      "\t(valid) loss: 1.4485881817646515 - accuracy: 0.33110261345520997 - bleu: 0.01199333936034144\n",
      "Evaluating batch 39/39\n",
      "Epoch 2/5\n",
      "\t(train) loss: 1.3196139595447443 - accuracy: 0.3695648503609193 - bleu: 0.030331061497335617\n",
      "\t(valid) loss: 1.212600277020381 - accuracy: 0.40372603902449977 - bleu: 0.047088649383548294\n",
      "Evaluating batch 39/39\n",
      "Epoch 3/5\n",
      "\t(train) loss: 1.1058441591568482 - accuracy: 0.43287440026417756 - bleu: 0.06973255766539602\n",
      "\t(valid) loss: 1.0736533525662544 - accuracy: 0.4464820676889175 - bleu: 0.07360283948444667\n",
      "Evaluating batch 39/39\n",
      "Epoch 4/5\n",
      "\t(train) loss: 0.947786437968413 - accuracy: 0.4818474271167547 - bleu: 0.10347354581510186\n",
      "\t(valid) loss: 0.9735986498685983 - accuracy: 0.48121631680390775 - bleu: 0.10747072579117517\n",
      "Evaluating batch 39/39\n",
      "Epoch 5/5\n",
      "\t(train) loss: 0.8230568886949465 - accuracy: 0.5244687414513185 - bleu: 0.13671692051662485\n",
      "\t(valid) loss: 0.8996234964101743 - accuracy: 0.509787013897529 - bleu: 0.1232997804591892\n",
      "\t(test) loss: 0.9086625560736045 - accuracy: 0.5111884642869998 - bleu: 0.12581887340240752\n",
      "\n",
      "It took 258.0026841163635 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
    "\n",
    "\n",
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, epochs, batch_size):\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    \n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of validation batches\n",
    "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "\n",
    "    # Evaluate one validation batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update validation logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n",
    "    \n",
    "        \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\" Training the model \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "\n",
    "    shuffle_inds = None\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)        \n",
    "            # Evaluate the model to get the metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            # Get the final prediction to compute BLEU\n",
    "            pred_y = model.predict(x)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
    "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
    "        val_de_labels = data_dict['valid']['decoder_labels']\n",
    "            \n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model, vectorizer, val_en_inputs, val_de_inputs, val_de_labels, epochs, batch_size\n",
    "        )\n",
    "            \n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "        print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
    "        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(val_loss, val_accuracy, val_bleu))\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #    \n",
    "    \n",
    "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
    "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
    "    test_de_labels = data_dict['test']['decoder_labels']\n",
    "            \n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model, vectorizer, test_en_inputs, test_de_inputs, test_de_labels, epochs, batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\t(test) loss: {} - accuracy: {} - bleu: {}\".format(test_loss, test_accuracy, test_bleu))\n",
    "\n",
    "t1 = time.time()    \n",
    "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_inference_model(save_path):\n",
    "    \n",
    "    model = tf.keras.models.load_model(os.path.join('models', 'seq2seq'))\n",
    "    \n",
    "    en_model = model.get_layer(\"encoder\")\n",
    "    \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
    "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n",
    "    \n",
    "    d_vec_layer = model.get_layer('d_vectorizer')    \n",
    "    d_vec_out = d_vec_layer(inp)\n",
    "    \n",
    "    d_emb_out = model.get_layer('d_embedding')(d_vec_out)\n",
    "        \n",
    "    d_gru_layer = model.get_layer(\"d_gru\")#.get_weights()\n",
    "    \n",
    "    #d_gru_layer = tf.keras.layers.GRU(256)    \n",
    "    #d_gru_layer.initial_state = d_state_inp\n",
    "    d_gru_layer.return_sequences = False\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp) \n",
    "    \n",
    "    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out) \n",
    "    \n",
    "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out) \n",
    "    \n",
    "    de_model = tf.keras.models.Model(inputs=[inp, d_state_inp], outputs=[d_final_out, d_gru_out])\n",
    "    \n",
    "    return en_model, de_model\n",
    "\n",
    "\n",
    "en_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Tom seems to think that he's invincible.\n",
      "Translation: tom scheint zu [UNK] zu sein eos\n",
      "\n",
      "Input: I don't want to go to jail.\n",
      "Translation: ich will nicht nach boston gehen eos\n",
      "\n",
      "Input: Tom needs to act quickly.\n",
      "Translation: tom muss sich [UNK] eos\n",
      "\n",
      "Input: Tom didn't know what Mary was talking about.\n",
      "Translation: tom wusste nicht was mary zu helfen hat eos\n",
      "\n",
      "Input: I respect you for what you have done.\n",
      "Translation: ich bin froh was du es getan hast eos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_new_translation(en_model, de_model, sample_en_text):\n",
    "\n",
    "    print(\"Input: {}\".format(sample_en_text))\n",
    "\n",
    "    d_state = en_model.predict(np.array([sample_en_text]))\n",
    "    de_word = 'sos'\n",
    "    de_translation = []\n",
    "\n",
    "    while de_word != 'eos':\n",
    "\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state])    \n",
    "        de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "        de_translation.append(de_word)\n",
    "\n",
    "    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n",
    "\n",
    "for i in range(5):\n",
    "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
    "    generate_new_translation(en_model, de_model, sample_en_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_vectorizer(list_of_strings, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,                \n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "    \n",
    "        \n",
    "def get_encoder_with_attention(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.GRU(128, return_sequences=True, return_state=True, name='e_gru'), \n",
    "        name='e_bidirectional_gru'\n",
    "    )\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out_sequence, gru_fwd_out, gru_bwd_out = gru_layer(emb_out)\n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=[gru_fwd_out, gru_bwd_out, gru_out_sequence], name='encoder')\n",
    "        \n",
    "    # We are also returning the final state of the encoder as that\n",
    "    # is passed as the initial state to the decoder\n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_final_seq2seq_model_with_attention(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')    \n",
    "    fwd_state, bwd_state, en_states = encoder(e_inp)\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    d_init_state = tf.keras.layers.Concatenate(axis=-1)([fwd_state, bwd_state])\n",
    "    de_state = gru_layer(emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    attn_layer = AttentionLayer(name='attention_layer')\n",
    "    attn_out, attn_states = attn_layer([en_states, de_state])\n",
    "    \n",
    "    #print(attn_out.shape)\n",
    "    #print(de_state.shape)\n",
    "    decoder_concat_input = tf.keras.layers.Concatenate(axis=-1, name='concat_layer')([de_state, attn_out])\n",
    "\n",
    "    # Define an intermediate dense layer\n",
    "    dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    dense1_out = dense_layer_1(decoder_concat_input)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
    "    dense_final_out = dense_layer_final(dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[e_inp, inp], outputs=dense_final_out, name='final_seq2seq')\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder = get_encoder_with_attention(en_vocab, en_vectorizer)\n",
    "final_model_with_attention = get_final_seq2seq_model_with_attention(de_vocab, encoder, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_input (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "e_input_final (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_vectorizer (Functional)       (None, 20)           0           d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            [(None, 128), (None, 486272      e_input_final[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "d_embedding (Embedding)         (None, 20, 128)      314624      d_vectorizer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 256)          0           encoder[0][0]                    \n",
      "                                                                 encoder[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "d_gru (GRU)                     (None, 20, 256)      296448      d_embedding[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, 20, 256), (N 131328      encoder[0][2]                    \n",
      "                                                                 d_gru[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, 20, 512)      0           d_gru[0][0]                      \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_1 (Dense)               (None, 20, 512)      262656      concat_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_final (Dense)           (None, 20, 2458)     1260954     d_dense_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,752,282\n",
      "Trainable params: 2,752,282\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "final_model_with_attention.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model_with_attention.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 39/39\n",
      "Epoch 1/5\n",
      "\t(train) loss: 1.7329568889660714 - accuracy: 0.2647226378799249 - bleu: 0.00472143272555505\n",
      "\t(valid) loss: 1.4059809904832106 - accuracy: 0.3558326141956525 - bleu: 0.02620224793187979\n",
      "Evaluating batch 39/39\n",
      "Epoch 2/5\n",
      "\t(train) loss: 1.2265979938018017 - accuracy: 0.4132461631909395 - bleu: 0.0649594651608318\n",
      "\t(valid) loss: 1.0886339698082361 - accuracy: 0.46195775728959304 - bleu: 0.09881763329497112\n",
      "Evaluating batch 39/39\n",
      "Epoch 3/5\n",
      "\t(train) loss: 0.9293508781836584 - accuracy: 0.5138479712872933 - bleu: 0.14285005496217076\n",
      "\t(valid) loss: 0.8862711420426002 - accuracy: 0.529658331320836 - bleu: 0.13323894577232187\n",
      "Evaluating batch 39/39\n",
      "Epoch 4/5\n",
      "\t(train) loss: 0.7369613229082181 - accuracy: 0.586090661967412 - bleu: 0.20732611939912957\n",
      "\t(valid) loss: 0.7729042202998431 - accuracy: 0.5811995833348005 - bleu: 0.2061423912444988\n",
      "Evaluating batch 39/39\n",
      "Epoch 5/5\n",
      "\t(train) loss: 0.6236180976415292 - accuracy: 0.6333687748664465 - bleu: 0.2576824772799462\n",
      "\t(valid) loss: 0.7105109630486904 - accuracy: 0.6054706986133869 - bleu: 0.21524254015938163\n",
      "\t(test) loss: 0.7193389687782679 - accuracy: 0.6081753556544964 - bleu: 0.22349188939727838\n",
      "\n",
      "It took 320.21411895751953 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "t1 = time.time()\n",
    "train_model(final_model_with_attention, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq_attention/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq_attention/assets\n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model_with_attention, os.path.join('models', 'seq2seq_attention'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "os.makedirs(os.path.join('models', 'seq2seq_attention_vocab'), exist_ok=True)\n",
    "with open(os.path.join('models', 'seq2seq_attention_vocab', 'de_vocab.json'), 'w') as f:\n",
    "    json.dump(de_vocabulary, f)\n",
    "    \n",
    "with open(os.path.join('models', 'seq2seq_attention_vocab', 'en_vocab.json'), 'w') as f:\n",
    "    json.dump(en_vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_en_text = test_df[\"EN\"].iloc[2]\n",
    "print(\"Input: {}\".format(sample_en_text))\n",
    "\n",
    "d_state = en_model.predict(np.array([sample_en_text]))\n",
    "de_word = 'sos'\n",
    "de_translation = []\n",
    "\n",
    "while de_word != 'eos':\n",
    "    \n",
    "    de_pred, d_state = de_model.predict([np.array([de_word]), d_state])    \n",
    "    de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "    de_translation.append(de_word)\n",
    "\n",
    "print(\"Translation: {}\".format(' '.join(de_translation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing attention patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
