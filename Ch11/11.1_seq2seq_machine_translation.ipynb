{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11/11.1_Seq2seq_machine_translation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (227080, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=50000, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `[SOS]` and `[EOS]` tokens\n",
    "\n",
    "We will add these special tokens to the translated targets. `[SOS]` indicates the start of the sentence and `[EOS]` marks the end of the sentence.\n",
    "\n",
    "E.g. `Grüß Gott!` becomes `[SOS] Grüß Gott! [EOS]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DE\"] = '[SOS] ' + df[\"DE\"] + ' [EOS]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=5000, random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=5000, random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary sizes (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9427\n",
      "to     8673\n",
      "I      8436\n",
      "the    6999\n",
      "you    6125\n",
      "a      5680\n",
      "is     4374\n",
      "in     2664\n",
      "of     2613\n",
      "was    2298\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2238\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "[SOS]    40000\n",
      "[EOS]    40000\n",
      "Tom       9928\n",
      "Ich       7749\n",
      "ist       4753\n",
      "nicht     4414\n",
      "zu        3583\n",
      "Sie       3465\n",
      "du        3112\n",
      "das       2909\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2497\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length (English and German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         6.360650\n",
      "std          2.667726\n",
      "min          1.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max        101.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39504.000000\n",
      "mean         6.228002\n",
      "std          2.328172\n",
      "min          2.000000\n",
      "25%          5.000000\n",
      "50%          6.000000\n",
      "75%          8.000000\n",
      "max         14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 8.0\n",
      "\n",
      "count    40000.000000\n",
      "mean         8.397875\n",
      "std          2.652027\n",
      "min          3.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         77.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    39166.000000\n",
      "mean         8.299035\n",
      "std          2.291474\n",
      "min          5.000000\n",
      "25%          7.000000\n",
      "50%          8.000000\n",
      "75%         10.000000\n",
      "max         16.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vocabulary size and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2238\n",
      "DE vocabulary size: 2497\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `TextVectorization` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "\n",
    "# Create the text vectorization layer (English)\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Here we are calling adapt to fit the vectorization layer with text\n",
    "# so that it learns the vocabulary\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "# Create the text vectorization layer (German)\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextVectorization` layer in action\n",
    "\n",
    "### How to use the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "[['run'], ['how are you'], ['ectoplasmic residue']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[427   0   0]\n",
      " [ 40  23   4]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"run\"], [\"how are you\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2238\n"
     ]
    }
   ],
   "source": [
    "# Print first few words in the vocabulary\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(en_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'sos', 'eos', 'ich', 'tom', 'nicht', 'ist', 'das', 'du']\n",
      "2497\n"
     ]
    }
   ],
   "source": [
    "# Print first few words in the vocabulary\n",
    "print(de_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(de_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Seq2Seq model\n",
    "\n",
    "Encoder decoder\n",
    "Uses internal vectorizers in both encoder and decoder\n",
    "Bidirectional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_vectorizer(list_of_strings, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,        \n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out), vectorize_layer.get_vocabulary()        \n",
    "    \n",
    "        \n",
    "def get_encoder_and_state(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128))\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "        \n",
    "    # We are also returning the final state of the encoder as that\n",
    "    # is passed as the initial state to the decoder\n",
    "    return encoder, gru_out\n",
    "\n",
    "\n",
    "def get_final_seq2seq_model(n_vocab, encoder, init_state, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    gru_layer = tf.keras.layers.GRU(256, return_sequences=True)\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out, initial_state=init_state)\n",
    "    \n",
    "    # Define an intermediate dense layer\n",
    "    dense_layer_1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "    dense1_out = dense_layer_1(gru_out)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax')\n",
    "    dense_final_out = dense_layer_final(dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[encoder.input, inp], outputs=dense_final_out)\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='en_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='de_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder, enc_final_state = get_encoder_and_state(en_vocab, en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(de_vocab, encoder, enc_final_state, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "e_input (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_input (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              (None, 19)           0           e_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 20)           0           d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "e_embedding (Embedding)         (None, 19, 128)      286720      model[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "d_embedding (Embedding)         (None, 20, 128)      319872      model_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 256)          198144      e_embedding[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 20, 256)      296448      d_embedding[0][0]                \n",
      "                                                                 bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20, 512)      131584      gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20, 2499)     1281987     dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,514,755\n",
      "Trainable params: 2,514,755\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MT models - BLEU metric\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
    "\n",
    "### Defining the BLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            t = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"eos.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            t = np.char.decode(t.numpy().astype(np.bytes_), encoding='utf-8')\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            t = [doc if len(doc)>0 else '[UNK]' for doc in t ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            t = np.char.split(t).tolist()\n",
    "            \n",
    "            return t\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[r] for r in clean_text(real_tokens)]\n",
    "        \n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BLEU metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7598356856515925,\n",
       " [0.8, 0.7777777777777778, 0.75, 0.7142857142857143],\n",
       " 1.0,\n",
       " 1.0,\n",
       " 10,\n",
       " 10)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation = [['[UNK]', 'einmal', 'mÃ¼ssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃ¼ssen', 'mÃ¼ssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "compute_bleu(reference, translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with a custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 39/39\n",
      "Epoch 1/10\n",
      "\t(train) loss: 0.41324986469669217 - accuracy: 0.7189025989709756 - bleu: 0.35723207206008084\n",
      "\t(valid) loss: 0.811834665445181 - accuracy: 0.5698876319787441 - bleu: 0.18294922356038132\n",
      "Evaluating batch 39/39\n",
      "Epoch 2/10\n",
      "\t(train) loss: 0.37136018696503764 - accuracy: 0.7441589690935917 - bleu: 0.39565518355736073\n",
      "\t(valid) loss: 0.8277023021991436 - accuracy: 0.5713850213931158 - bleu: 0.1789816527107478\n",
      "Evaluating batch 39/39\n",
      "Epoch 3/10\n",
      "\t(train) loss: 0.33318772687552833 - accuracy: 0.768034258332008 - bleu: 0.43357104161468396\n",
      "\t(valid) loss: 0.8534994400464572 - accuracy: 0.5705839701187916 - bleu: 0.1800040226641332\n",
      "Evaluating batch 39/39\n",
      "Epoch 4/10\n",
      "\t(train) loss: 0.29957190447319776 - accuracy: 0.7894737848486656 - bleu: 0.47271358934189067\n",
      "\t(valid) loss: 0.8748689416127328 - accuracy: 0.5686705769636692 - bleu: 0.18438219690140717\n",
      "Evaluating batch 39/39\n",
      "Epoch 5/10\n",
      "\t(train) loss: 0.2685514767773641 - accuracy: 0.8104559049392358 - bleu: 0.5122759491375292\n",
      "\t(valid) loss: 0.9045940270790687 - accuracy: 0.5673545950498337 - bleu: 0.17420128660916104\n",
      "Evaluating batch 39/39\n",
      "Epoch 6/10\n",
      "\t(train) loss: 0.2409711181639861 - accuracy: 0.8292077552431669 - bleu: 0.5493905482665165\n",
      "\t(valid) loss: 0.9340984744903369 - accuracy: 0.5662565750953479 - bleu: 0.171757562017331\n",
      "Training batch 242/312\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4c4d2cb18223>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0maccuracy_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mbleu_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_bleu_from_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# =================================================================== #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-695d58fb2185>\u001b[0m in \u001b[0;36mcalculate_bleu_from_predictions\u001b[0;34m(self, real, pred)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Get the predicted token IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpred_argmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Convert token IDs to words using the vocabulary and the StringLookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36margmax_v2\u001b[0;34m(input, axis, output_type, name)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36marg_max\u001b[0;34m(input, dimension, output_type, name)\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 831\u001b[0;31m         _ctx, \"ArgMax\", name, input, dimension, \"output_type\", output_type)\n\u001b[0m\u001b[1;32m    832\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
    "\n",
    "# Define the metric\n",
    "bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "# Define the data\n",
    "data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "\n",
    "shuffle_inds = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Reset metric logs every epoch\n",
    "    bleu_log, val_bleu_log = [],[]\n",
    "    accuracy_log, val_accuracy_log = [],[]\n",
    "    loss_log, val_loss_log = [], []\n",
    "        \n",
    "    # =================================================================== #\n",
    "    #                         Train Phase                                 #\n",
    "    # =================================================================== #\n",
    "    \n",
    "    # Shuffle data at the beginning of every epoch\n",
    "    (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
    "        data_dict['train']['encoder_inputs'],\n",
    "        data_dict['train']['decoder_inputs'],\n",
    "        data_dict['train']['decoder_labels'],\n",
    "        shuffle_inds\n",
    "    )\n",
    "    \n",
    "    # Get the number of training batches\n",
    "    n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    \n",
    "    # Train one batch at a time\n",
    "    for i in range(n_train_batches):\n",
    "        # Status update\n",
    "        print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
    "        \n",
    "        # Get a batch of inputs (english and german sequences)\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        # Get a batch of targets (german sequences offset by 1)\n",
    "        y = de_vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "        # Train for a single step\n",
    "        final_model.train_on_batch(x, y)        \n",
    "        # Evaluate the model to get the metrics\n",
    "        loss, accuracy = final_model.evaluate(x, y, verbose=0)\n",
    "        # Get the final prediction to compute BLEU\n",
    "        pred_y = final_model.predict(x)\n",
    "        \n",
    "        # Update the epoch's log records of the metrics\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Validation Phase                               #\n",
    "    # =================================================================== #\n",
    "    \n",
    "    # Get the validation data\n",
    "    val_en_inputs_raw = data_dict['valid']['encoder_inputs']\n",
    "    val_de_inputs_raw = data_dict['valid']['decoder_inputs']\n",
    "    val_de_labels_raw = data_dict['valid']['decoder_labels']\n",
    "    \n",
    "    # Get the number of validation batches\n",
    "    n_valid_batches = val_en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "    \n",
    "    # Evaluate one validation batch at a time\n",
    "    for i in range(n_valid_batches):\n",
    "        # Status update\n",
    "        print(\"Evaluating batch {}/{}\".format(i+1, n_valid_batches), end='\\r')\n",
    "        \n",
    "        # Get the inputs and targers\n",
    "        x = [val_en_inputs_raw[i*batch_size:(i+1)*batch_size], val_de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = de_vectorizer(val_de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "        \n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = final_model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = final_model.predict(x)\n",
    "        \n",
    "        # Update validation logs\n",
    "        val_loss_log.append(loss)\n",
    "        val_accuracy_log.append(accuracy)\n",
    "        val_bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    # Print the evaluation metrics of each epoch\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "    print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
    "    print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(val_loss_log), np.mean(val_accuracy_log), np.mean(val_bleu_log)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_and_return_conditional_losses, gru_cell_3_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 15). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/seq2seq/assets\n"
     ]
    }
   ],
   "source": [
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  1  41   4 104   1 274   3   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]], shape=(1, 20), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[  1   7   4 505   1 274   3   1   1   1   1   1   1   1   1   1   1   1\n",
      "    1   1]], shape=(1, 20), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "i=345\n",
    "batch_size = 1\n",
    "x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "y = de_vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "pred_y = final_model.predict(x)\n",
    "print(y)\n",
    "print(tf.constant(np.argmax(pred_y, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
