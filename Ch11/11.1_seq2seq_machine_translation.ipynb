{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11/11.1_Seq2seq_machine_translation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    print(\"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\")\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (227080, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=50000, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DE\"] = '[SOS] ' + df[\"DE\"] + ' [EOS]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "test_df = df.sample(n=5000, random_state=random_seed)\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=5000, random_state=random_seed)\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary sizes (English-German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9427\n",
      "to     8673\n",
      "I      8436\n",
      "the    6999\n",
      "you    6125\n",
      "a      5680\n",
      "is     4374\n",
      "in     2664\n",
      "of     2613\n",
      "was    2298\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2238\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "[SOS]    40000\n",
      "[EOS]    40000\n",
      "Tom       9928\n",
      "Ich       7749\n",
      "ist       4753\n",
      "nicht     4414\n",
      "zu        3583\n",
      "Sie       3465\n",
      "du        3112\n",
      "das       2909\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2497\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    counter = Counter(words)\n",
    "\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 29.0\n",
      "\n",
      "count    40000.000000\n",
      "mean        31.841100\n",
      "std         13.496887\n",
      "min          4.000000\n",
      "25%         23.000000\n",
      "50%         29.000000\n",
      "75%         38.000000\n",
      "max        537.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    32161.000000\n",
      "mean        30.086658\n",
      "std          7.659525\n",
      "min         18.000000\n",
      "33%         26.000000\n",
      "50%         29.000000\n",
      "66%         33.000000\n",
      "max         47.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 46.0\n",
      "\n",
      "count    40000.000000\n",
      "mean        49.175300\n",
      "std         16.145143\n",
      "min         18.000000\n",
      "25%         38.000000\n",
      "50%         46.000000\n",
      "75%         57.000000\n",
      "max        493.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    31818.000000\n",
      "mean        47.255453\n",
      "std          9.185303\n",
      "min         33.000000\n",
      "33%         42.000000\n",
      "50%         46.000000\n",
      "66%         51.000000\n",
      "max         68.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "\n",
    "    print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "    p_10 = seq_length_ser.quantile(0.1)\n",
    "    p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "    print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66]))\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2238\n",
      "DE vocabulary size: 2497\n",
      "EN max sequence length: 50\n",
      "DE max sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "en_seq_length = 50\n",
    "de_seq_length = 60\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "# Create the layer.\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "en_vectorize_layer.adapt(train_df[\"EN\"].tolist())\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "# Create the layer.\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(train_df[\"DE\"].tolist())\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization layer in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[427   0   0]\n",
      " [ 40  23   4]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"run\"], [\"how are you\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2238\n"
     ]
    }
   ],
   "source": [
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "print(len(en_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_vectorizer(list_of_strings, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "        \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,        \n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "        \n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out)    \n",
    "    else:\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out), vectorize_layer.get_vocabulary()        \n",
    "    \n",
    "        \n",
    "def get_encoder_and_state(n_vocab, vectorizer):\n",
    "    \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 32, mask_zero=True, name='e_embedding')\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    gru_layer = tf.keras.layers.GRU(32)\n",
    "    \n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "        \n",
    "    return encoder, gru_out\n",
    "\n",
    "\n",
    "def get_final_model_and_state(n_vocab, encoder, init_state, vectorizer):\n",
    "        \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 32, mask_zero=True, name='d_embedding')\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    gru_layer = tf.keras.layers.GRU(32, return_sequences=True)\n",
    "    \n",
    "    gru_out = gru_layer(emb_out, initial_state=init_state)\n",
    "    \n",
    "    dense_layer = tf.keras.layers.Dense(n_vocab+2, activation='softmax')\n",
    "    \n",
    "    dense_out = dense_layer(gru_out)\n",
    "    \n",
    "    decoder = tf.keras.models.Model(inputs=[encoder.input, inp], outputs=dense_out)\n",
    "    \n",
    "    return decoder, gru_out\n",
    "\n",
    "\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(train_df[\"EN\"].tolist(), en_vocab, max_length=en_seq_length, name='en_vectorizer')\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(train_df[\"DE\"].tolist(), de_vocab, max_length=de_seq_length-1, name='de_vectorizer')\n",
    "\n",
    "encoder, enc_final_state = get_encoder_and_state(en_vocab, en_vectorizer)\n",
    "final_model, _ = get_final_model_and_state(de_vocab, encoder, enc_final_state, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Python implementation of BLEU and smooth-BLEU.\n",
    "\n",
    "This module provides a Python implementation of BLEU and smooth-BLEU.\n",
    "Smooth BLEU is computed following the method outlined in the paper:\n",
    "Chin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\n",
    "evaluation metrics for machine translation. COLING 2004.\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def _get_ngrams(segment, max_order):\n",
    "  \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
    "\n",
    "  Args:\n",
    "    segment: text segment from which n-grams will be extracted.\n",
    "    max_order: maximum length in tokens of the n-grams returned by this\n",
    "        methods.\n",
    "\n",
    "  Returns:\n",
    "    The Counter containing all n-grams upto max_order in segment\n",
    "    with a count of how many times each n-gram occurred.\n",
    "  \"\"\"\n",
    "  ngram_counts = collections.Counter()\n",
    "  for order in range(1, max_order + 1):\n",
    "    for i in range(0, len(segment) - order + 1):\n",
    "      ngram = tuple(segment[i:i+order])\n",
    "      ngram_counts[ngram] += 1\n",
    "  return ngram_counts\n",
    "\n",
    "\n",
    "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
    "                 smooth=False):\n",
    "  \"\"\"Computes BLEU score of translated segments against one or more references.\n",
    "\n",
    "  Args:\n",
    "    reference_corpus: list of lists of references for each translation. Each\n",
    "        reference should be tokenized into a list of tokens.\n",
    "    translation_corpus: list of translations to score. Each translation\n",
    "        should be tokenized into a list of tokens.\n",
    "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
    "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
    "\n",
    "  Returns:\n",
    "    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
    "    precisions and brevity penalty.\n",
    "  \"\"\"\n",
    "\n",
    "  if isinstance(reference_corpus, tf.Tensor):\n",
    "    reference_corpus = reference_corpus.numpy().astype('str').tolist()\n",
    "    print(reference_corpus)\n",
    "  if isinstance(translation_corpus, tf.Tensor):\n",
    "    translation_corpus = translation_corpus.numpy().astype('str').tolist()\n",
    "  matches_by_order = [0] * max_order\n",
    "  possible_matches_by_order = [0] * max_order\n",
    "  reference_length = 0\n",
    "  translation_length = 0\n",
    "  for (references, translation) in zip(reference_corpus,\n",
    "                                       translation_corpus):\n",
    "\n",
    "    reference_length += min(len(r) for r in references)\n",
    "    translation_length += len(translation)\n",
    "\n",
    "    merged_ref_ngram_counts = collections.Counter()\n",
    "    for reference in references:\n",
    "      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
    "    translation_ngram_counts = _get_ngrams(translation, max_order)\n",
    "\n",
    "    overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
    "    \n",
    "\n",
    "    for ngram in overlap:\n",
    "      matches_by_order[len(ngram)-1] += overlap[ngram]\n",
    "    for order in range(1, max_order+1):\n",
    "      possible_matches = len(translation) - order + 1\n",
    "      if possible_matches > 0:\n",
    "        possible_matches_by_order[order-1] += possible_matches\n",
    "\n",
    "  precisions = [0] * max_order\n",
    "  for i in range(0, max_order):\n",
    "    if smooth:\n",
    "      precisions[i] = ((matches_by_order[i] + 1.) /\n",
    "                       (possible_matches_by_order[i] + 1.))\n",
    "    else:\n",
    "      if possible_matches_by_order[i] > 0:\n",
    "        precisions[i] = (float(matches_by_order[i]) /\n",
    "                         possible_matches_by_order[i])\n",
    "      else:\n",
    "        precisions[i] = 0.0\n",
    "\n",
    "  if min(precisions) > 0:\n",
    "    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
    "    geo_mean = math.exp(p_log_sum)\n",
    "  else:\n",
    "    geo_mean = 0\n",
    "\n",
    "  ratio = float(translation_length) / reference_length\n",
    "\n",
    "  if ratio > 1.0:\n",
    "    bp = 1.\n",
    "  else:\n",
    "    bp = math.exp(1 - 1. / ratio)\n",
    "\n",
    "  bleu = geo_mean * bp\n",
    "\n",
    "  return (bleu, precisions, bp, ratio, translation_length, reference_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-41-945396f2073d>:46 update_state  *\n        bleu = self._calculate_bleu(y_true, y_pred)\n    <ipython-input-39-54e66107b70b>:40 _calculate_bleu  *\n        bleu = tf.py_function(partial_compute_bleu, [real_tokens, pred_tokens], Tout='float32')\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:513 eager_py_func\n        func=func, inp=inp, Tout=Tout, name=name, use_tape_cache=True)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:420 _eager_py_func\n        use_tape_cache=use_tape_cache)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:348 _internal_py_func\n        name=name)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py:55 eager_py_func\n        ctx=_ctx)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py:96 eager_py_func_eager_fallback\n        _attr_Tin, input = _execute.convert_to_mixed_eager_tensors(input, ctx)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:295 convert_to_mixed_eager_tensors\n        v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:295 <listcomp>\n        v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:276 _constant_impl\n        return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:301 _constant_eager_impl\n        t = convert_to_eager_tensor(value, ctx, dtype)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:98 convert_to_eager_tensor\n        return ops.EagerTensor(value, ctx.device_name, dtype)\n\n    ValueError: Can't convert non-rectangular Python sequence to Tensor.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3fc8f85e46b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m bleu.update_state(\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mupdate_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mcontrol_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mag_update_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_update_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mag_update_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-41-945396f2073d>:46 update_state  *\n        bleu = self._calculate_bleu(y_true, y_pred)\n    <ipython-input-39-54e66107b70b>:40 _calculate_bleu  *\n        bleu = tf.py_function(partial_compute_bleu, [real_tokens, pred_tokens], Tout='float32')\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:513 eager_py_func\n        func=func, inp=inp, Tout=Tout, name=name, use_tape_cache=True)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:420 _eager_py_func\n        use_tape_cache=use_tape_cache)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py:348 _internal_py_func\n        name=name)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py:55 eager_py_func\n        ctx=_ctx)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py:96 eager_py_func_eager_fallback\n        _attr_Tin, input = _execute.convert_to_mixed_eager_tensors(input, ctx)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:295 convert_to_mixed_eager_tensors\n        v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/eager/execute.py:295 <listcomp>\n        v = [ops.convert_to_tensor(t, ctx=ctx) for t in values]\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1540 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:276 _constant_impl\n        return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:301 _constant_eager_impl\n        t = convert_to_eager_tensor(value, ctx, dtype)\n    /home/thushv89/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py:98 convert_to_eager_tensor\n        return ops.EagerTensor(value, ctx.device_name, dtype)\n\n    ValueError: Can't convert non-rectangular Python sequence to Tensor.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "#from bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "    \n",
    "    def _calculate_bleu(self, real, pred):\n",
    "      \n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_padding(tokens):\n",
    "            \"\"\" If padding left in the sequence, they will count towards BLEU \"\"\"\n",
    "            t = tf.strings.split(\n",
    "                    tf.strings.strip(\n",
    "                        tf.strings.join(\n",
    "                            tf.transpose(tokens), separator=' '\n",
    "                        )\n",
    "                    ),\n",
    "                    sep=' '\n",
    "                ).to_list()\n",
    "                \n",
    "            #t = np.char.split(t.numpy().astype('str')).tolist()\n",
    "            \n",
    "            return t\n",
    "        \n",
    "        pred_tokens = clean_padding(pred_tokens)\n",
    "        real_tokens = clean_padding(real_tokens)\n",
    "        \n",
    "        partial_compute_bleu = partial(compute_bleu, smooth=True)\n",
    "        bleu = tf.py_function(partial_compute_bleu, [real_tokens, pred_tokens], Tout='float32')\n",
    "        #print(bleu)\n",
    "        return bleu\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):      \n",
    "        # bleu, precisions, bp, ratio, translation_length, reference_length\n",
    "        bleu = self._calculate_bleu(y_true, y_pred)      \n",
    "        #print(bleu)\n",
    "        super().update_state(bleu)\n",
    "\n",
    "        \n",
    "bleu = BLEUMetric(de_vocabulary)\n",
    "\n",
    "bleu.update_state(\n",
    "    np.array([[0,1,0],[0,2,2]]), np.array([[[1,0,0],[0,1,0],[0,1,0]],[[1,0,0],[0,0,1],[0,0,1]]])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metric (<__main__.BLEUMetric object at 0x7fe74deadfd0>) passed to model.compile was created inside of a different distribution strategy scope than the model. All metrics must be created in the same distribution strategy scope as the model (in this case <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7fe756816dd8>). If you pass in a string identifier for a metric to compile the metric will automatically be created in the correct distribution strategy scope.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c9da82869096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSparseCategoricalAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLEUMetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m           \u001b[0msteps_per_execution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'experimental_steps_per_execution'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/manning.tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   2548\u001b[0m               \u001b[0;34m'identifier for a metric to compile the metric will '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2549\u001b[0m               \u001b[0;34m'automatically be created in the correct distribution '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2550\u001b[0;31m               \u001b[0;34m'strategy scope.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2551\u001b[0m           )\n\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Metric (<__main__.BLEUMetric object at 0x7fe74deadfd0>) passed to model.compile was created inside of a different distribution strategy scope than the model. All metrics must be created in the same distribution strategy scope as the model (in this case <tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy object at 0x7fe756816dd8>). If you pass in a string identifier for a metric to compile the metric will automatically be created in the correct distribution strategy scope."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "#final_model_text = tf.keras.models.Model(inputs=[en_vectorizer.input, de_vectorizer.input], outputs=final_model.output)\n",
    "\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the following for BLEU\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5683 - accuracy: 0.2484\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5268 - accuracy: 0.2920\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.5002 - accuracy: 0.3191\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4811 - accuracy: 0.3359\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4671 - accuracy: 0.3471\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4556 - accuracy: 0.3566\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.4454 - accuracy: 0.3638\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4363 - accuracy: 0.3706\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4281 - accuracy: 0.3778\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4206 - accuracy: 0.3832\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4136 - accuracy: 0.3887\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4071 - accuracy: 0.3940\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.4010 - accuracy: 0.3987\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3954 - accuracy: 0.4038\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3905 - accuracy: 0.4075\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3858 - accuracy: 0.4119\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3814 - accuracy: 0.4162\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3772 - accuracy: 0.4212\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3732 - accuracy: 0.4252\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 0.3693 - accuracy: 0.4298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2c0054cc0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "en_inputs = np.array(train_df[\"EN\"].tolist())\n",
    "de_inputs = np.array(train_df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "de_labels = de_vectorizer.predict(train_df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "    \n",
    "\n",
    "final_model.fit([en_inputs, de_inputs], de_labels, epochs=20)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_labels_vec = de_vectorizer.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
