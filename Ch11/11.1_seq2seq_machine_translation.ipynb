{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11/11.1_Seq2seq_machine_translation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Numpy is not imported. Setting the seed for Numpy failed.\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    print(\"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\")\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (227080, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=50000, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"DE\"] = '[SOS] ' + df[\"DE\"] + ' [EOS]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (5000, 2)\n",
      "valid_df.shape = (5000, 2)\n",
      "train_df.shape = (40000, 2)\n"
     ]
    }
   ],
   "source": [
    "test_df = df.sample(n=5000, random_state=random_seed)\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=5000, random_state=random_seed)\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary sizes (English-German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    9427\n",
      "to     8673\n",
      "I      8436\n",
      "the    6999\n",
      "you    6125\n",
      "a      5680\n",
      "is     4374\n",
      "in     2664\n",
      "of     2613\n",
      "was    2298\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2238\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "[SOS]    40000\n",
      "[EOS]    40000\n",
      "Tom       9928\n",
      "Ich       7749\n",
      "ist       4753\n",
      "nicht     4414\n",
      "zu        3583\n",
      "Sie       3465\n",
      "du        3112\n",
      "das       2909\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 2497\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    counter = Counter(words)\n",
    "\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 29.0\n",
      "\n",
      "count    40000.000000\n",
      "mean        31.841100\n",
      "std         13.496887\n",
      "min          4.000000\n",
      "25%         23.000000\n",
      "50%         29.000000\n",
      "75%         38.000000\n",
      "max        537.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    32161.000000\n",
      "mean        30.086658\n",
      "std          7.659525\n",
      "min         18.000000\n",
      "33%         26.000000\n",
      "50%         29.000000\n",
      "66%         33.000000\n",
      "max         47.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 46.0\n",
      "\n",
      "count    40000.000000\n",
      "mean        49.175300\n",
      "std         16.145143\n",
      "min         18.000000\n",
      "25%         38.000000\n",
      "50%         46.000000\n",
      "75%         57.000000\n",
      "max        493.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    31818.000000\n",
      "mean        47.255453\n",
      "std          9.185303\n",
      "min         33.000000\n",
      "33%         42.000000\n",
      "50%         46.000000\n",
      "66%         51.000000\n",
      "max         68.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "\n",
    "    print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "    p_10 = seq_length_ser.quantile(0.1)\n",
    "    p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "    print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe(percentiles=[0.33, 0.66]))\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 2238\n",
      "DE vocabulary size: 2497\n",
      "EN max sequence length: 50\n",
      "DE max sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "en_seq_length = 50\n",
    "de_seq_length = 60\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "# Create the layer.\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "en_vectorize_layer.adapt(train_df[\"EN\"].tolist())\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "# Create the layer.\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(train_df[\"DE\"].tolist())\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization layer in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb2cd5117b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[427   0   0]\n",
      " [ 40  23   4]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"run\"], [\"how are you\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'tom', 'to', 'you', 'the', 'i', 'a', 'is', 'that']\n",
      "2238\n"
     ]
    }
   ],
   "source": [
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "print(len(en_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the real model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_vectorization_layer (TextVe (None, None)         0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dec_vectorization_layer (TextVe (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, None, 128)    286720      enc_vectorization_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)        (None, None, 128)    319872      dec_vectorization_layer[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "gru_14 (GRU)                    (None, 128)          99072       embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gru_15 (GRU)                    (None, 128)          99072       embedding_14[0][0]               \n",
      "                                                                 gru_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 2499)         322371      gru_15[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,127,107\n",
      "Trainable params: 1,127,107\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_encoder_and_state(list_of_strings, n_vocab):\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=None,        \n",
    "        name='enc_vectorization_layer'\n",
    "    )\n",
    "\n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "    \n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True)\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    gru_layer = tf.keras.layers.GRU(128)\n",
    "    \n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "        \n",
    "    return encoder, gru_out, vectorize_layer\n",
    "\n",
    "\n",
    "def get_final_model_and_state(list_of_strings, n_vocab, encoder, init_state):\n",
    "    \n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='decoder_input')\n",
    "    \n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=None,\n",
    "        name='dec_vectorization_layer'\n",
    "    )\n",
    "\n",
    "    vectorize_layer.adapt(list_of_strings)\n",
    "    \n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "    \n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True)\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    gru_layer = tf.keras.layers.GRU(128)\n",
    "    \n",
    "    gru_out = gru_layer(emb_out, initial_state=init_state)\n",
    "    \n",
    "    dense_layer = tf.keras.layers.Dense(n_vocab+2, activation='softmax')\n",
    "    \n",
    "    dense_out = dense_layer(gru_out)\n",
    "    \n",
    "    decoder = tf.keras.models.Model(inputs=[encoder.input, inp], outputs=dense_out)\n",
    "    \n",
    "    return decoder, gru_out, vectorize_layer\n",
    "\n",
    "\n",
    "encoder, enc_final_state, en_vectorizer = get_encoder_and_state(train_df[\"EN\"].tolist(), en_vocab)\n",
    "final_model, _, de_vectorizer = get_final_model_and_state(train_df[\"DE\"].tolist(), de_vocab, encoder, enc_final_state)\n",
    "\n",
    "\n",
    "final_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use the following for BLEU\n",
    "\n",
    "https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-6e1eac04c85b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mde_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#final_model.fit((train_df[\"EN\"], train_df[\"DE\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "en_inputs = train_df[\"EN\"].tolist()\n",
    "de_inputs = train_df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist()\n",
    "de_labels = train_df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist()\n",
    "\n",
    "#print(de_inputs.head())\n",
    "#print(de_labels.head())\n",
    "#final_model.fit((train_df[\"EN\"], train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_labels_vec = de_vectorizer.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.fit((en_inputs, de_inputs), de_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
