{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modelling (Natural Language Processing)\n",
    "\n",
    "Natural Language Processing (NLP) is a vast subject with many different specializations. Here we are going to discuss a topic that gave rise to ground breaking models like BERT that changed the NLP landscape dramatically; language modelling. Language modelling is an unsupervised training method, where you ask a model to predict the next character/word/sentence given the previous characters/words/sentences.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch09/9.2_Language_modelling.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the bAbI's children story dataset\n",
    "\n",
    "For this task, we'll be using a popular children story dataset from the [bAbI project](https://research.fb.com/downloads/babi/) of Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "# http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'lm','CBTest.tgz')):\n",
    "    url = \"http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists(os.path.join('data','lm')):\n",
    "        os.makedirs(os.path.join('data','lm'))\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'lm', 'CBTest.tgz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'lm', 'CBTest')):\n",
    "    # Write to a file\n",
    "    tarf = tarfile.open(os.path.join(\"data\",\"lm\",\"CBTest.tgz\"))\n",
    "    tarf.extractall(os.path.join(\"data\",\"lm\"))  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data\n",
    "\n",
    "After downloading the data, let's read that into memory. There are three sets; training validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    stories = []\n",
    "\n",
    "    with open(path, 'r') as f:    \n",
    "        s = [] \n",
    "        for row in f:\n",
    "            \n",
    "            if row.startswith(\"_BOOK_TITLE_\"):\n",
    "                if len(s)>0:\n",
    "                    stories.append(' '.join(s).lower())            \n",
    "                s = []           \n",
    "\n",
    "            s.append(row)\n",
    "            \n",
    "    if len(s)>0:\n",
    "        stories.append(' '.join(s).lower())  \n",
    "    \n",
    "    return stories\n",
    "\n",
    "stories = read_data(os.path.join('data','lm','CBTest','data','cbt_train.txt'))\n",
    "val_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_valid.txt'))\n",
    "test_stories = read_data(os.path.join('data','lm','CBTest','data','cbt_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 98 stories (train)\n",
      "Collected 5 stories (valid)\n",
      "Collected 5 stories (test)\n",
      "_book_title_ : andrew_lang___prince_prigio.txt.out\n",
      " chapter i. -lcb- chapter heading picture : p1.jp\n",
      "\n",
      " _book_title_ : andrew_lang___the_violet_fairy_book.txt.out\n",
      " a tale of the tontlawald long , long ago\n"
     ]
    }
   ],
   "source": [
    "print(\"Collected {} stories (train)\".format(len(stories)))\n",
    "print(\"Collected {} stories (valid)\".format(len(val_stories)))\n",
    "print(\"Collected {} stories (test)\".format(len(test_stories)))\n",
    "print(stories[0][:100])\n",
    "print('\\n', stories[10][:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99464\n",
      "99834\n",
      "136758\n",
      "761257\n",
      "524783\n",
      "522998\n",
      "528840\n",
      "531058\n",
      "527601\n",
      "674648\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(len(stories[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick drive to vocabulary-ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",      348650\n",
      "the    242890\n",
      ".\\n    192549\n",
      "and    179205\n",
      "to     120821\n",
      "a      101990\n",
      "of      96748\n",
      "i       79780\n",
      "he      78129\n",
      "was     66593\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 14473\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in stories for w in doc.split(' ')]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Count of words >= n frequent\n",
    "n=10\n",
    "print(\"\\nVocabulary size (>={} frequent): {}\".format(n, (freq_df>=n).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert strings to n-grams\n",
    "\n",
    "For our language modelling task, we're going to split strings into bigrams. That is, given the string\n",
    "\n",
    "`i went to the office`, it is converted to,\n",
    "\n",
    "`[\"i \", \"we\", \"nt\", \" t\", \"o \", \"th\", \"e \", \"of\", \"fi\", \"ce\"]`\n",
    "\n",
    "We will also look at what are the most common bigrams and some summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I like chocolates\n",
      "\t1-grams: ['I', ' ', 'l', 'i', 'k', 'e', ' ', 'c', 'h', 'o', 'c', 'o', 'l', 'a', 't', 'e', 's']\n",
      "\t2-grams: ['I ', 'li', 'ke', ' c', 'ho', 'co', 'la', 'te', 's']\n",
      "\t3-grams: ['I l', 'ike', ' ch', 'oco', 'lat', 'es']\n",
      "\n",
      "Sample of most-common bigrams\n",
      "e     455292\n",
      " t    344971\n",
      "he    310539\n",
      "d     308390\n",
      "th    284425\n",
      " a    268496\n",
      "t     257788\n",
      "s     227961\n",
      " h    192544\n",
      " s    182830\n",
      "dtype: int64\n",
      "\n",
      "Median: 136.5\n",
      "\n",
      "count      1074.000000\n",
      "mean      12106.919926\n",
      "std       36358.817692\n",
      "min           1.000000\n",
      "25%           5.000000\n",
      "50%         136.500000\n",
      "75%        6406.750000\n",
      "90%       34184.600000\n",
      "max      455292.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    \"\"\" This function takes a given string and split it into desired sized n-grams \"\"\"\n",
    "    return [text[i:i+n] for i in range(0,len(text),n)]\n",
    "\n",
    "# Test the ngrams function with a variety of ngrams\n",
    "test_string = \"I like chocolates\"\n",
    "print(\"Original: {}\".format(test_string))\n",
    "for i in list(range(3)):\n",
    "    print(\"\\t{}-grams: {}\".format(i+1, get_ngrams(test_string, i+1)))\n",
    "\n",
    "# Create a counter with the bi-grams\n",
    "ngrams = 2\n",
    "\n",
    "text = chain(*[get_ngrams(s, ngrams) for s in stories])\n",
    "cnt = Counter(text)\n",
    "\n",
    "# Create a pandas series with the counter results\n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "print(\"\\nSample of most-common bigrams\")\n",
    "print(freq_df.head(n=10))\n",
    "print(\"\\nMedian: {}\\n\".format(freq_df.median()))\n",
    "# Get summary statistics\n",
    "print(freq_df.describe(percentiles=[0.25,0.5,0.75,0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the size of the vocabulary\n",
    "\n",
    "We will set the vocabulary size to the number of words (bi-grams) that appear at least 10 times in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 735\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df>=10).sum()\n",
    "print(\"Size of vocabulary: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-grams to IDs: Defining a Keras tokenizer\n",
    "\n",
    "Here, we're going to fit a tokenizer on the train data in order to convert bi-grams to IDs. The tokenizer will assign a specific ID to each unique bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer for the determined vocabulary size\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Get ngrams in the training data\n",
    "train_ngram_stories = [get_ngrams(s,ngrams) for s in stories]\n",
    "# Fit the tokenizer\n",
    "tokenizer.fit_on_texts(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for training data\n",
    "train_data_seq = tokenizer.texts_to_sequences(train_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for validation data\n",
    "val_ngram_stories = [get_ngrams(s,ngrams) for s in val_stories]\n",
    "val_data_seq = tokenizer.texts_to_sequences(val_ngram_stories)\n",
    "\n",
    "# Get the ID sequence for testing data\n",
    "test_ngram_stories = [get_ngrams(s,ngrams) for s in test_stories]\n",
    "test_data_seq = tokenizer.texts_to_sequences(test_ngram_stories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some word ID sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: the yellow fairy book the cat and the mouse in par\n",
      "n-grams: ['th', 'e ', 'ye', 'll', 'ow', ' f', 'ai', 'ry', ' b', 'oo', 'k ', 'th', 'e ', 'ca', 't ', 'an', 'd ', 'th', 'e ', 'mo', 'us', 'e ', 'in', ' p', 'ar']\n",
      "Word ID sequence: [6, 2, 216, 55, 85, 35, 95, 146, 26, 97, 123, 6, 2, 128, 8, 15, 5, 6, 2, 147, 114, 2, 17, 65, 52]\n",
      "\n",
      "\n",
      "Original: chapter i. down the rabbit-hole alice was beginnin\n",
      "n-grams: ['ch', 'ap', 'te', 'r ', 'i.', ' d', 'ow', 'n ', 'th', 'e ', 'ra', 'bb', 'it', '-h', 'ol', 'e ', 'al', 'ic', 'e ', 'wa', 's ', 'be', 'gi', 'nn', 'in']\n",
      "Word ID sequence: [93, 206, 57, 19, 544, 47, 85, 18, 6, 2, 126, 343, 39, 402, 136, 2, 70, 140, 2, 66, 9, 71, 218, 251, 17]\n",
      "\n",
      "\n",
      "Original: a patent medicine testimonial `` you might as well\n",
      "n-grams: ['a ', 'pa', 'te', 'nt', ' m', 'ed', 'ic', 'in', 'e ', 'te', 'st', 'im', 'on', 'ia', 'l ', '``', ' y', 'ou', ' m', 'ig', 'ht', ' a', 's ', 'we', 'll']\n",
      "Word ID sequence: [60, 182, 57, 78, 33, 31, 140, 17, 2, 57, 49, 125, 43, 266, 56, 122, 92, 29, 33, 151, 149, 7, 9, 102, 55]\n",
      "\n",
      "\n",
      "Original: mowgli 's brothers now rann the kite brings home t\n",
      "n-grams: ['mo', 'wg', 'li', \" '\", 's ', 'br', 'ot', 'he', 'rs', ' n', 'ow', ' r', 'an', 'n ', 'th', 'e ', 'ki', 'te', ' b', 'ri', 'ng', 's ', 'ho', 'me', ' t']\n",
      "Word ID sequence: [147, 513, 74, 46, 9, 203, 99, 4, 134, 58, 85, 79, 15, 18, 6, 2, 191, 57, 26, 89, 37, 9, 73, 63, 3]\n",
      "\n",
      "\n",
      "Original: i jimmy skunk is puzzled old mother west wind had \n",
      "n-grams: ['i ', 'ji', 'mm', 'y ', 'sk', 'un', 'k ', 'is', ' p', 'uz', 'zl', 'ed', ' o', 'ld', ' m', 'ot', 'he', 'r ', 'we', 'st', ' w', 'in', 'd ', 'ha', 'd ']\n",
      "Word ID sequence: [87, 390, 308, 21, 274, 116, 123, 48, 65, 474, 496, 31, 24, 98, 33, 99, 4, 19, 102, 49, 12, 17, 5, 30, 5]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s, tokens, seq in zip(test_stories[:5], test_ngram_stories[:5], test_data_seq[:5]):\n",
    "    print(\"Original: {}\".format(s[:50]))\n",
    "    print(\"n-grams: {}\".format(tokens[:25]))\n",
    "    print(\"Word ID sequence: {}\".format(seq[:25]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the TensorFlow `tf.data` pipeline\n",
    "\n",
    "Here we will define a `tf.data` pipeline to generate data for the model. In language modelling, data is generated as follows. Say you want to provide a `n` elements long sequence as the input to the model in order to generate text. Then you take a `n+1` long sequence `text` and split it into two parts; `text[:-1]` and `text[1:]`. At any step of the implementation, you can check the specification of the dataset with `print(tf.data.DatasetSpec.from_value(ds))`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_pipeline(data_seq, n_seq, batch_size=64, shift=1, shuffle=True):\n",
    "    \"\"\" Define a tf.data pipeline that takes a set of sequences of text and \n",
    "    convert them to fixed length sequences for the model \"\"\"\n",
    "    \n",
    "    # Define a tf.dataset from a ragged tensor created from data_seq\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(data_seq)) # tf.ragged.constant(data_seq)\n",
    "    \n",
    "    # If shuffle is set, shuffle the data (shuffle story order)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=len(data_seq)//2)\n",
    "    \n",
    "    # This function will create windows from data, given a window size and a shift\n",
    "    # Each window is a single entity    \n",
    "    \n",
    "    # windows function create neted dataset within text ds\n",
    "    # This is a special trick we use to unwrap those nested structures\n",
    "    #text_ds = text_ds.flat_map(lambda window: window.batch(n_seq+1, drop_remainder=True))    \n",
    "    text_ds = text_ds.flat_map(\n",
    "        lambda x: tf.data.Dataset.from_tensor_slices(\n",
    "            x\n",
    "        ).window(\n",
    "            n_seq+1, shift=shift\n",
    "        ).flat_map(\n",
    "            lambda window: window.batch(n_seq+1, drop_remainder=True)\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "    # Shuffle the data (shuffle the order of n_seq+1 long sequences)\n",
    "    if shuffle:\n",
    "        text_ds = text_ds.shuffle(buffer_size=10*batch_size)\n",
    "    \n",
    "    # Batch the data\n",
    "    text_ds = text_ds.batch(batch_size)\n",
    "    \n",
    "    # Split each sequence to an input and a target\n",
    "    text_ds = tf.data.Dataset.zip(text_ds.map(lambda x: (x[:,:-1], x[:, 1:]))).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return text_ds    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some data\n",
    "\n",
    "Here you can see that `a` is a tuple with two Tensors; an input tensor and a target tensor. If you check the target tensor, each row in target is essentially a shift by 1 to the right of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[161,  12,  69, 396,  17],\n",
      "       [  2,  72,  77,  84,  24],\n",
      "       [ 87,   6,   2,  72,  77],\n",
      "       [276, 484,  57,   5,  15],\n",
      "       [ 75, 150,   3,   4,  11],\n",
      "       [ 11,  73, 211,  35, 141]])>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 12,  69, 396,  17,  44],\n",
      "       [ 72,  77,  84,  24,  51],\n",
      "       [  6,   2,  72,  77,  84],\n",
      "       [484,  57,   5,  15,  67],\n",
      "       [150,   3,   4,  11,  73],\n",
      "       [ 73, 211,  35, 141,  98]])>)\n",
      "(<tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 48,  69,  43,   2,  14],\n",
      "       [ 84,  24,  51,  93,  15],\n",
      "       [104,  28,  23,  72,   2],\n",
      "       [ 22,  31,   7,  22,  11],\n",
      "       [  4,  11,  73, 211,  35],\n",
      "       [ 19,  87,   6,   2,  72]])>, <tf.Tensor: shape=(6, 5), dtype=int32, numpy=\n",
      "array([[ 69,  43,   2,  14, 124],\n",
      "       [ 24,  51,  93,  15, 151],\n",
      "       [ 28,  23,  72,   2,  15],\n",
      "       [ 31,   7,  22,  11, 280],\n",
      "       [ 11,  73, 211,  35, 141],\n",
      "       [ 87,   6,   2,  72,  77]])>)\n"
     ]
    }
   ],
   "source": [
    "ds = get_tf_pipeline(train_data_seq, 5, batch_size=6)\n",
    "\n",
    "for a in ds.take(2):\n",
    "\n",
    "    print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print and save hyperparameters so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_grams uses n=2\n",
      "Vocabulary size: 735\n",
      "Sequence length for model: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"n_grams uses n={}\".format(ngrams))\n",
    "print(\"Vocabulary size: {}\".format(n_vocab))\n",
    "\n",
    "n_seq=100\n",
    "print(\"Sequence length for model: {}\".format(n_seq))\n",
    "\n",
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'wb') as f:\n",
    "    pickle.dump({'n_vocab': n_vocab, 'ngrams':ngrams, 'n_seq': n_seq}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "Here we're going to define an embedding layer, a single LSTM layer and two dense layers. \n",
    "\n",
    "More on regularizing LSTM models: https://arxiv.org/pdf/1708.02182.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.GRU(1024, return_state=False, return_sequences=True),\n",
    "    \n",
    "    # Defining a Dense layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    \n",
    "    # Defining a final Dense layer and softmax activation\n",
    "    tf.keras.layers.Dense(n_vocab, name='final_out'),\n",
    "    tf.keras.layers.Activation(activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Perplexity Metric\n",
    "\n",
    "Perplexity measures given a sequence of $n-1$ words, how surprised (or perplexed) the model was to see the $n^{th}$ word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Inspired by https://gist.github.com/Gregorgeous/dbad1ec22efc250c76354d949a13cec3\n",
    "class PerplexityMetric(tf.keras.metrics.Mean):\n",
    "    \n",
    "    def __init__(self, name='perplexity', **kwargs):\n",
    "      super().__init__(name=name, **kwargs)\n",
    "      self.cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "\n",
    "    def _calculate_perplexity(self, real, pred):\n",
    "      # The next 4 lines zero-out the padding from loss calculations, \n",
    "      # this follows the logic from: https://www.tensorflow.org/beta/tutorials/text/transformer#loss_and_metrics \t\t\t      \n",
    "      loss_ = self.cross_entropy(real, pred)\n",
    "      \n",
    "      # Calculating the perplexity steps: \n",
    "      step1 = K.mean(loss_, axis=-1)\n",
    "      perplexity = K.exp(step1)\n",
    "      #perplexity = K.mean(step2)\n",
    "    \n",
    "      return perplexity \n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):            \n",
    "      perplexity = self._calculate_perplexity(y_true, y_pred)\n",
    "      # Remember self.perplexity is a tensor (tf.Variable), so using simply \"self.perplexity = perplexity\" will result in error because of mixing EagerTensor and Graph operations \n",
    "      super().update_state(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Perpelxity calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.2082006, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "p = PerplexityMetric()\n",
    "# Define a set of true targets\n",
    "true = [[0, 1,2],[0, 1,2]]\n",
    "# Define a set of predictions\n",
    "pred = [[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]],[[0.9, 0.1, 0.0], [0.3, 0.7, 0.0], [0.0, 0.1, 0.9]]]\n",
    "\n",
    "# Compute perplexity\n",
    "p.update_state(true, pred)\n",
    "print(p.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "We will compile the model with `sparse_categorical_crossentropy`, `adam` optimizer and `accuracy` and `perplexity` metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 512)         376832    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 1024)        4724736   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 512)         524800    \n",
      "_________________________________________________________________\n",
      "final_out (Dense)            (None, None, 735)         377055    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, None, 735)         0         \n",
      "=================================================================\n",
      "Total params: 6,003,423\n",
      "Trainable params: 6,003,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Here we're going to train the model. To keep the training shorter, we will only use 50/98 storings in the training set. We will also generate sequences at a shift of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using metric=val_perplexity and mode=min for EarlyStopping\n",
      "Epoch 1/50\n",
      "2936/2936 [==============================] - 341s 116ms/step - loss: 2.3857 - accuracy: 0.4313 - perplexity: 15.2877 - val_loss: 2.4790 - val_accuracy: 0.4113 - val_perplexity: 12.6962 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "2936/2936 [==============================] - 343s 117ms/step - loss: 2.0692 - accuracy: 0.4853 - perplexity: 8.4284 - val_loss: 2.4818 - val_accuracy: 0.4150 - val_perplexity: 12.8089 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "2936/2936 [==============================] - 344s 117ms/step - loss: 2.0456 - accuracy: 0.4889 - perplexity: 8.2086 - val_loss: 2.4728 - val_accuracy: 0.4130 - val_perplexity: 12.6940 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "2936/2936 [==============================] - 345s 117ms/step - loss: 2.0390 - accuracy: 0.4891 - perplexity: 8.1284 - val_loss: 2.4252 - val_accuracy: 0.4215 - val_perplexity: 12.0610 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "2936/2936 [==============================] - 347s 118ms/step - loss: 2.0380 - accuracy: 0.4885 - perplexity: 8.0946 - val_loss: 2.4085 - val_accuracy: 0.4273 - val_perplexity: 11.9038 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "2936/2936 [==============================] - 345s 118ms/step - loss: 2.0324 - accuracy: 0.4891 - perplexity: 8.0472 - val_loss: 2.4290 - val_accuracy: 0.4226 - val_perplexity: 12.1469 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "2936/2936 [==============================] - 337s 115ms/step - loss: 2.0316 - accuracy: 0.4886 - perplexity: 8.0190 - val_loss: 2.3837 - val_accuracy: 0.4278 - val_perplexity: 11.5808 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "2936/2936 [==============================] - 333s 114ms/step - loss: 2.0386 - accuracy: 0.4867 - perplexity: 8.0632 - val_loss: 2.3939 - val_accuracy: 0.4268 - val_perplexity: 11.7099 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "2936/2936 [==============================] - 331s 113ms/step - loss: 2.0346 - accuracy: 0.4872 - perplexity: 8.0271 - val_loss: 2.3929 - val_accuracy: 0.4320 - val_perplexity: 11.6827 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 2.0752 - accuracy: 0.4750 - perplexity: 8.2709 - val_loss: 2.2333 - val_accuracy: 0.4550 - val_perplexity: 9.9208 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 2.0099 - accuracy: 0.4875 - perplexity: 7.7175 - val_loss: 2.2053 - val_accuracy: 0.4639 - val_perplexity: 9.7113 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9782 - accuracy: 0.4940 - perplexity: 7.4629 - val_loss: 2.2219 - val_accuracy: 0.4588 - val_perplexity: 9.8068 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9527 - accuracy: 0.4993 - perplexity: 7.2663 - val_loss: 2.2132 - val_accuracy: 0.4616 - val_perplexity: 9.7681 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9496 - accuracy: 0.4984 - perplexity: 7.2252 - val_loss: 2.1881 - val_accuracy: 0.4662 - val_perplexity: 9.5086 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9349 - accuracy: 0.5020 - perplexity: 7.1151 - val_loss: 2.1880 - val_accuracy: 0.4666 - val_perplexity: 9.5139 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9284 - accuracy: 0.5036 - perplexity: 7.0667 - val_loss: 2.1921 - val_accuracy: 0.4656 - val_perplexity: 9.5717 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9254 - accuracy: 0.5040 - perplexity: 7.0432 - val_loss: 2.1885 - val_accuracy: 0.4668 - val_perplexity: 9.5469 - lr: 1.0000e-06\n",
      "Epoch 18/50\n",
      "2936/2936 [==============================] - 327s 112ms/step - loss: 1.9238 - accuracy: 0.5044 - perplexity: 7.0315 - val_loss: 2.1881 - val_accuracy: 0.4670 - val_perplexity: 9.5387 - lr: 1.0000e-06\n",
      "Epoch 19/50\n",
      "2936/2936 [==============================] - 328s 112ms/step - loss: 1.9230 - accuracy: 0.5046 - perplexity: 7.0259 - val_loss: 2.1880 - val_accuracy: 0.4670 - val_perplexity: 9.5393 - lr: 1.0000e-07\n",
      "It took 6348.724717617035 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "train_ds = get_tf_pipeline(train_data_seq[:50], n_seq, shift=25, batch_size=128)\n",
    "valid_ds = get_tf_pipeline(val_data_seq, n_seq, shift=n_seq, batch_size=128)\n",
    "\n",
    "os.makedirs('eval', exist_ok=True)\n",
    "\n",
    "# Logging the performance metrics to a CSV file\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join('eval','1_language_modelling.log'))\n",
    "\n",
    "monitor_metric = 'val_perplexity'\n",
    "mode = 'min' \n",
    "print(\"Using metric={} and mode={} for EarlyStopping\".format(monitor_metric, mode))\n",
    "\n",
    "# Reduce LR callback\n",
    "# This function keeps the initial learning rate for the first ten epochs\n",
    "# and decreases it exponentially after that.\n",
    "def scheduler(epoch, lr):  \n",
    "    if epoch==0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.1\n",
    "\n",
    "#lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=monitor_metric, factor=0.1, patience=2, mode=mode, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# EarlyStopping itself increases the memory requirement\n",
    "# restore_best_weights will increase the memory req for large models\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=monitor_metric, patience=5, mode=mode, restore_best_weights=False\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "model.fit(train_ds, epochs=50, \n",
    "          validation_data = valid_ds,\n",
    "          callbacks=[es_callback, lr_callback, csv_logger])\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"It took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 2s 39ms/step - loss: 2.2620 - accuracy: 0.4574 - perplexity: 10.5495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2619853019714355, 0.457405149936676, 10.549492835998535]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "test_ds = get_tf_pipeline(test_data_seq, n_seq, shift=n_seq, batch_size=batch_size)\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(model, os.path.join('models', '2_gram_lm.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('models', 'text_hyperparams.pkl'), 'rb') as f:\n",
    "    hparams = pickle.load(f)\n",
    "\n",
    "ngrams = hparams['ngrams']\n",
    "n_vocab = hparams[\"n_vocab\"]\n",
    "n_seq = hparams[\"n_seq\"]\n",
    "\n",
    "model = tf.keras.models.load_model(os.path.join('models', '2_gram_lm.h5'), compile=False)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy', PerplexityMetric()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model (Functional API)\n",
    "\n",
    "Here, we're going to define an inference model. We need to actually define a new model with identical weights to the original but will make changes to inputs and outputs. Essentially, we will define a model to which we can pass in an intial state (hidden state of GRU) and outputs the final prediction as well as the new hidden state.\n",
    "\n",
    "This way we can recursively call our model on new predictions to generate a story for any number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 512)    376832      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1024)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, None, 1024), 4724736     embedding[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 512)    524800      gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "final_out (Dense)               (None, None, 735)    377055      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 735)    0           final_out[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 6,003,423\n",
      "Trainable params: 6,003,423\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define inputs to the model\n",
    "inp = tf.keras.layers.Input(shape=(None,))\n",
    "inp_state = tf.keras.layers.Input(shape=(1024,))\n",
    "\n",
    "# Define embedding layer and output\n",
    "emb_layer = tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=512, input_shape=(None,))\n",
    "emb_out = emb_layer(inp)\n",
    "\n",
    "# Defining a GRU layer and output\n",
    "gru_layer = tf.keras.layers.GRU(1024, return_state=True, return_sequences=True)\n",
    "gru_out, gru_state = gru_layer(emb_out, initial_state=inp_state)\n",
    "\n",
    "# Defining a Dense layer and output\n",
    "dense_layer = tf.keras.layers.Dense(512, activation='relu')\n",
    "dense_out = dense_layer(gru_out)\n",
    "\n",
    "# Defining the final Dense layer and output\n",
    "final_layer = tf.keras.layers.Dense(n_vocab, name='final_out')\n",
    "final_out = final_layer(dense_out)\n",
    "softmax_out = tf.keras.layers.Activation(activation='softmax')(final_out)\n",
    "\n",
    "# Define final model\n",
    "infer_model = tf.keras.models.Model(inputs=[inp, inp_state], outputs=[softmax_out, gru_state])\n",
    "\n",
    "# Copy the weights from the original model\n",
    "emb_layer.set_weights(model.get_layer('embedding').get_weights())\n",
    "gru_layer.set_weights(model.get_layer('gru').get_weights())\n",
    "dense_layer.set_weights(model.get_layer('dense').get_weights())\n",
    "final_layer.set_weights(model.get_layer('final_out').get_weights())\n",
    "\n",
    "# Summary\n",
    "infer_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with greedy decoding\n",
    "\n",
    "Here we will generate text with the simplest approach we can think of. At time $t=1$, we start with a predefined sequence, and feed that to `infer_model`. At the end of the sequence we get $w_1$ (the prediction at $t=1$). $w_1$ will be the input to the model at $t=2$ and the model will generate $w_2$ and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions from a 54 element long input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and then they went to the shore , and then the princess was so stilling that he was a little girl , and then he wenw into the sea , and then he sail , as she spoke , they were all sorts of things , and they were all so happy and the most splendid of them .\n",
      " then the princess was so happy , anne , who was sitting on his way , and the princess was still as a little white silk dress .\n",
      " `` i 'm going that i have n't got anything else .\n",
      " i 'm sure young man was a little girl , anyway ... and i 'm so glassy and happy .\n",
      " i 'm goin ' to the door , and i 'll taken me to the shore .\n",
      " i 'm sure i can not bear to be anything to me .\n",
      " i 'm not going to say thanking that it is n't any one else .\n",
      " i 'm not going-out , and i 'm going to be seen .\n",
      " it 's all right , and i 'll tell you how young things would n't be able to do it .\n",
      " i 'm not goin ' to think of itself , and i 'm going to be sure to see you .\n",
      " i 'm sure i can notice them .\n",
      " i 'm going to see you again , and i 'll tell you what i 've got , '\n"
     ]
    }
   ],
   "source": [
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(), \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making predictions from a {} element long input\".format(len(seq[0])))\n",
    "\n",
    "# Reset the state of the model initially\n",
    "model.reset_states()\n",
    "# Definin the initial state as all zeros\n",
    "state = np.zeros(shape=(1,1024))\n",
    "# Recursively update the model by assining new state to state\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state])\n",
    "\n",
    "# Get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "# Define first input to generate text recursively from\n",
    "x = np.array([[wid]])\n",
    "\n",
    "for _ in range(500):\n",
    "    \n",
    "    # Get the next output and state\n",
    "    out, state = infer_model.predict([x, state])\n",
    "    \n",
    "    # Get the word id and the word from out\n",
    "    out_argsort = np.argsort(out[0], axis=-1).ravel()        \n",
    "    wid = int(out_argsort[-1])\n",
    "    word = tokenizer.index_word[wid]\n",
    "    \n",
    "    # If the word ends with space, we introduce a bit of randomness\n",
    "    # Essentially pick one of the top 3 outputs for that timestep depending on their likelihood\n",
    "    if word.endswith(' '):\n",
    "        if np.random.normal()>0.5:\n",
    "            width = 3\n",
    "            i = np.random.choice(list(range(-width,0)), p=out_argsort[-width:]/out_argsort[-width:].sum())    \n",
    "            wid = int(out_argsort[i])    \n",
    "            word = tokenizer.index_word[wid]\n",
    "            \n",
    "    # Append the prediction\n",
    "    text.append(word)\n",
    "    \n",
    "    # Recursively make the current prediction the next input\n",
    "    x = np.array([[wid]])\n",
    "    \n",
    "# Print the final output    \n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search decoding\n",
    "\n",
    "Beam search is a more sophisticated and better decoding technique. In beam search we predict several timesteps in to the future and pick the sequence that gives the best joint probability. Remember that, in greedy decoding we only predicted 1 step into the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the beam search logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_one_step(model, input_, state):    \n",
    "    \"\"\" Perform the model update and output for one step\"\"\"\n",
    "    output, new_state = model.predict([input_, state])\n",
    "    return output, new_state\n",
    "\n",
    "\n",
    "\n",
    "def beam_search(model, input_, state, beam_depth=5, beam_width=3, ignore_blank=True):\n",
    "    \"\"\" Defines an outer wrapper for the computational function of beam search \"\"\"\n",
    "    \n",
    "    def recursive_fn(input_, state, sequence, log_prob, i):\n",
    "        \"\"\" This function performs actual recursive computation of the long string\"\"\"\n",
    "        \n",
    "        if i == beam_depth:\n",
    "            \"\"\" Base case: Terminate the beam search \"\"\"\n",
    "            results.append((list(sequence), state, np.exp(log_prob)))            \n",
    "            return sequence, log_prob, state\n",
    "        else:\n",
    "            \"\"\" Recursive case: Keep computing the output using the previous outputs\"\"\"\n",
    "            output, new_state = beam_one_step(model, input_, state)\n",
    "            \n",
    "            # Get the top beam_widht candidates for the given depth\n",
    "            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)\n",
    "            top_probs, top_ids = top_probs.numpy().ravel(), top_ids.numpy().ravel()\n",
    "            \n",
    "            # For each candidate compute the next prediction\n",
    "            for p, wid in zip(top_probs, top_ids):                \n",
    "                new_log_prob = log_prob + np.log(p)\n",
    "                \n",
    "                # we are going to penalize joint probability whenever the same symbol is repeating\n",
    "                if len(sequence)>0 and wid == sequence[-1]:\n",
    "                    new_log_prob = new_log_prob + np.log(1e-1)\n",
    "                    \n",
    "                sequence.append(wid)                \n",
    "                _ = recursive_fn(np.array([[wid]]), new_state, sequence, new_log_prob, i+1)                                         \n",
    "                sequence.pop()\n",
    "        \n",
    "    \n",
    "    results = []\n",
    "    sequence = []\n",
    "    log_prob = 0.0\n",
    "    recursive_fn(input_, state, sequence, log_prob, 0)    \n",
    "\n",
    "    results = sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 54 predictions from input\n",
      "\n",
      "\n",
      "============================================================\n",
      "Final text: \n",
      "chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank , and there was no reason that her father had brought him the story girl 's face .\n",
      " `` i 'm going to bed , '' said the prince , `` and you can not be able to do it . ''\n",
      " `` i 'm sure i shall have to go to bed , '' he answered , with a smile .\n",
      " `` i 'm so happy , '' she said .\n",
      " `` i do n't know how to bring you into the world , and i 'll be sure that you would have thought that it would have been a long time .\n",
      " there was no time to be able to do it , and it would have been a little thing . ''\n",
      " `` i do n't know , '' she said .\n",
      " `` i 'm so glad you come back . ''\n",
      " `` i do n't know what to do with you , '' said the princess , who would not have been able to do that .\n",
      " `` well , i do n't know , '' said anne .\n",
      " `` i 'm going to believe that i have n't anything more if i could , and i 'm glad you have a good deal .\n",
      " that is what it is . ''\n",
      " `` i 'm glad of it , '' said mr. meredith , `` and then you will be so good to me , and that is why they are all right .\n",
      " i 'm not going to be there , but i do n't want anything exciting . ''\n",
      " `` what is the matter with you ? ''\n",
      " `` i do n't know what it is . ''\n",
      " `` i do n't know , '' said the princess , who had been a good deal of father .\n",
      " `` i 'm going to bed , '' said the king .\n",
      " `` what is the matter ? ''\n",
      " `` no , '' said anne , with a smile .\n",
      " `` i do n't know what to do , '' said mary .\n",
      " `` i 'm so glad you come back , '' said mrs. march , with\n"
     ]
    }
   ],
   "source": [
    "text = get_ngrams(\n",
    "    \"CHAPTER I. Down the Rabbit-Hole Alice was beginning to get very tired of sitting by her sister on the bank ,\".lower(),     \n",
    "    ngrams\n",
    ")\n",
    "\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# build up model state using the given string\n",
    "print(\"Making {} predictions from input\".format(len(seq[0])))\n",
    "\n",
    "#model.reset_states()\n",
    "state = np.zeros(shape=(1,1024))\n",
    "for c in seq[0]:    \n",
    "    out, state = infer_model.predict([np.array([[c]]), state])\n",
    "\n",
    "# get final prediction after feeding the input string\n",
    "wid = int(np.argmax(out[0],axis=-1).ravel())\n",
    "word = tokenizer.index_word[wid]\n",
    "text.append(word)\n",
    "\n",
    "x = np.array([[wid]])\n",
    "\n",
    "# Predict for 100 time steps\n",
    "for i in range(100):    \n",
    "    \n",
    "    # Get the results from beam search\n",
    "    result = beam_search(infer_model, x, state, 7, 2)\n",
    "    \n",
    "    # Get one of the top 10 results based on their likelihood\n",
    "    n_probs = np.array([p for _,_,p in result[:10]])\n",
    "    p_j = np.random.choice(list(range(n_probs.size)), p=n_probs/n_probs.sum())                    \n",
    "    best_beam_ids, state, _ = result[p_j]\n",
    "    x = np.array([[best_beam_ids[-1]]])\n",
    "            \n",
    "    text.extend([tokenizer.index_word[w] for w in best_beam_ids])    \n",
    "\n",
    "print('\\n')\n",
    "print('='*60)\n",
    "print(\"Final text: \")\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
