{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classification with BERT\n",
    "\n",
    "Deep learning has been revolutionized by transformer models. Transformer based models like BERT are heavily used in NLP to solve tasks due to the rich numerical representations of text they provide. Here we will be discussing how to download a transformer model and then adapt it to solve a spam classification problem.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch12/12.1_Spam_Classification_with_BERT.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.4.1 installed\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from official import nlp\n",
    "from official.nlp import bert\n",
    "import official.nlp.optimization\n",
    "import official.nlp.bert.bert_models\n",
    "import official.nlp.bert.configs\n",
    "\n",
    "print(\"TensorFlow: {} installed\".format(tf.__version__))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the data\n",
    "\n",
    "For this, we will be using the spam classification dataset available [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip). It is a tab separated file with two columns. First column is a single word (ham/spam), where the second column contains the SMS message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The zip file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "import shutil\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "        \n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data', 'smsspamcollection.zip')):\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data', 'smsspamcollection.zip'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "          \n",
    "else:\n",
    "    print(\"The zip file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'SMSSpamCollection')):\n",
    "    with zipfile.ZipFile(os.path.join('data', 'smsspamcollection.zip'), 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')  \n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4827 ham and 747 spam\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n', 'Ok lar... Joking wif u oni...\\n', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", 'U dun say so early hor... U c already then say...\\n', \"Nah I don't think he goes to usf, he lives around here though\\n\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs and labels will be stored in this\n",
    "inputs = []\n",
    "labels = []\n",
    "# Total number of instances for spam and ham\n",
    "n_ham, n_spam = 0,0\n",
    "with open(os.path.join('data', 'SMSSpamCollection'), 'r') as f:\n",
    "    for r in f:        \n",
    "        # Ham input\n",
    "        if r.startswith('ham'):\n",
    "            label = 0\n",
    "            txt = r[4:]\n",
    "            n_ham += 1\n",
    "        # Spam input\n",
    "        elif r.startswith('spam'):\n",
    "            label = 1\n",
    "            txt = r[5:]\n",
    "            n_spam += 1\n",
    "        inputs.append(txt)\n",
    "        labels.append(label)\n",
    "        \n",
    "print(\"Found {} ham and {} spam\".format(n_ham, n_spam))\n",
    "print(inputs[:5])\n",
    "\n",
    "# Convert them to arrays\n",
    "inputs = np.array(inputs).reshape(-1,1)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting train/valid/test\n",
    "\n",
    "Here we will split the data to three sets using `imbalanced-learn` library. Specifically we,\n",
    "\n",
    "* Create a balanced test set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced validation set with 100 spam and 100 ham (Random)\n",
    "* Create a balanced train set from the left over instances (undersampled using Near miss algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistics\n",
      "1    100\n",
      "0    100\n",
      "dtype: int64\n",
      "Valid statistics\n",
      "1    100\n",
      "0    100\n",
      "dtype: int64\n",
      "Train statistics\n",
      "0    4627\n",
      "1     547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import  NearMiss, RandomUnderSampler\n",
    "\n",
    "\n",
    "n=100 # Number of instances for each class for train/validation sets\n",
    "rus = RandomUnderSampler(sampling_strategy={0:n, 1:n}, random_state=random_seed)\n",
    "rus.fit_resample(inputs, labels)\n",
    "\n",
    "# Get test indices\n",
    "test_inds = rus.sample_indices_\n",
    "test_x, test_y = inputs[test_inds], np.array(labels)[test_inds]\n",
    "print(\"Test statistics\")\n",
    "print(pd.Series(test_y).value_counts())\n",
    "\n",
    "# Get rest (train + valid)\n",
    "rest_inds = [i for i in range(inputs.shape[0]) if i not in test_inds]\n",
    "rest_x, rest_y = inputs[rest_inds], labels[rest_inds]\n",
    "\n",
    "# Get valid indices\n",
    "rus.fit_resample(rest_x, rest_y)\n",
    "valid_inds = rus.sample_indices_\n",
    "valid_x, valid_y = rest_x[valid_inds], rest_y[valid_inds]\n",
    "print(\"Valid statistics\")\n",
    "print(pd.Series(valid_y).value_counts())\n",
    "\n",
    "# Rest goes in training\n",
    "train_inds = [i for i in range(rest_x.shape[0]) if i not in valid_inds]\n",
    "train_x, train_y = rest_x[train_inds], rest_y[train_inds]\n",
    "print(\"Train statistics\")\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    547\n",
      "0    547\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To use near miss algorithm, we need a numerical representation of the messages\n",
    "# We will use the bag of words representation\n",
    "countvec = CountVectorizer()\n",
    "train_bow = countvec.fit_transform(train_x.reshape(-1).tolist())\n",
    "\n",
    "# NearMiss is a common undersampling technique\n",
    "oss = NearMiss()\n",
    "X_res, y_res = oss.fit_resample(train_bow, train_y)\n",
    "train_inds = oss.sample_indices_\n",
    "\n",
    "train_x, train_y = train_x[train_inds], train_y[train_inds]\n",
    "\n",
    "print(pd.Series(train_y).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse some of the removed samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the BERT model\n",
    "\n",
    "Here we download the BERT model from the TensorFlow hub. and create a Keras layer from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "bert_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\"\n",
    "bert_layer = hub.KerasLayer(bert_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inputs for the BERT model\n",
    "\n",
    "BERT model needs three inputs,\n",
    "\n",
    "* Input word IDs - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* Input mask - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* Segment IDs - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 768)\n",
      "(None, 128, 768)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "\n",
    "# Contains input token ids\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "# Contains input mask values\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "# Contains input type (whether token belongs to sequence A or B) values\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
    "                            trainable=True)\n",
    "\n",
    "# Check the outputs of the Bert layer\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "print(pooled_output.shape)\n",
    "print(sequence_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file is at: b'C:\\\\Users\\\\thush\\\\AppData\\\\Local\\\\Temp\\\\tfhub_modules\\\\ce53fe6769d2ac3a260e92555120c54e1aecbea6\\\\assets\\\\vocab.txt'\n",
      "Configuration do_lower_case of BERT: True\n"
     ]
    }
   ],
   "source": [
    "import official.nlp.bert.tokenization as tokenization\n",
    "\n",
    "# Get the vocab file path from the BERT layer\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "print(\"Vocabulary file is at: {}\".format(vocab_file))\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "print(\"Configuration {} of BERT: {}\".format(\"do_lower_case\", do_lower_case))\n",
    "\n",
    "# Define a tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tokenization in BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: She sells seashells by the seashore\n",
      "Tokens generated by BERT: ['she', 'sells', 'seas', '##hell', '##s', 'by', 'the', 'seas', '##hore']\n",
      "Token IDs generated by BERT: [2016, 15187, 11915, 18223, 2015, 2011, 1996, 11915, 16892]\n"
     ]
    }
   ],
   "source": [
    "text = \"She sells seashells by the seashore\"\n",
    "print(\"Original text: {}\".format(text))\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens generated by BERT: {}\".format(tokens))\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs generated by BERT: {}\".format(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens used by BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: [CLS] has ID: 101\n",
      "Token: [SEP] has ID: 102\n",
      "Token: [MASK] has ID: 103\n",
      "Token: [PAD] has ID: 0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]', '[MASK]', '[PAD]']\n",
    "ids = tokenizer.convert_tokens_to_ids(special_tokens)\n",
    "for t, i in zip(special_tokens, ids):\n",
    "    print(\"Token: {} has ID: {}\".format(t, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the sentence in a way suitable to BERT\n",
    "\n",
    "Here we will add the `[CLS]` token to the front of the input and `[SEP]` to the back of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s):\n",
    "    \"\"\" Encode a given sentence by tokenizing it and adding special tokens \"\"\"\n",
    "    \n",
    "    tokens = ['[CLS]'] + list(tokenizer.tokenize(s)) + ['[SEP]']\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1094.000000\n",
       "mean       33.248629\n",
       "std        19.922181\n",
       "min         9.000000\n",
       "25%        15.000000\n",
       "50%        22.000000\n",
       "75%        53.000000\n",
       "90%        60.000000\n",
       "max        76.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = pd.Series([len(encode_sentence(str(s))) for s in train_x])\n",
    "seq_lengths.describe(percentiles=[0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the correct input format for BERT\n",
    "\n",
    "BERT model needs three inputs. These are formed into a dictionary having the following keys.\n",
    "\n",
    "* `input_word_ids` - These are the input tokens generated from text and padded to `max_seq_length` with zeros\n",
    "* `input_mask` - A matrix of the shape of `input_word_ids` that represents whether an element is a token of a padded value (0s and 1s)\n",
    "* `input_type_ids` - A matrix of the shape of `input_word_ids` that represents which sentence/sequence each token belongs to (0s and 1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the ragged input: (1094, None)\n",
      "Shape of the transformed input: (1094, 60)\n",
      "Shape of the ragged input: (200, None)\n",
      "Shape of the transformed input: (200, 60)\n",
      "Shape of the ragged input: (200, None)\n",
      "Shape of the transformed input: (200, 60)\n"
     ]
    }
   ],
   "source": [
    "def get_bert_inputs(docs,max_seq_len=None):\n",
    "    \"\"\" Generate inputs for BERT using a set of documents \"\"\"\n",
    "    \n",
    "    tokens = tf.ragged.constant([encode_sentence(str(s)) for s in docs])\n",
    "    \n",
    "    print(\"Shape of the ragged input: {}\".format(tokens.shape))\n",
    "    tokens_pad = tokens.to_tensor()\n",
    "    \n",
    "    if max_seq_len and max_seq_len - tokens_pad.shape[1]>0:\n",
    "        # If the specified max_seq_len is greater than the size of the padded tensor\n",
    "        more_pad = tf.zeros(shape=[tokens_pad.shape[0], max_seq_len - tokens_pad.shape[1]], dtype='int32')\n",
    "        tokens_pad = tf.concat([tokens_pad, more_pad], axis=1)\n",
    "    elif max_seq_len and max_seq_len - tokens_pad.shape[1]<0:\n",
    "        # If the specified max_seq_len is smaller than the size of the padded tensor\n",
    "        tokens_pad = tokens_pad[:, :max_seq_len]\n",
    "        \n",
    "    # Which are actual words\n",
    "    tokens_mask = tf.cast((tokens_pad != 0), 'float32')\n",
    "    # Which sentence each token belongs to\n",
    "    tokens_type = tf.zeros_like(tokens_pad)\n",
    "    print(\"Shape of the transformed input: {}\".format(tokens_pad.shape))\n",
    "    \n",
    "    # Final output\n",
    "    return {\n",
    "        'input_word_ids': tokens_pad,\n",
    "        'input_mask': tokens_mask,\n",
    "        'input_type_ids': tokens_type\n",
    "    }\n",
    "\n",
    "# Creating train/validation/test data\n",
    "train_inputs = get_bert_inputs(train_x, max_seq_len=60)\n",
    "valid_inputs = get_bert_inputs(valid_x, max_seq_len=60)\n",
    "test_inputs = get_bert_inputs(test_x, max_seq_len=60)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a downstream classifier from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Config\n",
      "{'vocab_size': 30522, 'hidden_size': 768, 'num_hidden_layers': 12, 'num_attention_heads': 12, 'hidden_act': 'gelu', 'intermediate_size': 3072, 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'type_vocab_size': 2, 'initializer_range': 0.02, 'embedding_size': None, 'backward_compatible': True, 'attention_dropout_rate': 0.1, 'dropout_rate': 0.1, 'hidden_activation': 'gelu', 'num_layers': 12}\n"
     ]
    }
   ],
   "source": [
    "from official.nlp import bert\n",
    "import yaml\n",
    "\n",
    "# https://github.com/tensorflow/models/blob/master/official/nlp/configs/models/bert_en_uncased_base.yaml\n",
    "with open(os.path.join(\"data\", \"bert_en_uncased_base.yaml\"), 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)['task']['model']['encoder']['bert']\n",
    "\n",
    "# Generate BERT config\n",
    "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n",
    "\n",
    "# Print BERT config\n",
    "print(\"BERT Config\")\n",
    "print(bert_config.to_dict())\n",
    "\n",
    "# Generating a classifier and the encoder\n",
    "hub_classifier, hub_encoder = bert.bert_models.classifier_model(\n",
    "    # Caution: Most of `bert_config` is ignored if you pass a hub url.\n",
    "    bert_config=bert_config, hub_module_url=bert_url, num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up epochs and steps\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "\n",
    "train_data_size = train_x.shape[0]\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
    "\n",
    "# creates an optimizer with learning rate schedule\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    2e-6, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning BERT and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "18/18 [==============================] - 544s 29s/step - loss: 0.7082 - accuracy: 0.4555 - val_loss: 0.6764 - val_accuracy: 0.5150\n",
      "Epoch 2/3\n",
      "18/18 [==============================] - 518s 29s/step - loss: 0.6645 - accuracy: 0.6589 - val_loss: 0.6480 - val_accuracy: 0.8150\n",
      "Epoch 3/3\n",
      "18/18 [==============================] - 518s 29s/step - loss: 0.6414 - accuracy: 0.7608 - val_loss: 0.6391 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x216b64bd0b8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model\n",
    "hub_classifier.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "hub_classifier.fit(\n",
    "      x=train_inputs, \n",
    "      y=train_y,\n",
    "      validation_data=(valid_inputs, valid_y),\n",
    "      validation_batch_size=eval_batch_size,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(hub_classifier, os.path.join('models', 'bert_spam.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 22s 3s/step - loss: 0.6432 - accuracy: 0.7950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6432445049285889, 0.7950000166893005]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub_classifier.evaluate(test_inputs, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
