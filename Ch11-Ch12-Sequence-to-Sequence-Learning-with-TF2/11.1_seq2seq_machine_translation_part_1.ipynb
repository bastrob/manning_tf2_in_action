{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq models (Sequence-to-Sequence)\n",
    "\n",
    "Sequence to sequence models are a variant of deep learning models that consists of an encoder and a decoder. They are used for problems that map an abitrarily long sequence to another arbitrarliy long sequence. For example, in machine translation, you convert a sequence of words in a source language to a sequence of words in a target language. Here we will see how we can use a seq2seq model to solve a machine translation task to convert English to German.\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch11-Ch12-Sequence-to-Sequence-Learning-with-TF2/11.1_seq2seq_machine_translation_part_1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    " \n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.manythings.org/anki/\n",
    "    \n",
    "german-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    }
   ],
   "source": [
    "# Not setting this led to the following error\n",
    "# _Derived_]RecvAsync is cancelled.   \n",
    "# [[{{node gradient_tape/model_1/embedding_1/embedding_lookup/Reshape/_172}}]] [Op:__inference_train_function_31985]\n",
    "\n",
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (Requires manual download)\n",
    "\n",
    "Unfortunately, this dataset **must be manually downloaded** by clicking [this link](http://www.manythings.org/anki/deu-eng.zip). Then place the downloaded `deu-eng.zip` file in the `Ch11/data` folder before running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Make sure the zip file has been downloaded\n",
    "if not os.path.exists(os.path.join('data','deu-eng.zip')):\n",
    "    raise FileNotFoundError(\n",
    "        \"Uh oh! Did you download the deu-eng.zip from http://www.manythings.org/anki/deu-eng.zip manually and place it in the Ch11/data folder?\"\n",
    "    )\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(os.path.join('data', 'deu.txt')):\n",
    "        with zipfile.ZipFile(os.path.join('data','deu-eng.zip'), 'r') as zip_ref:\n",
    "            zip_ref.extractall('data')\n",
    "    else:\n",
    "        print(\"The extracted data already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data\n",
    "\n",
    "Data is in a single `.txt` file. It is a parallel corpus meaning there is a English sentence/phrase/paragraph and a corresponding German translation of it side-by-side. In the file, the source input and the translation are separated by a tab (i.e. tab-seperated file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape = (227080, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv(os.path.join('data', 'deu.txt'), delimiter='\\t', header=None)\n",
    "# Set column names\n",
    "df.columns = [\"EN\", \"DE\", \"Attribution\"]\n",
    "df = df[[\"EN\", \"DE\"]]\n",
    "print('df.shape = {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Geh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run.</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EN          DE\n",
       "0   Go.        Geh.\n",
       "1   Hi.      Hallo!\n",
       "2   Hi.  Grüß Gott!\n",
       "3  Run!       Lauf!\n",
       "4  Run.       Lauf!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EN</th>\n",
       "      <th>DE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227075</th>\n",
       "      <td>Even if some sentences by non-native speakers ...</td>\n",
       "      <td>Auch wenn Sätze von Nichtmuttersprachlern mitu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227076</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227077</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227078</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der nicht weiß, woher man kommt, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227079</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Ohne Zweifel findet sich auf dieser Welt zu je...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       EN  \\\n",
       "227075  Even if some sentences by non-native speakers ...   \n",
       "227076  If someone who doesn't know your background sa...   \n",
       "227077  If someone who doesn't know your background sa...   \n",
       "227078  If someone who doesn't know your background sa...   \n",
       "227079  Doubtless there exists in this world precisely...   \n",
       "\n",
       "                                                       DE  \n",
       "227075  Auch wenn Sätze von Nichtmuttersprachlern mitu...  \n",
       "227076  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
       "227077  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "227078  Wenn jemand, der nicht weiß, woher man kommt, ...  \n",
       "227079  Ohne Zweifel findet sich auf dieser Welt zu je...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a smaller sample for computational speed\n",
    "\n",
    "There are more than 220000 samples in the original dataset. We will be using a smaller set of 50000 for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50000\n",
    "df = df.sample(n=n_samples, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the `SOS` and `EOS` tokens (Decoder)\n",
    "\n",
    "We will add these special tokens to the translated targets. `sos` indicates the start of the sentence and `eos` marks the end of the sentence. \n",
    "\n",
    "E.g. `Grüß Gott!` becomes `sos Grüß Gott! eos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = 'sos'\n",
    "end_token = 'eos'\n",
    "\n",
    "df[\"DE\"] = start_token + ' ' + df[\"DE\"] + ' ' + end_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data\n",
    "\n",
    "We will be creating three datasets by sampling randomly (without replacement);\n",
    "\n",
    "* Test dataset - 5000 samples\n",
    "* Validation dataset - 5000 samples\n",
    "* Training dataset - 40000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_df.shape = (500, 2)\n",
      "valid_df.shape = (500, 2)\n",
      "train_df.shape = (4000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "test_df = df.sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Randomly sample 5000 examples from the total 50000 randomly\n",
    "valid_df = df.loc[~df.index.isin(test_df.index)].sample(n=int(n_samples/10), random_state=random_seed)\n",
    "# Assign the rest to training data\n",
    "train_df = df.loc[~(df.index.isin(test_df.index) | df.index.isin(valid_df.index))]\n",
    "\n",
    "print('test_df.shape = {}'.format(test_df.shape))\n",
    "print('valid_df.shape = {}'.format(valid_df.shape))\n",
    "print('train_df.shape = {}'.format(train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary sizes (English and German)\n",
    "\n",
    "Calculate the vocabulary size. We will only consider the words that appear at least 10 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "Tom    903\n",
      "to     873\n",
      "I      836\n",
      "the    738\n",
      "a      620\n",
      "you    616\n",
      "is     422\n",
      "of     280\n",
      "in     272\n",
      "was    237\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 359\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "sos      8000\n",
      "eos      8000\n",
      "Tom       956\n",
      "Ich       777\n",
      "nicht     494\n",
      "ist       429\n",
      "Sie       364\n",
      "zu        346\n",
      "du        318\n",
      "das       298\n",
      "dtype: int64\n",
      "\n",
      "Vocabulary size (>=10 frequent): 336\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a flattened list from English words\n",
    "en_words = train_df[\"EN\"].str.split().sum()\n",
    "# Create a flattened list of German words\n",
    "de_words = train_df[\"DE\"].str.split().sum()\n",
    "\n",
    "# Get the vocabulary size of words appearing more than or equal to 10 times\n",
    "n=10\n",
    "\n",
    "def get_vocabulary_size_greater_than(words, n, verbose=True):\n",
    "    \n",
    "    \"\"\" Get the vocabulary size above a certain threshold \"\"\"\n",
    "    \n",
    "    # Generate a counter object i.e. dict word -> frequency\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    # Create a pandas series from the counter, then sort most frequent to least\n",
    "    freq_df = pd.Series(list(counter.values()), index=list(counter.keys())).sort_values(ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        # Print most common words\n",
    "        print(freq_df.head(n=10))\n",
    "\n",
    "    # Count of words >= n frequent    \n",
    "    n_vocab = (freq_df>=n).sum()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nVocabulary size (>={} frequent): {}\".format(n, n_vocab))\n",
    "        \n",
    "    return n_vocab\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "en_vocab = get_vocabulary_size_greater_than(en_words, n)\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "de_vocab = get_vocabulary_size_greater_than(de_words, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length (English and German)\n",
    "\n",
    "Here we compute the sequence length of the sequences in the English and German corpora. To ignore the outliers, we only consider data between the 1% and 99% quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 6.0\n",
      "\n",
      "count    4000.000000\n",
      "mean        6.369250\n",
      "std         2.588548\n",
      "min         1.000000\n",
      "25%         5.000000\n",
      "50%         6.000000\n",
      "75%         8.000000\n",
      "max        29.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    3957.000000\n",
      "mean        6.268132\n",
      "std         2.376713\n",
      "min         2.000000\n",
      "25%         5.000000\n",
      "50%         6.000000\n",
      "75%         8.000000\n",
      "max        14.000000\n",
      "Name: EN, dtype: float64\n",
      "\n",
      "German corpus\n",
      "==================================================\n",
      "\n",
      "Some summary statistics\n",
      "Median length: 10.0\n",
      "\n",
      "count    4000.000000\n",
      "mean       10.394000\n",
      "std         2.590549\n",
      "min         5.000000\n",
      "25%         9.000000\n",
      "50%        10.000000\n",
      "75%        12.000000\n",
      "max        29.000000\n",
      "Name: DE, dtype: float64\n",
      "\n",
      "Computing the statistics between the 1% and 99% quantiles (to ignore outliers)\n",
      "count    3913.000000\n",
      "mean       10.300281\n",
      "std         2.311734\n",
      "min         7.000000\n",
      "25%         9.000000\n",
      "50%        10.000000\n",
      "75%        12.000000\n",
      "max        18.000000\n",
      "Name: DE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def print_sequence_length(str_ser):\n",
    "    \n",
    "    \"\"\" Print the summary stats of the sequence length \"\"\"\n",
    "    \n",
    "    # Create a pd.Series, which contain the sequence length for each review\n",
    "    seq_length_ser = str_ser.str.split(' ').str.len()\n",
    "\n",
    "    # Get the median as well as summary statistics of the sequence length\n",
    "    print(\"\\nSome summary statistics\")\n",
    "    print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "    print(seq_length_ser.describe())\n",
    "    \n",
    "    # Get the quantiles at given marks\n",
    "    print(\"\\nComputing the statistics between the 1% and 99% quantiles (to ignore outliers)\")\n",
    "    p_01 = seq_length_ser.quantile(0.01)\n",
    "    p_99 = seq_length_ser.quantile(0.99)\n",
    "    \n",
    "    # Print the summary stats of the data between the defined quantlies\n",
    "    print(seq_length_ser[(seq_length_ser >= p_01) & (seq_length_ser < p_99)].describe())\n",
    "\n",
    "print(\"English corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"EN\"])\n",
    "\n",
    "print(\"\\nGerman corpus\")\n",
    "print('='*50)\n",
    "print_sequence_length(train_df[\"DE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the vocabulary size and sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN vocabulary size: 359\n",
      "DE vocabulary size: 336\n",
      "EN max sequence length: 19\n",
      "DE max sequence length: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"EN vocabulary size: {}\".format(en_vocab))\n",
    "print(\"DE vocabulary size: {}\".format(de_vocab))\n",
    "\n",
    "# Define sequence lengths with some extra space for longer sequences\n",
    "en_seq_length = 19\n",
    "de_seq_length = 21\n",
    "\n",
    "print(\"EN max sequence length: {}\".format(en_seq_length))\n",
    "print(\"DE max sequence length: {}\".format(de_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow `TextVectorization` layer\n",
    "\n",
    "The `TextVectorization` layer takes in strings and convert them to token IDs. The layer can build a vocabulary using a given text corups and uses that to generate the token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the vectorization layer for English\n",
      "Fitting the EN vectorization layer on data\n",
      "\tDone\n",
      "\n",
      "Defined the vectorization layer for German\n",
      "Fitting the DE vectorization layer on data\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "print(\"Defined the vectorization layer for English\")\n",
    "\n",
    "# Create the text vectorization layer (English)\n",
    "en_vectorize_layer = TextVectorization(\n",
    "    max_tokens=en_vocab,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=None\n",
    ")\n",
    "\n",
    "print(\"Fitting the EN vectorization layer on data\")\n",
    "# Here we are calling adapt to fit the vectorization layer with text\n",
    "# so that it learns the vocabulary\n",
    "en_vectorize_layer.adapt(np.array(train_df[\"EN\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")\n",
    "\n",
    "print(\"\\nDefined the vectorization layer for German\")\n",
    "\n",
    "# Create the text vectorization layer (German)\n",
    "de_vectorize_layer = TextVectorization(\n",
    "    max_tokens=de_vocab,    \n",
    "    output_mode='int',\n",
    "    output_sequence_length=de_seq_length,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "print(\"Fitting the DE vectorization layer on data\")\n",
    "de_vectorize_layer.adapt(np.array(train_df[\"DE\"].tolist()).astype('str'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TextVectorization` layer in action\n",
    " \n",
    "### How to use the layer (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "[['run'], [\"I'll go home\"], ['ectoplasmic residue']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[  1   0   0]\n",
      " [ 72  46 110]\n",
      " [  1   1   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(en_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"run\"], [\"I\\'ll go home\"],[\"ectoplasmic residue\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the layer (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "[['[sos] Geh'], ['geh lauf']]\n",
      "\n",
      "\n",
      "Token IDs: \n",
      "[[  2 591   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [591   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "toy_model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "toy_model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "toy_model.add(de_vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, \n",
    "input_data = [[\"[sos] Geh\"], [\"geh lauf\"]]\n",
    "pred = toy_model.predict(input_data)\n",
    "\n",
    "print(\"Input data: \\n{}\\n\".format(input_data))\n",
    "print(\"\\nToken IDs: \\n{}\".format(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of the vocabulary\n",
    "\n",
    "Let's print some words from the two vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "['', '[UNK]', 'tom', 'you', 'to', 'the', 'i', 'a', 'is', 'that']\n",
      "349\n",
      "\n",
      "German\n",
      "['', '[UNK]', 'sos', 'eos', 'tom', 'ich', 'nicht', 'ist', 'du', 'sie']\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "print(\"English\")\n",
    "# Print first few words in the vocabulary\n",
    "print(en_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(en_vectorize_layer.get_vocabulary()))\n",
    "\n",
    "print(\"\\nGerman\")\n",
    "# Print first few words in the vocabulary\n",
    "print(de_vectorize_layer.get_vocabulary()[:10])\n",
    "# Print the size of the vocabulary\n",
    "print(len(de_vectorize_layer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Seq2Seq model\n",
    "\n",
    "Here we define an encoder decoder model to translate between English and German. We will be using a bidirectional encoder and a standard decoder. The model will use Gated Recurrent Unit (GRU) as the recurrent component. The encoder and the decoder has their own `TextVectorization` layers as they use two different languages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_vectorizer(corpus, n_vocab, max_length=None, return_vocabulary=True, name=None):\n",
    "    \n",
    "    \"\"\" Return a text vectorization layer or a model \"\"\"\n",
    "    \n",
    "    # Definie an input layer that takes a list of strings (or an array of strings)\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='encoder_input')\n",
    "    \n",
    "    # When defining the vocab size, we'd add two for special tokens '' (Padding) and '[UNK]' (Oov tokens)\n",
    "    vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "        max_tokens=n_vocab+2,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length,                \n",
    "    )\n",
    "    \n",
    "    # Fit the vectorizer layer on the data\n",
    "    vectorize_layer.adapt(corpus)\n",
    "        \n",
    "    # Get the token IDs\n",
    "    vectorized_out = vectorize_layer(inp)\n",
    "        \n",
    "    if not return_vocabulary: \n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name)    \n",
    "    else:\n",
    "        # Returns the vocabulary in addition to the model\n",
    "        return tf.keras.models.Model(inputs=inp, outputs=vectorized_out, name=name), vectorize_layer.get_vocabulary()\n",
    "    \n",
    "        \n",
    "def get_encoder(n_vocab, vectorizer):\n",
    "    \"\"\" Define the encoder of the seq2seq model\"\"\"\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input')\n",
    "\n",
    "    # Vectorize the data (assign token IDs)\n",
    "    vectorized_out = vectorizer(inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='e_embedding')\n",
    "    # Get the embeddings of the token IDs\n",
    "    emb_out = emb_layer(vectorized_out)\n",
    "    \n",
    "    # Define a bidirectional GRU layer\n",
    "    # Encoder looks at the english text (i.e. the input) both backwards and forward\n",
    "    # this leads to better performance\n",
    "    gru_layer = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, name='e_gru'), name='e_bidirectional_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    gru_out = gru_layer(emb_out)\n",
    "    \n",
    "    # Define the encoder model\n",
    "    encoder = tf.keras.models.Model(inputs=inp, outputs=gru_out, name='encoder')\n",
    "        \n",
    "    return encoder\n",
    "\n",
    "\n",
    "def get_final_seq2seq_model(n_vocab, encoder, vectorizer):\n",
    "    \"\"\" Define the final encoder-decoder model \"\"\"\n",
    "    \n",
    "    # Encoder's input\n",
    "    e_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='e_input_final')    \n",
    "    # Get the encoders final output\n",
    "    d_init_state = encoder(e_inp)\n",
    "    \n",
    "    # The input is (None,1) shaped and accepts an array of strings\n",
    "    # This input layer is used to train the seq2seq model with teacher-forcing\n",
    "    # we feed the German sequence as the input and ask the model to predict \n",
    "    # it with the words offset by 1 (i.e. next word)\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_input')\n",
    "    \n",
    "    # Vectorize the data (assign token IDs)\n",
    "    d_vectorized_out = vectorizer(d_inp)\n",
    "    \n",
    "    # Define an embedding layer to convert IDs to word vectors\n",
    "    # Note that this is a different embedding layer to the encoder's embedding layer\n",
    "    d_emb_layer = tf.keras.layers.Embedding(n_vocab+2, 128, mask_zero=True, name='d_embedding')\n",
    "    \n",
    "    # Get the embeddings of the token IDs\n",
    "    d_emb_out = d_emb_layer(d_vectorized_out)\n",
    "    \n",
    "    # Define a GRU layer\n",
    "    # Unlike the encoder, we cannot define a bidirectional GRU for the decoder\n",
    "    # Why?\n",
    "    d_gru_layer = tf.keras.layers.GRU(256, return_sequences=True, name='d_gru')\n",
    "    \n",
    "    # Get the output of the gru layer\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_init_state)\n",
    "    \n",
    "    # Define an intermediate dense layer\n",
    "    d_dense_layer_1 = tf.keras.layers.Dense(512, activation='relu', name='d_dense_1')\n",
    "    d_dense1_out = d_dense_layer_1(d_gru_out)\n",
    "    \n",
    "    # The final prediction layer with softmax\n",
    "    d_dense_layer_final = tf.keras.layers.Dense(n_vocab+2, activation='softmax', name='d_dense_final')\n",
    "    d_final_out = d_dense_layer_final(d_dense1_out)\n",
    "    \n",
    "    # Define the full model\n",
    "    seq2seq = tf.keras.models.Model(inputs=[e_inp, d_inp], outputs=d_final_out, name='final_seq2seq')\n",
    "    \n",
    "    return seq2seq\n",
    "\n",
    "# Get the English vectorizer/vocabulary\n",
    "en_vectorizer, en_vocabulary = get_vectorizer(np.array(train_df[\"EN\"].tolist()), en_vocab, max_length=en_seq_length, name='e_vectorizer')\n",
    "# Get the German vectorizer/vocabulary\n",
    "de_vectorizer, de_vocabulary = get_vectorizer(np.array(train_df[\"DE\"].tolist()), de_vocab, max_length=de_seq_length-1, name='d_vectorizer')\n",
    "\n",
    "# Define the final model\n",
    "encoder = get_encoder(en_vocab, en_vectorizer)\n",
    "final_model = get_final_seq2seq_model(de_vocab, encoder, de_vectorizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "Compile the model with a suitable loss, an optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"final_seq2seq\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "d_input (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_vectorizer (Functional)       (None, 20)           0           d_input[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "e_input_final (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "d_embedding (Embedding)         (None, 20, 128)      319872      d_vectorizer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, 256)          484864      e_input_final[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "d_gru (GRU)                     (None, 20, 256)      296448      d_embedding[0][0]                \n",
      "                                                                 encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_1 (Dense)               (None, 20, 512)      131584      d_gru[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "d_dense_final (Dense)           (None, 20, 2499)     1281987     d_dense_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,514,755\n",
      "Trainable params: 2,514,755\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "# Compile the model\n",
    "final_model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating MT models - BLEU metric\n",
    "\n",
    "In machine translation, a popular choice for assessing performance is the BiLingual Evaluation Understudy (BLEU) metric. Word-to-word accuracy does not reflect the true performance of these models as there can be different ways the same phrase can be translated to. BLEU can take into account such multiple translations when computing the final score. Furthermore, BLEU is superior because it measures precision at multiple n-gram scales between the actual and predicted translations.\n",
    "\n",
    "The implementation is inspired by: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n",
    "\n",
    "### Defining the BLEU metric\n",
    "\n",
    "Below we define a `BLEUMetric` object that can be used to compute the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "class BLEUMetric(object):\n",
    "    \n",
    "    def __init__(self, vocabulary, name='perplexity', **kwargs):\n",
    "      \"\"\" Computes the BLEU score (Metric for machine translation) \"\"\"\n",
    "      super().__init__()\n",
    "      self.vocab = vocabulary\n",
    "      self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "    \n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\" Calculate the BLEU score for targets and predictions \"\"\"\n",
    "        \n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)  \n",
    "        \n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "        \n",
    "        def clean_text(tokens):\n",
    "            \n",
    "            \"\"\" Clean padding and [SOS]/[EOS] tokens to only keep meaningful words \"\"\"\n",
    "            \n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                        # 2. Replace everything after the eos token with blank\n",
    "                        tf.strings.regex_replace(\n",
    "                            # 1. Join all the tokens to one string in each sequence\n",
    "                            tf.strings.join(\n",
    "                                tf.transpose(tokens), separator=' '\n",
    "                            ),\n",
    "                        \"eos.*\", \"\"),\n",
    "                   )\n",
    "            \n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding='utf-8'\n",
    "            )\n",
    "            \n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [sent if len(sent)>0 else '[UNK]' for sent in translations ]\n",
    "            \n",
    "            # Split the sequences to individual tokens \n",
    "            translations = np.char.split(translations).tolist()\n",
    "            \n",
    "            return translations\n",
    "        \n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the BLEU metric\n",
    "\n",
    "Below you can see BLEU being used to computer the similarity between a translation (predicted) and reference (true target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [['[UNK]', '[UNK]', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [['[UNK]', 'einmal', 'mÃssen', '[UNK]', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]\n",
    "reference = [[['als', 'mÃssen', 'mÃssen', 'wir', 'in', 'erfahrung', 'bringen', 'wo', 'sie', 'wohnen']]]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(\"BLEU score with longer correctly predicte phrases: {}\".format(bleu1))\n",
    "print(\"BLEU score without longer correctly predicte phrases: {}\".format(bleu2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with a custom loop\n",
    "\n",
    "We will train the model using a custom loop as we want to incorporate BLEU as a metric in our training. We will follow the following procedure;\n",
    "\n",
    "* Each epoch,\n",
    "  * Shuffle the training data\n",
    "  * Train our model on all the training data (in batches)\n",
    "  * Evaluate the model on validation data\n",
    "* Finally, evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def prepare_data(train_df, valid_df, test_df):\n",
    "    \"\"\" Create a data dictionary from the dataframes containing data \"\"\"\n",
    "    \n",
    "    data_dict = {}\n",
    "    for label, df in zip(['train', 'valid', 'test'], [train_df, valid_df, test_df]):\n",
    "        en_inputs = np.array(df[\"EN\"].tolist())\n",
    "        de_inputs = np.array(df[\"DE\"].str.rsplit(n=1, expand=True).iloc[:,0].tolist())\n",
    "        de_labels = np.array(df[\"DE\"].str.split(n=1, expand=True).iloc[:,1].tolist())\n",
    "        data_dict[label] = {'encoder_inputs': en_inputs, 'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n",
    "    \"\"\" Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "        \n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "    \n",
    "    # Return shuffled data\n",
    "    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], de_labels[shuffle_inds]), shuffle_inds\n",
    "\n",
    "\n",
    "def evaluate_model(model, vectorizer, en_inputs_raw, de_inputs_raw, de_labels_raw, batch_size):\n",
    "    \"\"\" Evaluate the model on various metrics such as loss, accuracy and BLEU \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "    \n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0]//batch_size\n",
    "    print(\" \", end='\\r')\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(\"Evaluating batch {}/{}\".format(i+1, n_batches), end='\\r')\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "        y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "    \n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)\n",
    "    \n",
    "        \n",
    "def train_model(model, vectorizer, train_df, valid_df, test_df, epochs, batch_size):\n",
    "    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n",
    "    \n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(train_df, valid_df, test_df)\n",
    "\n",
    "    shuffle_inds = None\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        (en_inputs_raw,de_inputs_raw,de_labels_raw), shuffle_inds  = shuffle_data(\n",
    "            data_dict['train']['encoder_inputs'],\n",
    "            data_dict['train']['decoder_inputs'],\n",
    "            data_dict['train']['decoder_labels'],\n",
    "            shuffle_inds\n",
    "        )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0]//batch_size\n",
    "\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i+1, n_train_batches), end='\\r')\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = vectorizer(de_labels_raw[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)        \n",
    "            # Evaluate the model to get the metrics\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "            # Get the final prediction to compute BLEU\n",
    "            pred_y = model.predict(x)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "            bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        val_en_inputs = data_dict['valid']['encoder_inputs']\n",
    "        val_de_inputs = data_dict['valid']['decoder_inputs']\n",
    "        val_de_labels = data_dict['valid']['decoder_labels']\n",
    "            \n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model, vectorizer, val_en_inputs, val_de_inputs, val_de_labels, batch_size\n",
    "        )\n",
    "            \n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n",
    "        print(\"\\t(train) loss: {} - accuracy: {} - bleu: {}\".format(np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)))\n",
    "        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(val_loss, val_accuracy, val_bleu))\n",
    "    \n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #    \n",
    "    \n",
    "    test_en_inputs = data_dict['test']['encoder_inputs']\n",
    "    test_de_inputs = data_dict['test']['decoder_inputs']\n",
    "    test_de_labels = data_dict['test']['decoder_labels']\n",
    "            \n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model, vectorizer, test_en_inputs, test_de_inputs, test_de_labels, batch_size\n",
    "    )\n",
    "    \n",
    "    print(\"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(test_loss, test_accuracy, test_bleu))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating batch 39/39\n",
      "Epoch 1/5\n",
      "\t(train) loss: 1.777197730846894 - accuracy: 0.2397870160639286 - bleu: 0.0021600331903447365\n",
      "\t(valid) loss: 1.4460796943077674 - accuracy: 0.3329461751840053 - bleu: 0.010298257286395246\n",
      "Evaluating batch 39/39\n",
      "Epoch 2/5\n",
      "\t(train) loss: 1.3275893521614563 - accuracy: 0.36628206876608044 - bleu: 0.027879984428417225\n",
      "\t(valid) loss: 1.2044261266023686 - accuracy: 0.4015590296341823 - bleu: 0.051192284743172764\n",
      "Evaluating batch 39/39\n",
      "Epoch 3/5\n",
      "\t(train) loss: 1.1008458487116373 - accuracy: 0.4331808953713148 - bleu: 0.06821765584707452\n",
      "\t(valid) loss: 1.052441520568652 - accuracy: 0.45099978951307446 - bleu: 0.08018564473679013\n",
      "Evaluating batch 39/39\n",
      "Epoch 4/5\n",
      "\t(train) loss: 0.9400282400922898 - accuracy: 0.48275901424961215 - bleu: 0.10268714198449753\n",
      "\t(valid) loss: 0.9539370414538261 - accuracy: 0.4860225831850981 - bleu: 0.10763015207980114\n",
      "Evaluating batch 39/39\n",
      "Epoch 5/5\n",
      "\t(train) loss: 0.8170255527664454 - accuracy: 0.5254948609150373 - bleu: 0.13837378169400624\n",
      "\t(valid) loss: 0.8887128875805781 - accuracy: 0.5113050700762333 - bleu: 0.12647888980839941\n",
      "Evaluating batch 39/39\n",
      "(test) loss: 0.9102572844578669 - accuracy: 0.5060969445949945 - bleu: 0.12226939517811838\n",
      "\n",
      "It took 256.77871012687683 seconds to complete the training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()    \n",
    "train_model(final_model, de_vectorizer, train_df, valid_df, test_df, epochs, batch_size)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"\\nIt took {} seconds to complete the training\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model\n",
    "\n",
    "We save the trained model as well as the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model\n",
    "os.makedirs('models', exist_ok=True)\n",
    "tf.keras.models.save_model(final_model, os.path.join('models', 'seq2seq'))\n",
    "\n",
    "import json\n",
    "os.makedirs(os.path.join('models', 'seq2seq_vocab'), exist_ok=True)\n",
    "\n",
    "# Save the vocabulary files\n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'en_vocab.json'), 'w') as f:\n",
    "    json.dump(en_vocabulary, f)    \n",
    "with open(os.path.join('models', 'seq2seq_vocab', 'de_vocab.json'), 'w') as f:\n",
    "    json.dump(de_vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the inference model\n",
    "\n",
    "For inference we have to create a new model using the weights of the trained model. During training we used teacher forcing, i.e. providing words from the translation as inputs to the decoder. This cannot be done during inference as we do not have a translation, but want to generate one.\n",
    "\n",
    "Therefore, we create a decoder model that can generate one prediction at a time. We start the prediction process by giving the `sos` token as the initial input to the decoder and keep generating words until the decoder outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabularies\n",
      "Loading weights and generating the inference model\n",
      "\tDone\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "K.clear_session()\n",
    "\n",
    "def get_inference_model(save_path):\n",
    "    \"\"\" Load the saved model and create an inference model from that \"\"\"\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(save_path)\n",
    "    \n",
    "    # Get the encoder model\n",
    "    en_model = model.get_layer(\"encoder\")\n",
    "    \n",
    "    # Define two inputs\n",
    "    # 1. Takes a single word as the input to the decoder\n",
    "    d_inp = tf.keras.Input(shape=(1,), dtype=tf.string, name='d_infer_input')\n",
    "    # 2. Takes an initial state to pass to the decoder GRU as an input\n",
    "    d_state_inp = tf.keras.Input(shape=(256,), name='d_infer_state')\n",
    "    \n",
    "    # Generate the vectorized output of inp\n",
    "    d_vectorizer = model.get_layer('d_vectorizer')    \n",
    "    d_vectorized_out = d_vectorizer(d_inp)\n",
    "    \n",
    "    # Generate the embeddings from the vectorized input\n",
    "    d_emb_out = model.get_layer('d_embedding')(d_vectorized_out)\n",
    "    \n",
    "    # Get the GRU layer\n",
    "    d_gru_layer = model.get_layer(\"d_gru\")\n",
    "    # Since we generate one word at a time, we will not need the return_sequences\n",
    "    d_gru_layer.return_sequences = False\n",
    "    # Get the GRU out while using d_state_inp from earlier, as the initial state\n",
    "    d_gru_out = d_gru_layer(d_emb_out, initial_state=d_state_inp) \n",
    "    \n",
    "    # Get the dense output\n",
    "    d_dense1_out = model.get_layer(\"d_dense_1\")(d_gru_out) \n",
    "    \n",
    "    # Get the final output\n",
    "    d_final_out = model.get_layer(\"d_dense_final\")(d_dense1_out) \n",
    "    \n",
    "    # Define the final decoder\n",
    "    de_model = tf.keras.models.Model(inputs=[d_inp, d_state_inp], outputs=[d_final_out, d_gru_out])\n",
    "    \n",
    "    return en_model, de_model\n",
    "\n",
    "def get_vocabularies(save_dir):\n",
    "    \"\"\" Load the vocabulary files from a given path\"\"\"\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'en_vocab.json'), 'r') as f:\n",
    "        en_vocabulary = json.load(f)\n",
    "        \n",
    "    with open(os.path.join(save_dir, 'de_vocab.json'), 'r') as f:\n",
    "        de_vocabulary = json.load(f)\n",
    "        \n",
    "    return en_vocabulary, de_vocabulary\n",
    "\n",
    "print(\"Loading vocabularies\")\n",
    "en_vocabulary, de_vocabulary = get_vocabularies(os.path.join('models', 'seq2seq_vocab'))\n",
    "\n",
    "print(\"Loading weights and generating the inference model\")\n",
    "en_model, de_model = get_inference_model(os.path.join('models', 'seq2seq'))\n",
    "print(\"\\tDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new translations\n",
    "\n",
    "Here we generate a new translation by first starting with the `sos` token and asking the decoder to generate words until it outputs `eos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The pleasure's all mine.\n",
      "Translation: die [UNK] [UNK] mir eos\n",
      "\n",
      "Input: Tom was asking for it.\n",
      "Translation: tom sprach es zu tun eos\n",
      "\n",
      "Input: He denied having been involved in the affair.\n",
      "Translation: er [UNK] sich auf das [UNK] [UNK] eos\n",
      "\n",
      "Input: Is there something in particular that you want to drink?\n",
      "Translation: gibt es etwas [UNK] wenn du etwas [UNK] eos\n",
      "\n",
      "Input: Don't run. Walk slowly.\n",
      "Translation: [UNK] nicht zu fuß eos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text):\n",
    "    \"\"\" Generate a new translation \"\"\"\n",
    "    \n",
    "    start_token = 'sos'\n",
    "    \n",
    "    # Print the input\n",
    "    print(\"Input: {}\".format(sample_en_text))\n",
    "    \n",
    "    # Get the initial state for the decoder\n",
    "    d_state = en_model.predict(np.array([sample_en_text]))\n",
    "    # First word will be sos\n",
    "    de_word = start_token\n",
    "    # We collect the translation in this list\n",
    "    de_translation = []\n",
    "    \n",
    "    # Keep predicting until we get eos\n",
    "    while de_word != 'eos':\n",
    "        # Override the previous state input with the new state\n",
    "        de_pred, d_state = de_model.predict([np.array([de_word]), d_state])    \n",
    "        # Get the actual word from the token ID of the prediction\n",
    "        de_word = de_vocabulary[np.argmax(de_pred[0])]\n",
    "        # Add that to the translation\n",
    "        de_translation.append(de_word)\n",
    "\n",
    "    print(\"Translation: {}\\n\".format(' '.join(de_translation)))\n",
    "\n",
    "for i in range(5):\n",
    "    sample_en_text = test_df[\"EN\"].iloc[i]\n",
    "    generate_new_translation(en_model, de_model, de_vocabulary, sample_en_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
