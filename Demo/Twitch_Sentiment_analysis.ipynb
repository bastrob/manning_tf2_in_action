{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with TensorFlow\n",
    "\n",
    "\n",
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* MLOps: Deploying your models\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, CSVLogger\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from PIL import Image\n",
    "from PIL.PngImagePlugin import PngImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "# http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):\n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):\n",
    "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
    "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2015</td>\n",
       "      <td>ok game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>12 25, 2014</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime  \\\n",
       "0        5      True  10 17, 2015   \n",
       "1        4     False  07 27, 2015   \n",
       "2        3      True  02 23, 2015   \n",
       "3        2      True  02 20, 2015   \n",
       "4        5      True  12 25, 2014   \n",
       "\n",
       "                                          reviewText  \n",
       "0  This game is a bit hard to get the hang of, bu...  \n",
       "1  I played it a while but it was alright. The st...  \n",
       "2                                           ok game.  \n",
       "3  found the game a bit too complicated, not what...  \n",
       "4  great game, I love it and have played it since...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file\n",
    "review_df = pd.read_json(os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records')\n",
    "# Select on the columns we're interested in \n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (497577, 4)\n",
      "After cleaning up: (497419, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning up: {}\".format(review_df.shape))\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n",
    "print(\"After cleaning up: {}\".format(review_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verified reviews vs unverified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     332504\n",
       "False    164915\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"verified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    222335\n",
       "4     54878\n",
       "3     27973\n",
       "1     15200\n",
       "2     12118\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
    "verified_df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping stars to positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\manning.tf2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    277213\n",
       "0     55291\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas map function to map different star ratings to 0/1\n",
    "verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n",
    "verified_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are sampling 100% of the data in a random fashion, leading to a shuffled dataset\n",
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "\n",
    "# Splint the data to inputs (inputs) and targets (labels)\n",
    "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n",
    "\n",
    "Here we perform preprocessing. Mainly we're going to focus on the following \n",
    "\n",
    "* Lower case (nltk) - Turn \"I am\" to \"i am\"\n",
    "* Remove numbers (regex) - Turn \"i am 24 years old\" to \"i am years old\"\n",
    "* Remove stop words (nltk) - Turn \"i go to the shop\" to \"i go shop\"\n",
    "* Lemmatize (nltk) - Turn \"i went to buy flowers\" to \"i go to buy flower\"\n",
    "\n",
    "Preprocessing helps to reduce the features space, thus the model learning faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to nltk...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to nltk...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean: She sells seashells by the seashore.\n",
      "After clean: ['sells', 'seashells', 'seashore']\n",
      "Sample data\n",
      "470225    [this, favorie, game, far, ds, use, internet, ...\n",
      "297085    [9, year, old, girl, really, likes, game, im, ...\n",
      "413744    [gorgeous, graphics, catchy, tune, fun, game, ...\n",
      "495844                                   [love, star, wars]\n",
      "325355    [nintendo, childhood, memory, pink, color, fan...\n",
      "374332                                         [good, game]\n",
      "377366    [cheap, pricey, son, sidmt, play, worn, think,...\n",
      "225710    [great, replacement, bought, two, 2, ds, lite,...\n",
      "207365    [would, give, 0, stars, possible, waste, money...\n",
      "470515                                [i, loved, i, needed]\n",
      "169088    [love, game, say, good, thing, im, not, csi, a...\n",
      "142103    [it, promised, though, apps, taken, good, adva...\n",
      "494333    [this, rhythm, game, fitting, surprisingly, ca...\n",
      "127852                                        [great, game]\n",
      "466666                                             [thanks]\n",
      "2635      [the, n64, share, ups, downs, hit, boy, hit, t...\n",
      "424501    [pros, delivers, instantly, provides, plenty, ...\n",
      "402241                                    [works, expected]\n",
      "52404                                           [excellant]\n",
      "232508    [this, review, wii, sports, resort, nintendo, ...\n",
      "279320    [the, process, set, long, much, rather, wii, m...\n",
      "210564    [good, game, like, megaman, x, dont, expect, m...\n",
      "207881                                [awesome, thank, you]\n",
      "297743    [theres, no, cutscene, skip, one, would, think...\n",
      "181101    [my, order, arrived, today, arrived, expected,...\n",
      "Name: reviewText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# We need to download several nltk artefacts to perform the preprocessing\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='nltk')\n",
    "nltk.download('wordnet', download_dir='nltk')\n",
    "nltk.download('stopwords', download_dir='nltk')\n",
    "nltk.download('punkt', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Define a lemmatizer (converts words to base form)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define the English stopwords\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
    "\n",
    "def clean_text(doc):\n",
    "    \"\"\" A function that cleans a given document (i.e. a text string)\"\"\"\n",
    "    \n",
    "    # Turn to lower case\n",
    "    doc = doc.lower()\n",
    "    # the shortened form n't is expanded to not\n",
    "    doc = doc.replace(\"n\\'t \", ' not ')\n",
    "    # shortened forms like 'll 're 'd 've are removed as they don't add much value to this task\n",
    "    doc = re.sub(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \", doc)\n",
    "    # numbers are removed\n",
    "    doc = re.sub(r\"/d+\",\"\", doc)\n",
    "    # break the text in to tokens (or words), while doing that ignore stopwords from the result\n",
    "    # stopwords again do not add any value to the task\n",
    "    tokens = [w for w in word_tokenize(doc) if w not in EN_STOPWORDS and w not in string.punctuation]  \n",
    "\n",
    "    # return the clean text\n",
    "    return tokens\n",
    "\n",
    "# Run a sample\n",
    "sample_doc = 'She sells seashells by the seashore.'\n",
    "print(\"Before clean: {}\".format(sample_doc))\n",
    "print(\"After clean: {}\".format(clean_text(sample_doc)))\n",
    "\n",
    "# Apply the transformation to the full text\n",
    "# this is time consuming\n",
    "#print(\"\\nProcessing all the review data ... This can take a some time\")\n",
    "#inputs = inputs.apply(lambda x: clean_text(x))\n",
    "inputs = inputs.str.lower()\n",
    "\n",
    "# Remove punctuation \n",
    "inputs = inputs.str.replace('[{}]'.format(string.punctuation), '')\n",
    "inputs = inputs.str.replace(\"n\\'t \", \" not \")\n",
    "inputs = inputs.str.replace(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \")\n",
    "inputs = inputs.str.replace(r\"/d+\",\"\")\n",
    "\n",
    "stopwords_regex = \"(?:\" + \" | \".join(EN_STOPWORDS) + \")\"\n",
    "inputs = inputs.str.replace(stopwords_regex, \" \")\n",
    "inputs = inputs.str.split()\n",
    "\n",
    "print(\"Sample data\")\n",
    "print(inputs.head(n=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: this was my favorie game so far for the ds. Only had to use the internet to help get by two levels but very enjoyable game.\n",
      "Clean: ['this', 'favorie', 'game', 'far', 'ds', 'use', 'internet', 'help', 'get', 'two', 'levels', 'enjoyable', 'game']\n",
      "\n",
      "\n",
      "Actual: 9 year old girl really likes this game.  I'm a gamer and don't really understand it, but it works for her.\n",
      "Clean: ['9', 'year', 'old', 'girl', 'really', 'likes', 'game', 'im', 'gamer', 'dont', 'really', 'understand', 'works', 'her']\n",
      "\n",
      "\n",
      "Actual: Gorgeous graphics, catchy tune & FUN game play! A definite win!\n",
      "Clean: ['gorgeous', 'graphics', 'catchy', 'tune', 'fun', 'game', 'play', 'definite', 'win']\n",
      "\n",
      "\n",
      "Actual: Love Star Wars!!\n",
      "Clean: ['love', 'star', 'wars']\n",
      "\n",
      "\n",
      "Actual: Nintendo is my childhood memory. Pink color is fantastic. I bought two machines. One for myself and another one is for my girlfriend. We both like it.\n",
      "Clean: ['nintendo', 'childhood', 'memory', 'pink', 'color', 'fantastic', 'bought', 'two', 'machines', 'one', 'another', 'one', 'girlfriend', 'like', 'it']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for actual, clean in zip(verified_df[\"reviewText\"].iloc[:5], inputs.iloc[:5]):\n",
    "    print(\"Actual: {}\".format(actual))\n",
    "    print(\"Clean: {}\".format(clean))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 310388\n",
      "Validation data: 11058\n",
      "Test data: 11058\n"
     ]
    }
   ],
   "source": [
    "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
    "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"    \n",
    "    \n",
    "    # Separate indices of negative and positive data points\n",
    "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels==1)].index)\n",
    "    \n",
    "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
    "    n_test = n_valid\n",
    "    \n",
    "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
    "    \n",
    "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    pos_train_inds = pos_indices.loc[\n",
    "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
    "    ]\n",
    "    \n",
    "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    \n",
    "    print('Training data: {}'.format(len(tr_x)))\n",
    "    print('Validation data: {}'.format(len(v_x)))\n",
    "    print('Test data: {}'.format(len(ts_x)))\n",
    "    \n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "    \n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game     314666\n",
      "not      113939\n",
      "great     96881\n",
      "like      92636\n",
      "games     84281\n",
      "one       84259\n",
      "good      76938\n",
      "play      70806\n",
      "i         66669\n",
      "get       64096\n",
      "dtype: int64\n",
      "count    151130.000000\n",
      "mean         69.142288\n",
      "std        1291.196926\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           4.000000\n",
      "max      314666.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in tr_x for w in doc]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Print summary statistics\n",
    "print(freq_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 14487\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 25).sum()\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some summary statistics\n",
      "Median length: 13.0\n",
      "\n",
      "count    310388.000000\n",
      "mean         33.665844\n",
      "std          74.880459\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%          13.000000\n",
      "75%          31.000000\n",
      "max        3094.000000\n",
      "Name: reviewText, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    254893.000000\n",
      "mean         17.624866\n",
      "std          16.789406\n",
      "min           2.000000\n",
      "25%           5.000000\n",
      "50%          12.000000\n",
      "75%          24.000000\n",
      "max          76.000000\n",
      "Name: reviewText, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a pd.Series, which contain the sequence length for each review\n",
    "seq_length_ser = tr_x.str.len()\n",
    "\n",
    "# Get the median as well as summary statistics of the sequence length\n",
    "print(\"\\nSome summary statistics\")\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "print(seq_length_ser.describe())\n",
    "\n",
    "print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "p_10 = seq_length_ser.quantile(0.1)\n",
    "p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq_mid = 15\n",
    "n_seq_max = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* MLOps: Deploying your models\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer that will convert words to IDs\n",
    "# words that are less frequent will be replaced by 'unk'\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Fit the tokenizer on the data\n",
    "tokenizer.fit_on_texts(tr_x.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the attributes of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word id for \"game\" is: 2\n",
      "The word for id 4 is: great\n"
     ]
    }
   ],
   "source": [
    "# Checking the attributes of the tokenizer\n",
    "word = \"game\"\n",
    "wid = tokenizer.word_index[word]\n",
    "print(\"The word id for \\\"{}\\\" is: {}\".format(word, wid))\n",
    "wid = 4\n",
    "word = tokenizer.index_word[wid]\n",
    "print(\"The word for id {} is: {}\".format(wid, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory']\n",
      "Sequence: [58, 296, 80, 650, 334, 2340, 2723, 487]\n",
      "\n",
      "\n",
      "Text: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive']\n",
      "Sequence: [167, 2, 4277, 128, 20, 46, 3210, 151, 209, 268, 878, 1274]\n",
      "\n",
      "\n",
      "Text: [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game']\n",
      "Sequence: [1, 640, 2, 1418, 101, 347, 2, 1, 1, 276, 145, 666, 1, 293, 849, 17, 146, 2]\n",
      "\n",
      "\n",
      "Text: ['excellent', 'product', 'describe']\n",
      "Sequence: [103, 56, 2297]\n",
      "\n",
      "\n",
      "Text: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
      "Sequence: [130, 850, 4, 65, 21, 591, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert words to IDs\n",
    "test_text = [\n",
    "    ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory'],\n",
    "    ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive'],\n",
    "    [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game'],\n",
    "    ['excellent', 'product', 'describe'],\n",
    "    ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "for text, seq in zip(test_text, test_seq):\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Sequence: {}\".format(seq))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training/validation/test data to word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x_short = tr_x[(tr_x.str.len()<n_seq_mid)]\n",
    "tr_x_long = tr_x[(tr_x.str.len()>=n_seq_mid)]\n",
    "\n",
    "# Training labels separated to short and long\n",
    "tr_y_short = tr_y[(tr_x.str.len()<n_seq_mid)]\n",
    "tr_y_long = tr_y[(tr_x.str.len()>=n_seq_mid)]\n",
    "\n",
    "# Convert all of train/validation/test data to sequences of IDs\n",
    "tr_x_short_seq = tokenizer.texts_to_sequences(tr_x_short.tolist())\n",
    "tr_x_long_seq = tokenizer.texts_to_sequences(tr_x_long.tolist())\n",
    "\n",
    "v_x_seq = tokenizer.texts_to_sequences(v_x.tolist())\n",
    "ts_x_seq = tokenizer.texts_to_sequences(ts_x.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (Short): (169097, 15)\n",
      "Training (Long): (141291, 30)\n",
      "Sample training data\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0  422   15  357\n",
      "   534]\n",
      " [   0    0    0    0    0    0   23   34   20    5    2   10   17   43\n",
      "   463]\n",
      " [   0    0    0   21    2 2250  104 2076   44 3360 1998  194   46 1474\n",
      "  2046]\n",
      " [   0    0    0    0    0    3  121  500  248  634 1305  349  149 1502\n",
      "   246]\n",
      " [   0    0    0    0    0   23    2   13    4  363   62   61 1211   13\n",
      "   198]]\n",
      "\n",
      "Valid: (11058, 30)\n",
      "Test: (11058, 30)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tr_x_short_pad = pad_sequences(\n",
    "    tr_x_short_seq, maxlen=n_seq_mid, padding='pre',truncating='post'\n",
    ")\n",
    "print('Training (Short): {}'.format(tr_x_short_pad.shape))\n",
    "\n",
    "tr_x_long_pad = pad_sequences(\n",
    "    tr_x_long_seq, maxlen=n_seq_max, padding='pre', truncating='post'\n",
    ")\n",
    "print('Training (Long): {}'.format(tr_x_long_pad.shape))\n",
    "\n",
    "print('Sample training data')\n",
    "print(tr_x_short_pad[:5])\n",
    "\n",
    "v_x_pad = pad_sequences(v_x_seq, maxlen=n_seq_max, padding='pre', truncating='post')\n",
    "print('\\nValid: {}'.format(v_x_pad.shape))\n",
    "\n",
    "ts_x_pad = pad_sequences(ts_x_seq, maxlen=n_seq_max, padding='pre', truncating='post')\n",
    "print('Test: {}'.format(ts_x_pad.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1854464   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,052,609\n",
      "Trainable params: 2,052,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Create a mask to mask out zero inputs\n",
    "    #tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),    \n",
    "    # Adding an Embedding layer\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, \n",
    "                              mask_zero=True, \n",
    "                              input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining Dense layers\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Defining a dropout layer\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be using a weight of 6.017113919471887 for negative samples\n"
     ]
    }
   ],
   "source": [
    "neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
    "print(\"Will be using a weight of {} for negative samples\".format(neg_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4111 - accuracy: 0.8119\n",
      "Epoch 2\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4304 - accuracy: 0.8134\n",
      "Epoch 3\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4357 - accuracy: 0.8073\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "n_short, n_long = tr_x_short_pad.shape[0], tr_x_long_pad.shape[0]\n",
    "\n",
    "short_ratio = n_short/(n_short + n_long)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    print(\"Epoch {}\".format(e+1))\n",
    "    n_iter = (n_short + n_long)//batch_size\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        if np.random.uniform(0,1) <= short_ratio:\n",
    "            # pick a short batch\n",
    "            batch_ids = np.random.randint(0, n_short, size=[batch_size])\n",
    "            x = tr_x_short_pad[batch_ids, :]\n",
    "            y = tr_y_short.iloc[batch_ids]\n",
    "        else:\n",
    "            # pick a long batch\n",
    "            batch_ids = np.random.randint(0, n_long, size=[batch_size])\n",
    "            x = tr_x_long_pad[batch_ids, :]\n",
    "            y = tr_y_long.iloc[batch_ids]\n",
    "            \n",
    "        model.train_on_batch(x, y, class_weight={0:neg_weight, 1:1.0})\n",
    "    \n",
    "    print(\"\\tEvaluating...\")\n",
    "    res = model.evaluate(v_x_pad, v_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* MLOps: Deploying your models\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
