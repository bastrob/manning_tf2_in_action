{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with TensorFlow\n",
    "\n",
    "\n",
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* Creating complex data pipelines\n",
    "* MLOps: Monitoring and deploying your models\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "Use the discount code **twitgane40** to get 40% off TensorFlow in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "import requests\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import nltk\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        print(\"Couldn't set memory_growth\")\n",
    "        pass\n",
    "    \n",
    "    \n",
    "def fix_random_seed(seed):\n",
    "    \"\"\" Setting the random seed of various libraries \"\"\"\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "random_seed=4321\n",
    "fix_random_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tar file already exists.\n",
      "The extracted data already exists\n"
     ]
    }
   ],
   "source": [
    "# Downloading the data\n",
    "# http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# Retrieve the data\n",
    "if not os.path.exists(os.path.join('data','Video_Games_5.json.gz')):\n",
    "    url = \"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\"\n",
    "    # Get the file from web\n",
    "    r = requests.get(url)\n",
    "\n",
    "    if not os.path.exists('data'):\n",
    "        os.mkdir('data')\n",
    "    \n",
    "    # Write to a file\n",
    "    with open(os.path.join('data','Video_Games_5.json.gz'), 'wb') as f:\n",
    "        f.write(r.content)\n",
    "else:\n",
    "    print(\"The tar file already exists.\")\n",
    "    \n",
    "if not os.path.exists(os.path.join('data', 'Video_Games_5.json')):\n",
    "    with gzip.open(os.path.join('data','Video_Games_5.json.gz'), 'rb') as f_in:\n",
    "        with open(os.path.join('data','Video_Games_5.json'), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "else:\n",
    "    print(\"The extracted data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>10 17, 2015</td>\n",
       "      <td>This game is a bit hard to get the hang of, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>07 27, 2015</td>\n",
       "      <td>I played it a while but it was alright. The st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2015</td>\n",
       "      <td>ok game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>02 20, 2015</td>\n",
       "      <td>found the game a bit too complicated, not what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>12 25, 2014</td>\n",
       "      <td>great game, I love it and have played it since...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime  \\\n",
       "0        5      True  10 17, 2015   \n",
       "1        4     False  07 27, 2015   \n",
       "2        3      True  02 23, 2015   \n",
       "3        2      True  02 20, 2015   \n",
       "4        5      True  12 25, 2014   \n",
       "\n",
       "                                          reviewText  \n",
       "0  This game is a bit hard to get the hang of, bu...  \n",
       "1  I played it a while but it was alright. The st...  \n",
       "2                                           ok game.  \n",
       "3  found the game a bit too complicated, not what...  \n",
       "4  great game, I love it and have played it since...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the JSON file\n",
    "review_df = pd.read_json(os.path.join('data', 'Video_Games_5.json'), lines=True, orient='records')\n",
    "# Select on the columns we're interested in \n",
    "review_df = review_df[[\"overall\", \"verified\", \"reviewTime\", \"reviewText\"]]\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning up: (497577, 4)\n",
      "After cleaning up: (497419, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before cleaning up: {}\".format(review_df.shape))\n",
    "review_df = review_df[~review_df[\"reviewText\"].isna()]\n",
    "review_df = review_df[review_df[\"reviewText\"].str.strip().str.len()>0]\n",
    "print(\"After cleaning up: {}\".format(review_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verified reviews vs unverified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     332504\n",
       "False    164915\n",
       "Name: verified, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df[\"verified\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    222335\n",
       "4     54878\n",
       "3     27973\n",
       "1     15200\n",
       "2     12118\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified_df = review_df.loc[review_df[\"verified\"], :]\n",
    "verified_df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping stars to positive and negative labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\manning.tf2\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    277213\n",
       "0     55291\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas map function to map different star ratings to 0/1\n",
    "verified_df[\"label\"]=verified_df[\"overall\"].map({5:1, 4:1, 3:0, 2:0, 1:0})\n",
    "verified_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are sampling 100% of the data in a random fashion, leading to a shuffled dataset\n",
    "verified_df = verified_df.sample(frac=1.0, random_state=random_seed)\n",
    "\n",
    "# Splint the data to inputs (inputs) and targets (labels)\n",
    "inputs, labels = verified_df[\"reviewText\"], verified_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to nltk...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to nltk...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing all the review data ...\n",
      "Sample data\n",
      "122143    [worked, perfectly, wii, gamecube, no, issues,...\n",
      "444818    [loved, game, the, collectibles, came, it, wel...\n",
      "79331     [its, okay, game, be, honest, am, bad, these, ...\n",
      "97250                       [excellent, product, described]\n",
      "324411    [the, level, detail, great, can, feel, love, c...\n",
      "427782           [buy, game, buy, vita, buy, game, amazing]\n",
      "200896    [excellent, product, very, useful, 100, recomm...\n",
      "394648    [these, great, amiibos, add, your, collection,...\n",
      "480002    [first, zenses, game, bought, got, other, two,...\n",
      "194630                                         [nice, game]\n",
      "122215    [awesome, memory, card, worked, great, both, w...\n",
      "278172    [ive, started, playing, believe, is, great, ga...\n",
      "233269    [this, game, cool, alot, like, remembered, som...\n",
      "299538    [this, might, seem, interesting, you, bought, ...\n",
      "229458    [short, had, movement, issues, not, bad, could...\n",
      "34481     [i, dont, actually, believe, writing, review, ...\n",
      "258474    [this, a, good, game, those, us, like, movie, ...\n",
      "466203    [fun, first, person, shooter, nice, combinatio...\n",
      "414288                      [love, amiibo, classic, colors]\n",
      "162670    [ive, a, fan, the, halo, series, the, start, e...\n",
      "229094    [its, great, wii, game, specially, kids, saw, ...\n",
      "314590    [wow, kids, love, game, 100, upgrade, last, ma...\n",
      "238940    [i, ignored, experience, the, longest, time, e...\n",
      "238834    [story, weird, twisted, not, necessarily, a, b...\n",
      "410349    [i, really, like, fact, most, games, know, im,...\n",
      "Name: reviewText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# We need to download several nltk artefacts to perform the preprocessing\n",
    "nltk.download('stopwords', download_dir='nltk')\n",
    "nltk.download('punkt', download_dir='nltk')\n",
    "nltk.data.path.append(os.path.abspath('nltk'))\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Define the English stopwords\n",
    "EN_STOPWORDS = set(stopwords.words('english')) - {'not', 'no'}\n",
    "\n",
    "# Apply the transformation to the full text\n",
    "# this is time consuming\n",
    "print(\"\\nProcessing all the review data ...\")\n",
    "inputs = inputs.str.lower()\n",
    "\n",
    "# Remove punctuation \n",
    "inputs = inputs.str.replace('[{}]'.format(string.punctuation), '')\n",
    "inputs = inputs.str.replace(\"n\\'t \", \" not \")\n",
    "inputs = inputs.str.replace(r\"(?:\\'ll |\\'re |\\'d |\\'ve )\", \" \")\n",
    "inputs = inputs.str.replace(r\"/d+\",\"\")\n",
    "\n",
    "stopwords_regex = \"(?:\" + \" | \".join(EN_STOPWORDS) + \")\"\n",
    "inputs = inputs.str.replace(stopwords_regex, \" \")\n",
    "inputs = inputs.str.split()\n",
    "\n",
    "print(\"Sample data\")\n",
    "print(inputs.head(n=25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: Worked perfectly on Wii and Gamecube.\n",
      "No issues with compatibility or loss of memory.\n",
      "Clean: ['worked', 'perfectly', 'wii', 'gamecube', 'no', 'issues', 'compatibility', 'loss', 'memory']\n",
      "\n",
      "\n",
      "Actual: Loved the game and the other collectibles that came with it are well made.  The mask is big and it almost fits my face so that was impressive.\n",
      "Clean: ['loved', 'game', 'the', 'collectibles', 'came', 'it', 'well', 'made', 'mask', 'big', 'it', 'almost', 'fits', 'face', 'that', 'impressive']\n",
      "\n",
      "\n",
      "Actual: It's an okay game, to be honest, I am very bad at these types of games and to me-- it's very difficult! I am always dying, which depresses me. Maybe if I had more skill I would enjoy this game more!\n",
      "Clean: ['its', 'okay', 'game', 'be', 'honest', 'am', 'bad', 'these', 'types', 'games', 'to', 'its', 'difficult', 'am', 'always', 'dying', 'depresses', 'maybe', 'i', 'more', 'skill', 'would', 'enjoy', 'game', 'more']\n",
      "\n",
      "\n",
      "Actual: Excellent product as described\n",
      "Clean: ['excellent', 'product', 'described']\n",
      "\n",
      "\n",
      "Actual: the level of detail is great you can feel the love for cars in this game.\n",
      "Clean: ['the', 'level', 'detail', 'great', 'can', 'feel', 'love', 'cars', 'this', 'game']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for actual, clean in zip(verified_df[\"reviewText\"].iloc[:5], inputs.iloc[:5]):\n",
    "    print(\"Actual: {}\".format(actual))\n",
    "    print(\"Clean: {}\".format(clean))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting training/validation/testing data\n",
    "\n",
    "- Refer presentation for the detailed explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 310388\n",
      "Validation data: 11058\n",
      "Test data: 11058\n"
     ]
    }
   ],
   "source": [
    "def train_valid_test_split(inputs, labels, train_fraction=0.8):\n",
    "    \"\"\" Splits a given dataset into three sets; training, validation and test \"\"\"    \n",
    "    \n",
    "    # Separate indices of negative and positive data points\n",
    "    neg_indices = pd.Series(labels.loc[(labels==0)].index)\n",
    "    pos_indices = pd.Series(labels.loc[(labels==1)].index)\n",
    "    \n",
    "    n_valid = int(min([len(neg_indices), len(pos_indices)]) * ((1-train_fraction)/2.0))\n",
    "    n_test = n_valid\n",
    "    \n",
    "    neg_test_inds = neg_indices.sample(n=n_test, random_state=random_seed)\n",
    "    neg_valid_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    neg_train_inds = neg_indices.loc[~neg_indices.isin(neg_test_inds.tolist()+neg_valid_inds.tolist())]\n",
    "    \n",
    "    pos_test_inds = pos_indices.sample(n=n_test, random_state=random_seed)\n",
    "    pos_valid_inds = pos_indices.loc[~pos_indices.isin(pos_test_inds)].sample(n=n_test, random_state=random_seed)\n",
    "    pos_train_inds = pos_indices.loc[\n",
    "        ~pos_indices.isin(pos_test_inds.tolist()+pos_valid_inds.tolist())\n",
    "    ]\n",
    "    \n",
    "    tr_x = inputs.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    tr_y = labels.loc[neg_train_inds.tolist() + pos_train_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_x = inputs.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    v_y = labels.loc[neg_valid_inds.tolist() + pos_valid_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_x = inputs.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    ts_y = labels.loc[neg_test_inds.tolist() + pos_test_inds.tolist()].sample(frac=1.0, random_state=random_seed)\n",
    "    \n",
    "    print('Training data: {}'.format(len(tr_x)))\n",
    "    print('Validation data: {}'.format(len(v_x)))\n",
    "    print('Test data: {}'.format(len(ts_x)))\n",
    "    \n",
    "    return (tr_x, tr_y), (v_x, v_y), (ts_x, ts_y)\n",
    "    \n",
    "(tr_x, tr_y), (v_x, v_y), (ts_x, ts_y) = train_valid_test_split(inputs, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the      497674\n",
      "game     315080\n",
      "a        221368\n",
      "i        181830\n",
      "it       133993\n",
      "you      114603\n",
      "this     114250\n",
      "not      114052\n",
      "great     96708\n",
      "like      92532\n",
      "dtype: int64\n",
      "count    151379.000000\n",
      "mean         84.342458\n",
      "std        2093.623140\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           4.000000\n",
      "max      497674.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Create a large list which contains all the words in all the reviews\n",
    "data_list = [w for doc in tr_x for w in doc]\n",
    "\n",
    "# Create a Counter object from that list\n",
    "# Counter returns a dictionary, where key is a word and the value is the frequency\n",
    "cnt = Counter(data_list)\n",
    "\n",
    "# Convert the result to a pd.Series \n",
    "freq_df = pd.Series(list(cnt.values()), index=list(cnt.keys())).sort_values(ascending=False)\n",
    "# Print most common words\n",
    "print(freq_df.head(n=10))\n",
    "\n",
    "# Print summary statistics\n",
    "print(freq_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a vocabulary of size: 14513\n"
     ]
    }
   ],
   "source": [
    "n_vocab = (freq_df >= 25).sum()\n",
    "print(\"Using a vocabulary of size: {}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some summary statistics\n",
      "Median length: 15.0\n",
      "\n",
      "count    310388.000000\n",
      "mean         41.134570\n",
      "std          92.247816\n",
      "min           0.000000\n",
      "25%           4.000000\n",
      "50%          15.000000\n",
      "75%          38.000000\n",
      "max        3723.000000\n",
      "Name: reviewText, dtype: float64\n",
      "\n",
      "Computing the statistics between the 10% and 90% quantiles (to ignore outliers)\n",
      "count    254829.000000\n",
      "mean         21.354069\n",
      "std          20.845726\n",
      "min           2.000000\n",
      "25%           5.000000\n",
      "50%          15.000000\n",
      "75%          29.000000\n",
      "max          94.000000\n",
      "Name: reviewText, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a pd.Series, which contain the sequence length for each review\n",
    "seq_length_ser = tr_x.str.len()\n",
    "\n",
    "# Get the median as well as summary statistics of the sequence length\n",
    "print(\"\\nSome summary statistics\")\n",
    "print(\"Median length: {}\\n\".format(seq_length_ser.median()))\n",
    "print(seq_length_ser.describe())\n",
    "\n",
    "print(\"\\nComputing the statistics between the 10% and 90% quantiles (to ignore outliers)\")\n",
    "p_10 = seq_length_ser.quantile(0.1)\n",
    "p_90 = seq_length_ser.quantile(0.9)\n",
    "\n",
    "print(seq_length_ser[(seq_length_ser >= p_10) & (seq_length_ser < p_90)].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seq_mid = 15\n",
    "n_seq_max = 30"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* Creating complex data pipelines\n",
    "* MLOps: Monitoring and deploying your models\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "Use the discount code **twitgane40** to get 40% off TensorFlow in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a tokenizer that will convert words to IDs\n",
    "# words that are less frequent will be replaced by 'unk'\n",
    "tokenizer = Tokenizer(num_words=n_vocab, oov_token='unk', lower=False)\n",
    "\n",
    "# Fit the tokenizer on the data\n",
    "tokenizer.fit_on_texts(tr_x.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the attributes of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word id for \"game\" is: 3\n",
      "The word for id 4 is: a\n"
     ]
    }
   ],
   "source": [
    "# Checking the attributes of the tokenizer\n",
    "word = \"game\"\n",
    "wid = tokenizer.word_index[word]\n",
    "print(\"The word id for \\\"{}\\\" is: {}\".format(word, wid))\n",
    "wid = 4\n",
    "word = tokenizer.index_word[wid]\n",
    "print(\"The word for id {} is: {}\".format(wid, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's convert some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory']\n",
      "Sequence: [87, 359, 115, 692, 389, 2355, 2830, 537]\n",
      "\n",
      "\n",
      "Text: ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive']\n",
      "Sequence: [213, 3, 4419, 219, 30, 72, 3237, 189, 255, 324, 922, 1321]\n",
      "\n",
      "\n",
      "Text: [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game']\n",
      "Sequence: [1, 690, 3, 1463, 137, 400, 3, 1, 1, 335, 184, 716, 1, 354, 898, 26, 185, 3]\n",
      "\n",
      "\n",
      "Text: ['excellent', 'product', 'describe']\n",
      "Sequence: [140, 84, 2330]\n",
      "\n",
      "\n",
      "Text: ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
      "Sequence: [167, 897, 10, 97, 31, 644, 3]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert words to IDs\n",
    "test_text = [\n",
    "    ['work', 'perfectly', 'wii', 'gamecube', 'issue', 'compatibility', 'loss', 'memory'],\n",
    "    ['loved', 'game', 'collectible', 'come', 'well', 'make', 'mask', 'big', 'almost', 'fit', 'face', 'impressive'],\n",
    "    [\"'s\", 'okay', 'game', 'honest', 'bad', 'type', 'game', '--', \"'s\", 'difficult', 'always', 'die', 'depresses', 'maybe', 'skill', 'would', 'enjoy', 'game'],\n",
    "    ['excellent', 'product', 'describe'],\n",
    "    ['level', 'detail', 'great', 'feel', 'love', 'car', 'game']\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "for text, seq in zip(test_text, test_seq):\n",
    "    print(\"Text: {}\".format(text))\n",
    "    print(\"Sequence: {}\".format(seq))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert training/validation/test data to word IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x_short = tr_x[(tr_x.str.len()<n_seq_mid)]\n",
    "tr_x_long = tr_x[(tr_x.str.len()>=n_seq_mid)]\n",
    "\n",
    "# Training labels separated to short and long\n",
    "tr_y_short = tr_y[(tr_x.str.len()<n_seq_mid)]\n",
    "tr_y_long = tr_y[(tr_x.str.len()>=n_seq_mid)]\n",
    "\n",
    "# Convert all of train/validation/test data to sequences of IDs\n",
    "tr_x_short_seq = tokenizer.texts_to_sequences(tr_x_short.tolist())\n",
    "tr_x_long_seq = tokenizer.texts_to_sequences(tr_x_long.tolist())\n",
    "\n",
    "v_x_seq = tokenizer.texts_to_sequences(v_x.tolist())\n",
    "ts_x_seq = tokenizer.texts_to_sequences(ts_x.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to a fixed length\n",
    "\n",
    "- Refer to the presentation for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (Short): (148470, 15)\n",
      "Training (Long): (161918, 30)\n",
      "Sample training data\n",
      "[[  0   0   0   0   0   0   0   0   0   0  63  64 416  25 800]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  15]\n",
      " [  0   0   0   0   0  92  14 225 155  54 104  14 132   1 175]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  10]]\n",
      "\n",
      "Valid: (11058, 30)\n",
      "Test: (11058, 30)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tr_x_short_pad = pad_sequences(\n",
    "    tr_x_short_seq, maxlen=n_seq_mid, padding='pre',truncating='post'\n",
    ")\n",
    "print('Training (Short): {}'.format(tr_x_short_pad.shape))\n",
    "\n",
    "tr_x_long_pad = pad_sequences(\n",
    "    tr_x_long_seq, maxlen=n_seq_max, padding='pre', truncating='post'\n",
    ")\n",
    "print('Training (Long): {}'.format(tr_x_long_pad.shape))\n",
    "\n",
    "print('Sample training data')\n",
    "print(tr_x_short_pad[:5])\n",
    "\n",
    "v_x_pad = pad_sequences(v_x_seq, maxlen=n_seq_max, padding='pre', truncating='post')\n",
    "print('\\nValid: {}'.format(v_x_pad.shape))\n",
    "\n",
    "ts_x_pad = pad_sequences(ts_x_seq, maxlen=n_seq_max, padding='pre', truncating='post')\n",
    "print('Test: {}'.format(ts_x_pad.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "- Refer the presentation for detailed visualizations of the model and its components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1857792   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,055,937\n",
      "Trainable params: 2,055,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # Create a mask to mask out zero inputs\n",
    "    #tf.keras.layers.Masking(mask_value=0.0, input_shape=(None,)),    \n",
    "    # Adding an Embedding layer\n",
    "    tf.keras.layers.Embedding(input_dim=n_vocab+1, output_dim=128, \n",
    "                              mask_zero=True, \n",
    "                              input_shape=(None,)),\n",
    "    # Defining an LSTM layer\n",
    "    tf.keras.layers.LSTM(128, return_state=False, return_sequences=False),\n",
    "    # Defining Dense layers\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Defining a dropout layer\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will be using a weight of 6.017113919471887 for negative samples\n"
     ]
    }
   ],
   "source": [
    "neg_weight = (tr_y==1).sum()/(tr_y==0).sum()\n",
    "print(\"Will be using a weight of {} for negative samples\".format(neg_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4111 - accuracy: 0.8119\n",
      "Epoch 2\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4304 - accuracy: 0.8134\n",
      "Epoch 3\n",
      "\tEvaluating...\n",
      "346/346 [==============================] - 5s 15ms/step - loss: 0.4357 - accuracy: 0.8073\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "n_short, n_long = tr_x_short_pad.shape[0], tr_x_long_pad.shape[0]\n",
    "\n",
    "short_ratio = n_short/(n_short + n_long)\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    print(\"Epoch {}\".format(e+1))\n",
    "    n_iter = (n_short + n_long)//batch_size\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        if np.random.uniform(0,1) <= short_ratio:\n",
    "            # pick a short batch\n",
    "            batch_ids = np.random.randint(0, n_short, size=[batch_size])\n",
    "            x = tr_x_short_pad[batch_ids, :]\n",
    "            y = tr_y_short.iloc[batch_ids]\n",
    "        else:\n",
    "            # pick a long batch\n",
    "            batch_ids = np.random.randint(0, n_long, size=[batch_size])\n",
    "            x = tr_x_long_pad[batch_ids, :]\n",
    "            y = tr_y_long.iloc[batch_ids]\n",
    "            \n",
    "        model.train_on_batch(x, y, class_weight={0:neg_weight, 1:1.0})\n",
    "    \n",
    "    print(\"\\tEvaluating...\")\n",
    "    res = model.evaluate(v_x_pad, v_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [TensorFlow 2 in Action ](https://www.manning.com/books/tensorflow-in-action)\n",
    "\n",
    "* Link: https://www.manning.com/books/tensorflow-in-action\n",
    "\n",
    "### Topics covered:\n",
    "\n",
    "* All popular deep learning models like CNNs, GRUs, LSTMs and Transformers\n",
    "* Computer vision tasks like image classification and segmentation\n",
    "* NLP tasks like sentiment analysis, language modelling, machine translation\n",
    "* Creating complex data pipelines\n",
    "* MLOps: Monitoring and deploying your models\n",
    "\n",
    "\n",
    "\n",
    "<table align=\"center\">\n",
    "    <td>\n",
    "        <img src=\"book.png\" />\n",
    "    </td>\n",
    "</table>\n",
    "\n",
    "Use the discount code **twitgane40** to get 40% off TensorFlow in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
