{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Keras and Data Retrieval in TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>\n",
    "        <a target=\"_blank\" href=\"https://colab.research.google.com/github/thushv89/manning_tf2_in_action/blob/master/Ch03/3.2.Creating_Input_Pipelines.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Numpy is not imported. Setting the seed for Numpy failed.\n",
      "Warning: random module is not imported. Setting the seed for random failed.\n",
      "TensorFlow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def fix_random_seed(seed):\n",
    "    try:\n",
    "        np.random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: Numpy is not imported. Setting the seed for Numpy failed.\")\n",
    "    try:\n",
    "        tf.random.set_seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: TensorFlow is not imported. Setting the seed for TensorFlow failed.\")\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "    except NameError:\n",
    "        print(\"Warning: random module is not imported. Setting the seed for random failed.\")\n",
    "\n",
    "# Fixing the random seed\n",
    "fix_random_seed(4321)\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `tf.data` API to retrieve data\n",
    "\n",
    "Here we will be using the `tf.data` API to feed a dataset containing images of flowers. The dataset has a folder containing the images and a CSV file listing filenames and their corresponding label as an integer. We will write a TensorFlow data pipeline that does the following.\n",
    "\n",
    "* Extract filenames and classes from the CSV\n",
    "* Read in the images from the extracted filenames and resize them to 64x64\n",
    "* Convert the class labels to one-hot encoded vectors\n",
    "* Combine the processed images and one-hot encoded vectors to a single dataset\n",
    "* Finally, shuffle the data and output as batches\n",
    "\n",
    "### Downloading the data\n",
    "The dataset is available at https://www.kaggle.com/olgabelitskaya/flower-color-images/data . \n",
    "\n",
    "You need to download the zip file available in this URL and place it in the `data` folder in the `Ch02` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Extracting the flowers image data to a directory\n",
    "if os.path.exists('data/flower-color-images.zip'):\n",
    "    zfile = ZipFile('data/flower-color-images.zip')\n",
    "    zfile.extractall('data')\n",
    "else:\n",
    "    print(\"Did you download the dataset as a zip file and place it in the Ch02/data folder?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tf.data.Dataset \n",
    "\n",
    "Here we are creating the `tf.data` pipeline that executes the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image dataset contains: <MapDataset shapes: (64, 64, 3), types: tf.float32>\n",
      "(<tf.Tensor: shape=(5, 64, 64, 3), dtype=float32, numpy=\n",
      "array([[[[0.5852941 , 0.5088236 , 0.39411768],\n",
      "         [0.5852941 , 0.50980395, 0.4009804 ],\n",
      "         [0.5862745 , 0.51176476, 0.40490198],\n",
      "         ...,\n",
      "         [0.82156867, 0.7294118 , 0.62352943],\n",
      "         [0.82745105, 0.74509805, 0.6392157 ],\n",
      "         [0.8284314 , 0.75098044, 0.64509803]],\n",
      "\n",
      "        [[0.59607846, 0.51862746, 0.40980396],\n",
      "         [0.59411764, 0.5235294 , 0.40882355],\n",
      "         [0.59607846, 0.52254903, 0.41470593],\n",
      "         ...,\n",
      "         [0.82941186, 0.73921573, 0.63725495],\n",
      "         [0.8313726 , 0.7411765 , 0.64215684],\n",
      "         [0.82745105, 0.7401961 , 0.63823533]],\n",
      "\n",
      "        [[0.60882354, 0.5294118 , 0.41960788],\n",
      "         [0.6127451 , 0.532353  , 0.42352945],\n",
      "         [0.6117647 , 0.53431374, 0.42549023],\n",
      "         ...,\n",
      "         [0.8343138 , 0.7441176 , 0.63823533],\n",
      "         [0.82450986, 0.7303922 , 0.6303922 ],\n",
      "         [0.81274515, 0.71470594, 0.6147059 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.9196079 , 0.9117648 , 0.8764707 ],\n",
      "         [0.9539217 , 0.9441177 , 0.92647064],\n",
      "         [0.93725497, 0.89901966, 0.827451  ],\n",
      "         ...,\n",
      "         [0.70490193, 0.69509804, 0.6058824 ],\n",
      "         [0.65588236, 0.6666667 , 0.5568628 ],\n",
      "         [0.6156863 , 0.6411765 , 0.5107843 ]],\n",
      "\n",
      "        [[0.88725495, 0.8431373 , 0.804902  ],\n",
      "         [0.9343138 , 0.9294118 , 0.9000001 ],\n",
      "         [0.96372557, 0.9607844 , 0.94019616],\n",
      "         ...,\n",
      "         [0.6215687 , 0.6303922 , 0.5107843 ],\n",
      "         [0.5568628 , 0.6019608 , 0.4529412 ],\n",
      "         [0.48823532, 0.5647059 , 0.3803922 ]],\n",
      "\n",
      "        [[0.97450984, 0.96470594, 0.9450981 ],\n",
      "         [0.8568628 , 0.80686283, 0.6843138 ],\n",
      "         [0.84411776, 0.7313726 , 0.4166667 ],\n",
      "         ...,\n",
      "         [0.49411768, 0.5382353 , 0.38235295],\n",
      "         [0.45980394, 0.5196079 , 0.35196078],\n",
      "         [0.38431376, 0.4901961 , 0.2784314 ]]],\n",
      "\n",
      "\n",
      "       [[[0.05490196, 0.0872549 , 0.0372549 ],\n",
      "         [0.06764706, 0.09705883, 0.04411765],\n",
      "         [0.06862745, 0.09901962, 0.04509804],\n",
      "         ...,\n",
      "         [0.09019609, 0.11862746, 0.0627451 ],\n",
      "         [0.42941177, 0.5088235 , 0.26372552],\n",
      "         [0.53333336, 0.6039216 , 0.34607846]],\n",
      "\n",
      "        [[0.07352942, 0.10980393, 0.04705883],\n",
      "         [0.0754902 , 0.11176471, 0.04901961],\n",
      "         [0.07647059, 0.10882354, 0.05294118],\n",
      "         ...,\n",
      "         [0.0254902 , 0.02352941, 0.01764706],\n",
      "         [0.05784314, 0.08333334, 0.0372549 ],\n",
      "         [0.35294122, 0.44411767, 0.22352943]],\n",
      "\n",
      "        [[0.18529412, 0.2676471 , 0.07941177],\n",
      "         [0.12843138, 0.20000002, 0.06176471],\n",
      "         [0.0872549 , 0.13039216, 0.05392157],\n",
      "         ...,\n",
      "         [0.02058824, 0.02254902, 0.01666667],\n",
      "         [0.02450981, 0.02941177, 0.02254902],\n",
      "         [0.03627452, 0.04607844, 0.04215687]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.36764705, 0.3009804 , 0.22549021],\n",
      "         [0.34117648, 0.28235295, 0.20490198],\n",
      "         [0.32058823, 0.2627451 , 0.1901961 ],\n",
      "         ...,\n",
      "         [0.18725492, 0.14901961, 0.1137255 ],\n",
      "         [0.18235296, 0.1637255 , 0.11078432],\n",
      "         [0.25882354, 0.22941178, 0.15294118]],\n",
      "\n",
      "        [[0.37647063, 0.3019608 , 0.22352943],\n",
      "         [0.36470592, 0.29705882, 0.21568629],\n",
      "         [0.31176472, 0.25686276, 0.18529414],\n",
      "         ...,\n",
      "         [0.24901962, 0.20098041, 0.15196079],\n",
      "         [0.2676471 , 0.21764708, 0.15392157],\n",
      "         [0.26666668, 0.21176472, 0.16568628]],\n",
      "\n",
      "        [[0.36176473, 0.29607844, 0.22156864],\n",
      "         [0.37549022, 0.30588236, 0.23039217],\n",
      "         [0.33431375, 0.2735294 , 0.19803923],\n",
      "         ...,\n",
      "         [0.3362745 , 0.25686276, 0.21274512],\n",
      "         [0.26568627, 0.18823531, 0.16176471],\n",
      "         [0.2627451 , 0.18627453, 0.16960786]]],\n",
      "\n",
      "\n",
      "       [[[0.05490196, 0.06960785, 0.0509804 ],\n",
      "         [0.05      , 0.06176471, 0.04803922],\n",
      "         [0.05882353, 0.07058824, 0.0509804 ],\n",
      "         ...,\n",
      "         [0.67352945, 0.67941177, 0.67352945],\n",
      "         [0.6245098 , 0.6343137 , 0.62647057],\n",
      "         [0.5588235 , 0.577451  , 0.5803922 ]],\n",
      "\n",
      "        [[0.0627451 , 0.07941177, 0.05588236],\n",
      "         [0.04509804, 0.05686275, 0.04313726],\n",
      "         [0.05490196, 0.06666667, 0.04607844],\n",
      "         ...,\n",
      "         [0.7019608 , 0.70490193, 0.70392156],\n",
      "         [0.6911765 , 0.70000005, 0.6882353 ],\n",
      "         [0.73529416, 0.7431373 , 0.74705887]],\n",
      "\n",
      "        [[0.05882353, 0.07352941, 0.05392157],\n",
      "         [0.03921569, 0.05      , 0.03921569],\n",
      "         [0.05686275, 0.06862745, 0.0509804 ],\n",
      "         ...,\n",
      "         [0.7107844 , 0.71470594, 0.71274513],\n",
      "         [0.8078432 , 0.8078432 , 0.8196079 ],\n",
      "         [0.84215695, 0.8343138 , 0.8352942 ]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.32352942, 0.34705883, 0.3137255 ],\n",
      "         [0.36764708, 0.38725492, 0.32254905],\n",
      "         [0.3147059 , 0.34117648, 0.3127451 ],\n",
      "         ...,\n",
      "         [0.15196079, 0.22941178, 0.08431373],\n",
      "         [0.20294118, 0.29607844, 0.10980393],\n",
      "         [0.30686277, 0.42058825, 0.127451  ]],\n",
      "\n",
      "        [[0.29901963, 0.33039218, 0.28235295],\n",
      "         [0.34117648, 0.35882354, 0.30882353],\n",
      "         [0.29803923, 0.327451  , 0.2735294 ],\n",
      "         ...,\n",
      "         [0.16568628, 0.23529413, 0.07745098],\n",
      "         [0.18529414, 0.28039217, 0.10588236],\n",
      "         [0.24117649, 0.3480392 , 0.12843138]],\n",
      "\n",
      "        [[0.29607844, 0.32352942, 0.2735294 ],\n",
      "         [0.31764707, 0.3392157 , 0.28921568],\n",
      "         [0.3       , 0.3254902 , 0.25588238],\n",
      "         ...,\n",
      "         [0.20490196, 0.3009804 , 0.07647059],\n",
      "         [0.17058824, 0.24901962, 0.0754902 ],\n",
      "         [0.29411766, 0.3686275 , 0.14803922]]],\n",
      "\n",
      "\n",
      "       [[[0.28921568, 0.26568627, 0.23431374],\n",
      "         [0.28333336, 0.31764707, 0.21960786],\n",
      "         [0.25980395, 0.31078434, 0.19509806],\n",
      "         ...,\n",
      "         [0.22843139, 0.2137255 , 0.18725492],\n",
      "         [0.28431374, 0.25980395, 0.22450982],\n",
      "         [0.3147059 , 0.27450982, 0.21960786]],\n",
      "\n",
      "        [[0.34411764, 0.31862748, 0.28333336],\n",
      "         [0.33529413, 0.34509805, 0.26372552],\n",
      "         [0.26862746, 0.30686277, 0.19607845],\n",
      "         ...,\n",
      "         [0.28725493, 0.27156866, 0.2137255 ],\n",
      "         [0.31666666, 0.29607844, 0.21764708],\n",
      "         [0.23235296, 0.2137255 , 0.15784314]],\n",
      "\n",
      "        [[0.32254905, 0.33333334, 0.2617647 ],\n",
      "         [0.29901963, 0.33333334, 0.23823531],\n",
      "         [0.23725492, 0.29803923, 0.17843139],\n",
      "         ...,\n",
      "         [0.28529412, 0.28333336, 0.18431374],\n",
      "         [0.18039218, 0.18333334, 0.13039216],\n",
      "         [0.13431373, 0.1382353 , 0.11568628]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.10980393, 0.19313727, 0.07058824],\n",
      "         [0.11176471, 0.2029412 , 0.07647059],\n",
      "         [0.1137255 , 0.20980394, 0.08039216],\n",
      "         ...,\n",
      "         [0.14705883, 0.14411765, 0.1264706 ],\n",
      "         [0.1254902 , 0.1254902 , 0.11274511],\n",
      "         [0.1382353 , 0.1392157 , 0.1264706 ]],\n",
      "\n",
      "        [[0.20098041, 0.21666668, 0.15      ],\n",
      "         [0.17156863, 0.20784315, 0.1264706 ],\n",
      "         [0.13431373, 0.20490198, 0.09901962],\n",
      "         ...,\n",
      "         [0.15196079, 0.14901961, 0.12941177],\n",
      "         [0.13725491, 0.13529412, 0.12156864],\n",
      "         [0.14705883, 0.15      , 0.13431373]],\n",
      "\n",
      "        [[0.23627453, 0.21176472, 0.18235296],\n",
      "         [0.19215688, 0.17843139, 0.15882353],\n",
      "         [0.15196079, 0.15294118, 0.1264706 ],\n",
      "         ...,\n",
      "         [0.14215687, 0.1392157 , 0.12156864],\n",
      "         [0.1382353 , 0.13529412, 0.12058824],\n",
      "         [0.13333334, 0.13529412, 0.11960785]]],\n",
      "\n",
      "\n",
      "       [[[0.24509805, 0.31176472, 0.19117649],\n",
      "         [0.29803923, 0.34901962, 0.23137257],\n",
      "         [0.38235295, 0.36764708, 0.17843139],\n",
      "         ...,\n",
      "         [0.2901961 , 0.4009804 , 0.17745098],\n",
      "         [0.27058825, 0.377451  , 0.17941177],\n",
      "         [0.26960784, 0.3911765 , 0.18921569]],\n",
      "\n",
      "        [[0.23333335, 0.2882353 , 0.17941178],\n",
      "         [0.29607844, 0.3480392 , 0.23627453],\n",
      "         [0.21078432, 0.25882354, 0.13627452],\n",
      "         ...,\n",
      "         [0.36764708, 0.47745103, 0.23529413],\n",
      "         [0.30784315, 0.3911765 , 0.19411767],\n",
      "         [0.23627453, 0.3372549 , 0.15      ]],\n",
      "\n",
      "        [[0.23725492, 0.31176472, 0.18529414],\n",
      "         [0.23529413, 0.31176472, 0.1872549 ],\n",
      "         [0.13039216, 0.18725492, 0.09607844],\n",
      "         ...,\n",
      "         [0.36470592, 0.4754902 , 0.23431374],\n",
      "         [0.27745098, 0.35882354, 0.17156863],\n",
      "         [0.25882354, 0.35000002, 0.14411765]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.09215687, 0.13039216, 0.06764706],\n",
      "         [0.10294119, 0.1509804 , 0.08529412],\n",
      "         [0.10392158, 0.15686275, 0.08235294],\n",
      "         ...,\n",
      "         [0.09509805, 0.14411765, 0.07745098],\n",
      "         [0.11176471, 0.16666669, 0.09607844],\n",
      "         [0.13333334, 0.19509804, 0.10980393]],\n",
      "\n",
      "        [[0.07843138, 0.11274511, 0.06176471],\n",
      "         [0.09803922, 0.1382353 , 0.07352941],\n",
      "         [0.1137255 , 0.16470589, 0.0882353 ],\n",
      "         ...,\n",
      "         [0.13333336, 0.18235296, 0.09313726],\n",
      "         [0.16568628, 0.23725492, 0.1264706 ],\n",
      "         [0.20000002, 0.26078433, 0.14803922]],\n",
      "\n",
      "        [[0.07647059, 0.10784315, 0.05882353],\n",
      "         [0.07843138, 0.11078432, 0.05882353],\n",
      "         [0.11862746, 0.16078432, 0.0892157 ],\n",
      "         ...,\n",
      "         [0.17745098, 0.23529413, 0.12450981],\n",
      "         [0.2019608 , 0.27549022, 0.14509805],\n",
      "         [0.22450982, 0.28921568, 0.16470589]]]], dtype=float32)>, <tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "# Code listing 3.5\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session() # Making sure we are clearing out the TensorFlow graph\n",
    "\n",
    "# Read the CSV file with TensorFlow\n",
    "# The os.path.sep at the end is important for the get_image function\n",
    "data_dir = os.path.join('data','flower_images', 'flower_images') + os.path.sep\n",
    "assert os.path.exists(data_dir)\n",
    "csv_ds = tf.data.experimental.CsvDataset(\n",
    "    os.path.join(data_dir,'flower_labels.csv') , (\"\",-1), header=True\n",
    ")\n",
    "# Separate the image names and labels to two separate sets\n",
    "fname_ds = csv_ds.map(lambda a,b: a)\n",
    "label_ds = csv_ds.map(lambda a,b: b)\n",
    "\n",
    "def get_image(file_path):\n",
    "    \n",
    "    img = tf.io.read_file(data_dir + file_path)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [64, 64])\n",
    "\n",
    "# Get the images by running get_image across all the filenames\n",
    "image_ds = fname_ds.map(get_image)\n",
    "print(\"The image dataset contains: {}\".format(image_ds))\n",
    "# Create onehot encoded labels from label data\n",
    "label_ds = label_ds.map(lambda x: tf.one_hot(x, depth=10))\n",
    "# Zip the images and labels together\n",
    "data_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
    "\n",
    "# Shuffle the data so that we get a mix of labels in every batch\n",
    "data_ds = data_ds.shuffle(buffer_size= 20)\n",
    "# Define a batch of size 5 \n",
    "data_ds = data_ds.batch(5)\n",
    "# Iterate through the data to see what it contains\n",
    "for item in data_ds:\n",
    "    print(item)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and training a model\n",
    "\n",
    "Here we are defining a simple Convolution Neural Network (CNN) model to train it on the image data we just retrieved. You don't have to worry about the technical details of CNNs right now. We will discuss them in detail in the next chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 3.1604 - acc: 0.2571\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.4359 - acc: 0.5190\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.5586 - acc: 0.8286\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.2442 - acc: 0.9333\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.1048 - acc: 0.9762\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0399 - acc: 0.9952\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0678 - acc: 0.9810\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0577 - acc: 0.9810\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.0019 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2839a047198>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Defining a Convolution neural network for you to train for the flowers data\n",
    "# We will discuss convolution neural networks in more detail later\n",
    "model = Sequential([\n",
    "    Conv2D(64,(5,5), activation='relu', input_shape=(64,64,3)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Training the model with the tf.data pipeline\n",
    "model.fit(data_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras data generators to retrieve data\n",
    "\n",
    "Instead of `tf.data` API let us use the Keras `ImageDataGenerator` to retrieve the data. As you can see, the `ImageDataGenerator` involves much less code than the using the `tf.data` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210 validated image filenames.\n",
      "(array([[[[ 10.,  11.,  11.],\n",
      "         [ 51.,  74.,  46.],\n",
      "         [ 36.,  56.,  32.],\n",
      "         ...,\n",
      "         [  4.,   4.,   3.],\n",
      "         [ 16.,  25.,  11.],\n",
      "         [ 17.,  18.,  13.]],\n",
      "\n",
      "        [[ 12.,  13.,  10.],\n",
      "         [ 22.,  29.,  18.],\n",
      "         [ 35.,  46.,  26.],\n",
      "         ...,\n",
      "         [  3.,   3.,   3.],\n",
      "         [  6.,   6.,   5.],\n",
      "         [  9.,  10.,   9.]],\n",
      "\n",
      "        [[ 11.,  11.,  10.],\n",
      "         [ 13.,  14.,  12.],\n",
      "         [ 45.,  60.,  32.],\n",
      "         ...,\n",
      "         [  4.,   4.,   4.],\n",
      "         [  8.,   8.,   8.],\n",
      "         [ 13.,  15.,  12.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 63.,  98.,  51.],\n",
      "         [ 60.,  89.,  46.],\n",
      "         [ 37.,  59.,  31.],\n",
      "         ...,\n",
      "         [ 58.,  88.,  46.],\n",
      "         [ 55.,  83.,  43.],\n",
      "         [ 52.,  78.,  39.]],\n",
      "\n",
      "        [[ 64.,  96.,  46.],\n",
      "         [ 54.,  92.,  40.],\n",
      "         [ 68.,  96.,  48.],\n",
      "         ...,\n",
      "         [ 48.,  77.,  39.],\n",
      "         [ 49.,  82.,  39.],\n",
      "         [ 48.,  78.,  39.]],\n",
      "\n",
      "        [[ 34.,  45.,  32.],\n",
      "         [ 65.,  96.,  46.],\n",
      "         [ 65., 100.,  47.],\n",
      "         ...,\n",
      "         [ 49.,  82.,  39.],\n",
      "         [ 53.,  87.,  42.],\n",
      "         [ 47.,  83.,  39.]]],\n",
      "\n",
      "\n",
      "       [[[254., 255., 254.],\n",
      "         [252., 248., 245.],\n",
      "         [240., 237., 226.],\n",
      "         ...,\n",
      "         [192., 204., 189.],\n",
      "         [218., 184., 242.],\n",
      "         [228., 198., 252.]],\n",
      "\n",
      "        [[254., 255., 250.],\n",
      "         [249., 253., 243.],\n",
      "         [242., 252., 226.],\n",
      "         ...,\n",
      "         [134., 114., 178.],\n",
      "         [177., 131., 201.],\n",
      "         [215., 172., 249.]],\n",
      "\n",
      "        [[226., 231., 216.],\n",
      "         [249., 252., 244.],\n",
      "         [247., 254., 225.],\n",
      "         ...,\n",
      "         [107.,  69., 144.],\n",
      "         [146.,  99., 170.],\n",
      "         [188., 137., 218.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[211., 203., 197.],\n",
      "         [200., 189., 186.],\n",
      "         [217., 203., 202.],\n",
      "         ...,\n",
      "         [ 61.,  74.,  36.],\n",
      "         [173., 184., 143.],\n",
      "         [169., 218., 167.]],\n",
      "\n",
      "        [[183., 164., 170.],\n",
      "         [175., 154., 151.],\n",
      "         [144., 139., 131.],\n",
      "         ...,\n",
      "         [128., 123., 100.],\n",
      "         [116., 105., 118.],\n",
      "         [ 90., 106.,  94.]],\n",
      "\n",
      "        [[197., 199., 174.],\n",
      "         [162., 160., 137.],\n",
      "         [227., 222., 207.],\n",
      "         ...,\n",
      "         [ 57.,  58.,  50.],\n",
      "         [ 33.,  34.,  27.],\n",
      "         [ 55.,  54.,  43.]]]], dtype=float32), array([5, 6], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "# Code listing 3.6\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join('data','flower_images', 'flower_images')\n",
    "\n",
    "# Defining an image data generator provided in Keras\n",
    "img_gen = ImageDataGenerator()\n",
    "\n",
    "# Reading the CSV files containing filenames and labels\n",
    "labels_df = pd.read_csv(os.path.join(data_dir, 'flower_labels.csv'), header=0)\n",
    "\n",
    "# Generating data using the flow_from_dataframe function\n",
    "gen_iter = img_gen.flow_from_dataframe(\n",
    "    dataframe=labels_df, directory=data_dir, x_col='file', y_col='label', class_mode='raw', batch_size=2, target_size=(64,64))\n",
    "\n",
    "# Iterating through the data\n",
    "for item in gen_iter:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `tensorflow-datasets` library\n",
    "\n",
    "Here we will use the `tensorflow-datasets` package. It is a curated list of popular datasets available for machine learning projects. With this package you can download a dataset in a single line. This means you don't have to worry about downloading/extracting/formatting data manually. All of that will be already done when you import data using the `tensorflow-datasets` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists the available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'aeslc',\n",
       " 'aflw2k3d',\n",
       " 'amazon_us_reviews',\n",
       " 'arc',\n",
       " 'bair_robot_pushing_small',\n",
       " 'beans',\n",
       " 'big_patent',\n",
       " 'bigearthnet',\n",
       " 'billsum',\n",
       " 'binarized_mnist',\n",
       " 'binary_alpha_digits',\n",
       " 'c4',\n",
       " 'caltech101',\n",
       " 'caltech_birds2010',\n",
       " 'caltech_birds2011',\n",
       " 'cars196',\n",
       " 'cassava',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'cfq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_1',\n",
       " 'cifar10_corrupted',\n",
       " 'citrus_leaves',\n",
       " 'cityscapes',\n",
       " 'civil_comments',\n",
       " 'clevr',\n",
       " 'cmaterdb',\n",
       " 'cnn_dailymail',\n",
       " 'coco',\n",
       " 'coil100',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'cos_e',\n",
       " 'curated_breast_imaging_ddsm',\n",
       " 'cycle_gan',\n",
       " 'deep_weeds',\n",
       " 'definite_pronoun_resolution',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'div2k',\n",
       " 'dmlab',\n",
       " 'downsampled_imagenet',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'duke_ultrasound',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'eraser_multi_rc',\n",
       " 'esnli',\n",
       " 'eurosat',\n",
       " 'fashion_mnist',\n",
       " 'flic',\n",
       " 'flores',\n",
       " 'food101',\n",
       " 'gap',\n",
       " 'gigaword',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'i_naturalist2017',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imagenet_resized',\n",
       " 'imagenette',\n",
       " 'imagewang',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kitti',\n",
       " 'kmnist',\n",
       " 'lfw',\n",
       " 'librispeech',\n",
       " 'librispeech_lm',\n",
       " 'libritts',\n",
       " 'lm1b',\n",
       " 'lost_and_found',\n",
       " 'lsun',\n",
       " 'malaria',\n",
       " 'math_dataset',\n",
       " 'mnist',\n",
       " 'mnist_corrupted',\n",
       " 'movie_rationales',\n",
       " 'moving_mnist',\n",
       " 'multi_news',\n",
       " 'multi_nli',\n",
       " 'multi_nli_mismatch',\n",
       " 'natural_questions',\n",
       " 'newsroom',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'opinosis',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'patch_camelyon',\n",
       " 'pet_finder',\n",
       " 'places365_small',\n",
       " 'plant_leaves',\n",
       " 'plant_village',\n",
       " 'plantae_k',\n",
       " 'qa4mre',\n",
       " 'quickdraw_bitmap',\n",
       " 'reddit_tifu',\n",
       " 'resisc45',\n",
       " 'rock_paper_scissors',\n",
       " 'rock_you',\n",
       " 'scan',\n",
       " 'scene_parse150',\n",
       " 'scicite',\n",
       " 'scientific_papers',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'snli',\n",
       " 'so2sat',\n",
       " 'speech_commands',\n",
       " 'squad',\n",
       " 'stanford_dogs',\n",
       " 'stanford_online_products',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'super_glue',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'the300w_lp',\n",
       " 'tiny_shakespeare',\n",
       " 'titanic',\n",
       " 'trivia_qa',\n",
       " 'uc_merced',\n",
       " 'ucf101',\n",
       " 'vgg_face2',\n",
       " 'visual_domain_decathlon',\n",
       " 'voc',\n",
       " 'wider_face',\n",
       " 'wikihow',\n",
       " 'wikipedia',\n",
       " 'wmt14_translate',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_t2t_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli',\n",
       " 'xsum',\n",
       " 'yelp_polarity_reviews']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "# See all registered datasets\n",
    "tfds.list_builders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Cifar10 dataset and view information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    version=3.0.0,\n",
      "    description='The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.',\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    features=FeaturesDict({\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    total_num_examples=60000,\n",
      "    splits={\n",
      "        'test': 10000,\n",
      "        'train': 50000,\n",
      "    },\n",
      "    supervised_keys=('image', 'label'),\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session() # Making sure we are clearing out the TensorFlow graph\n",
    "\n",
    "# Load a given dataset by name, along with the DatasetInfo\n",
    "data, info = tfds.load(\"cifar10\", with_info=True)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data \n",
    "\n",
    "Here we will print the `data` and see what it provides. Then we will need to batch the data as data is provided as individual samples when you import it from `tensorflow-datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': <DatasetV1Adapter shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>, 'train': <DatasetV1Adapter shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>}\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(16, 32, 32, 3), dtype=uint8, numpy=\n",
      "array([[[[143,  96,  70],\n",
      "         [141,  96,  72],\n",
      "         [135,  93,  72],\n",
      "         ...,\n",
      "         [ 96,  37,  19],\n",
      "         [105,  42,  18],\n",
      "         [104,  38,  20]],\n",
      "\n",
      "        [[128,  98,  92],\n",
      "         [146, 118, 112],\n",
      "         [170, 145, 138],\n",
      "         ...,\n",
      "         [108,  45,  26],\n",
      "         [112,  44,  24],\n",
      "         [112,  41,  22]],\n",
      "\n",
      "        [[ 93,  69,  75],\n",
      "         [118,  96, 101],\n",
      "         [179, 160, 162],\n",
      "         ...,\n",
      "         [128,  68,  47],\n",
      "         [125,  61,  42],\n",
      "         [122,  59,  39]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[187, 150, 123],\n",
      "         [184, 148, 123],\n",
      "         [179, 142, 121],\n",
      "         ...,\n",
      "         [198, 163, 132],\n",
      "         [201, 166, 135],\n",
      "         [207, 174, 143]],\n",
      "\n",
      "        [[187, 150, 117],\n",
      "         [181, 143, 115],\n",
      "         [175, 136, 113],\n",
      "         ...,\n",
      "         [201, 164, 132],\n",
      "         [205, 168, 135],\n",
      "         [207, 171, 139]],\n",
      "\n",
      "        [[195, 161, 126],\n",
      "         [187, 153, 123],\n",
      "         [186, 151, 128],\n",
      "         ...,\n",
      "         [212, 177, 147],\n",
      "         [219, 185, 155],\n",
      "         [221, 187, 157]]],\n",
      "\n",
      "\n",
      "       [[[203, 214, 234],\n",
      "         [191, 207, 226],\n",
      "         [178, 200, 224],\n",
      "         ...,\n",
      "         [127, 172, 213],\n",
      "         [126, 171, 212],\n",
      "         [124, 170, 211]],\n",
      "\n",
      "        [[205, 214, 230],\n",
      "         [186, 199, 213],\n",
      "         [180, 197, 214],\n",
      "         ...,\n",
      "         [132, 178, 219],\n",
      "         [130, 176, 219],\n",
      "         [129, 175, 217]],\n",
      "\n",
      "        [[193, 200, 213],\n",
      "         [141, 151, 159],\n",
      "         [124, 137, 145],\n",
      "         ...,\n",
      "         [136, 178, 218],\n",
      "         [134, 177, 218],\n",
      "         [132, 176, 217]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 40,  47,  56],\n",
      "         [ 33,  37,  42],\n",
      "         [ 31,  35,  41],\n",
      "         ...,\n",
      "         [ 73,  99, 132],\n",
      "         [ 64,  91, 126],\n",
      "         [ 69,  97, 133]],\n",
      "\n",
      "        [[ 37,  44,  53],\n",
      "         [ 31,  34,  40],\n",
      "         [ 30,  34,  40],\n",
      "         ...,\n",
      "         [ 72,  98, 132],\n",
      "         [ 64,  92, 127],\n",
      "         [ 68,  96, 132]],\n",
      "\n",
      "        [[ 34,  41,  50],\n",
      "         [ 29,  32,  38],\n",
      "         [ 28,  32,  38],\n",
      "         ...,\n",
      "         [ 68,  94, 127],\n",
      "         [ 62,  89, 123],\n",
      "         [ 63,  91, 126]]],\n",
      "\n",
      "\n",
      "       [[[106, 103, 104],\n",
      "         [103,  97,  99],\n",
      "         [102,  93,  96],\n",
      "         ...,\n",
      "         [135, 126, 129],\n",
      "         [139, 130, 133],\n",
      "         [131, 122, 125]],\n",
      "\n",
      "        [[106, 104, 105],\n",
      "         [105,  99, 101],\n",
      "         [115, 106, 109],\n",
      "         ...,\n",
      "         [137, 129, 132],\n",
      "         [135, 126, 129],\n",
      "         [124, 115, 118]],\n",
      "\n",
      "        [[108, 105, 106],\n",
      "         [117, 111, 113],\n",
      "         [123, 114, 117],\n",
      "         ...,\n",
      "         [132, 123, 126],\n",
      "         [126, 117, 120],\n",
      "         [121, 112, 115]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[136, 163, 117],\n",
      "         [135, 162, 115],\n",
      "         [139, 166, 117],\n",
      "         ...,\n",
      "         [140, 144, 111],\n",
      "         [133, 135, 105],\n",
      "         [126, 125,  98]],\n",
      "\n",
      "        [[130, 150, 104],\n",
      "         [131, 150, 107],\n",
      "         [131, 150, 109],\n",
      "         ...,\n",
      "         [141, 150, 112],\n",
      "         [144, 152, 115],\n",
      "         [140, 144, 111]],\n",
      "\n",
      "        [[139, 151, 108],\n",
      "         [133, 144, 103],\n",
      "         [145, 156, 116],\n",
      "         ...,\n",
      "         [129, 137, 100],\n",
      "         [138, 144, 110],\n",
      "         [134, 136, 106]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[146, 149, 166],\n",
      "         [143, 147, 164],\n",
      "         [141, 144, 160],\n",
      "         ...,\n",
      "         [215, 194, 176],\n",
      "         [225, 204, 186],\n",
      "         [224, 203, 185]],\n",
      "\n",
      "        [[143, 149, 165],\n",
      "         [143, 148, 165],\n",
      "         [140, 144, 160],\n",
      "         ...,\n",
      "         [182, 164, 149],\n",
      "         [186, 168, 153],\n",
      "         [179, 161, 146]],\n",
      "\n",
      "        [[139, 144, 163],\n",
      "         [139, 144, 162],\n",
      "         [137, 142, 160],\n",
      "         ...,\n",
      "         [183, 161, 147],\n",
      "         [187, 165, 151],\n",
      "         [186, 164, 150]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 70,  70,  85],\n",
      "         [ 69,  69,  83],\n",
      "         [ 67,  67,  81],\n",
      "         ...,\n",
      "         [ 81,  83,  99],\n",
      "         [ 81,  84,  99],\n",
      "         [ 73,  75,  91]],\n",
      "\n",
      "        [[ 72,  71,  85],\n",
      "         [ 70,  69,  83],\n",
      "         [ 68,  67,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  96],\n",
      "         [ 81,  83,  98],\n",
      "         [ 73,  75,  90]],\n",
      "\n",
      "        [[ 72,  72,  85],\n",
      "         [ 70,  70,  83],\n",
      "         [ 69,  69,  81],\n",
      "         ...,\n",
      "         [ 79,  81,  95],\n",
      "         [ 81,  83,  97],\n",
      "         [ 73,  75,  89]]],\n",
      "\n",
      "\n",
      "       [[[133, 151, 172],\n",
      "         [129, 148, 172],\n",
      "         [131, 153, 173],\n",
      "         ...,\n",
      "         [226, 221, 211],\n",
      "         [228, 224, 215],\n",
      "         [218, 213, 207]],\n",
      "\n",
      "        [[135, 152, 174],\n",
      "         [130, 148, 174],\n",
      "         [132, 152, 176],\n",
      "         ...,\n",
      "         [215, 211, 201],\n",
      "         [213, 208, 200],\n",
      "         [203, 198, 193]],\n",
      "\n",
      "        [[142, 158, 179],\n",
      "         [133, 149, 177],\n",
      "         [131, 149, 175],\n",
      "         ...,\n",
      "         [206, 201, 194],\n",
      "         [203, 199, 193],\n",
      "         [197, 192, 188]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 82,  91,  97],\n",
      "         [ 92, 105, 109],\n",
      "         [ 92, 108, 118],\n",
      "         ...,\n",
      "         [ 86, 105, 113],\n",
      "         [ 92, 109, 116],\n",
      "         [ 90, 105, 114]],\n",
      "\n",
      "        [[ 88,  93,  99],\n",
      "         [ 96, 106, 110],\n",
      "         [ 94, 108, 117],\n",
      "         ...,\n",
      "         [ 87, 103, 117],\n",
      "         [ 91, 105, 115],\n",
      "         [ 91, 103, 116]],\n",
      "\n",
      "        [[ 88,  90,  97],\n",
      "         [ 94, 100, 105],\n",
      "         [ 91,  99, 108],\n",
      "         ...,\n",
      "         [ 89, 103, 117],\n",
      "         [ 87, 100, 111],\n",
      "         [ 90, 102, 114]]],\n",
      "\n",
      "\n",
      "       [[[ 75,  75,  80],\n",
      "         [ 75,  77,  81],\n",
      "         [ 81,  83,  88],\n",
      "         ...,\n",
      "         [ 71,  72,  70],\n",
      "         [ 68,  70,  68],\n",
      "         [ 68,  69,  64]],\n",
      "\n",
      "        [[ 72,  75,  80],\n",
      "         [ 77,  80,  87],\n",
      "         [ 82,  83,  87],\n",
      "         ...,\n",
      "         [ 71,  71,  70],\n",
      "         [ 67,  69,  68],\n",
      "         [ 67,  68,  63]],\n",
      "\n",
      "        [[ 73,  72,  75],\n",
      "         [ 79,  78,  83],\n",
      "         [ 91,  82,  79],\n",
      "         ...,\n",
      "         [ 67,  69,  66],\n",
      "         [ 66,  68,  68],\n",
      "         [ 67,  67,  64]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[118,  80,  46],\n",
      "         [114,  78,  46],\n",
      "         [112,  77,  45],\n",
      "         ...,\n",
      "         [109,  74,  40],\n",
      "         [107,  72,  38],\n",
      "         [106,  71,  37]],\n",
      "\n",
      "        [[122,  82,  45],\n",
      "         [118,  81,  46],\n",
      "         [121,  84,  48],\n",
      "         ...,\n",
      "         [114,  77,  42],\n",
      "         [113,  76,  40],\n",
      "         [115,  78,  41]],\n",
      "\n",
      "        [[123,  81,  45],\n",
      "         [119,  81,  47],\n",
      "         [120,  82,  46],\n",
      "         ...,\n",
      "         [128,  93,  60],\n",
      "         [129,  94,  61],\n",
      "         [123,  91,  58]]]], dtype=uint8)>, <tf.Tensor: shape=(16, 10), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Defining a dataset with batch size 16\n",
    "train_ds = data[\"train\"].batch(16)\n",
    "\n",
    "# Creating a dataset that returns an (image, one-hot label) tuple\n",
    "def format_data(x):\n",
    "    return (x[\"image\"], tf.one_hot(x[\"label\"], depth=10))\n",
    "train_ds = train_ds.map(format_data)\n",
    "\n",
    "# Iterating the dataset\n",
    "for item in train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a simple CNN on the Cifar10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3125/3125 [==============================] - 37s 12ms/step - loss: 3.1641 - acc: 0.1257\n",
      "Epoch 2/25\n",
      "3125/3125 [==============================] - 22s 7ms/step - loss: 2.2790 - acc: 0.1293\n",
      "Epoch 3/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 2.2457 - acc: 0.1509\n",
      "Epoch 4/25\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 2.2140 - acc: 0.1586\n",
      "Epoch 5/25\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 2.1870 - acc: 0.1669\n",
      "Epoch 6/25\n",
      "3125/3125 [==============================] - 22s 7ms/step - loss: 2.1570 - acc: 0.1856\n",
      "Epoch 7/25\n",
      "3125/3125 [==============================] - 21s 7ms/step - loss: 2.1155 - acc: 0.2001\n",
      "Epoch 8/25\n",
      "3125/3125 [==============================] - 21s 7ms/step - loss: 2.0815 - acc: 0.2108\n",
      "Epoch 9/25\n",
      "3125/3125 [==============================] - 24s 8ms/step - loss: 2.0614 - acc: 0.2199\n",
      "Epoch 10/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 2.0388 - acc: 0.2287\n",
      "Epoch 11/25\n",
      "3125/3125 [==============================] - 21s 7ms/step - loss: 2.0236 - acc: 0.2331\n",
      "Epoch 12/25\n",
      "3125/3125 [==============================] - 21s 7ms/step - loss: 1.9951 - acc: 0.2428\n",
      "Epoch 13/25\n",
      "3125/3125 [==============================] - 21s 7ms/step - loss: 1.9967 - acc: 0.2456\n",
      "Epoch 14/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9828 - acc: 0.2511\n",
      "Epoch 15/25\n",
      "3125/3125 [==============================] - 25s 8ms/step - loss: 1.9680 - acc: 0.2578\n",
      "Epoch 16/25\n",
      "3125/3125 [==============================] - 24s 8ms/step - loss: 1.9813 - acc: 0.2526\n",
      "Epoch 17/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9416 - acc: 0.2674\n",
      "Epoch 18/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9393 - acc: 0.2678\n",
      "Epoch 19/25\n",
      "3125/3125 [==============================] - 22s 7ms/step - loss: 1.9709 - acc: 0.2680\n",
      "Epoch 20/25\n",
      "3125/3125 [==============================] - 24s 8ms/step - loss: 1.9402 - acc: 0.2726\n",
      "Epoch 21/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9265 - acc: 0.2748\n",
      "Epoch 22/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9036 - acc: 0.2801\n",
      "Epoch 23/25\n",
      "3125/3125 [==============================] - 22s 7ms/step - loss: 1.9050 - acc: 0.2819\n",
      "Epoch 24/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.9075 - acc: 0.2837\n",
      "Epoch 25/25\n",
      "3125/3125 [==============================] - 23s 7ms/step - loss: 1.8830 - acc: 0.2901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x283a722bdd8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Section 3.2\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Defining a simple convolution neural network to process the CIFAR data\n",
    "model = Sequential([\n",
    "    Conv2D(64,(5,5), activation='relu', input_shape=(32,32,3)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Fitting the model on the data for 25 epochs\n",
    "model.fit(train_ds, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
